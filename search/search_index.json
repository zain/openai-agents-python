{"config":{"lang":["en","ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenAI Agents SDK","text":"<p>The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives:</p> <ul> <li>Agents, which are LLMs equipped with instructions and tools</li> <li>Handoffs, which allow agents to delegate to other agents for specific tasks</li> <li>Guardrails, which enable the inputs to agents to be validated</li> <li>Sessions, which automatically maintains conversation history across agent runs</li> </ul> <p>In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in tracing that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.</p>"},{"location":"#why-use-the-agents-sdk","title":"Why use the Agents SDK","text":"<p>The SDK has two driving design principles:</p> <ol> <li>Enough features to be worth using, but few enough primitives to make it quick to learn.</li> <li>Works great out of the box, but you can customize exactly what happens.</li> </ol> <p>Here are the main features of the SDK:</p> <ul> <li>Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.</li> <li>Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.</li> <li>Handoffs: A powerful feature to coordinate and delegate between multiple agents.</li> <li>Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.</li> <li>Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.</li> <li>Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.</li> <li>Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install openai-agents\n</code></pre>"},{"location":"#hello-world-example","title":"Hello world example","text":"<pre><code>from agents import Agent, Runner\n\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\nprint(result.final_output)\n\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n</code></pre> <p>(If running this, ensure you set the <code>OPENAI_API_KEY</code> environment variable)</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"agents/","title":"Agents","text":"<p>Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.</p>"},{"location":"agents/#basic-configuration","title":"Basic configuration","text":"<p>The most common properties of an agent you'll configure are:</p> <ul> <li><code>name</code>: A required string that identifies your agent.</li> <li><code>instructions</code>: also known as a developer message or system prompt.</li> <li><code>model</code>: which LLM to use, and optional <code>model_settings</code> to configure model tuning parameters like temperature, top_p, etc.</li> <li><code>tools</code>: Tools that the agent can use to achieve its tasks.</li> </ul> <pre><code>from agents import Agent, ModelSettings, function_tool\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Haiku agent\",\n    instructions=\"Always respond in haiku form\",\n    model=\"o3-mini\",\n    tools=[get_weather],\n)\n</code></pre>"},{"location":"agents/#context","title":"Context","text":"<p>Agents are generic on their <code>context</code> type. Context is a dependency-injection tool: it's an object you create and pass to <code>Runner.run()</code>, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.</p> <pre><code>@dataclass\nclass UserContext:\n    uid: str\n    is_pro_user: bool\n\n    async def fetch_purchases() -&gt; list[Purchase]:\n        return ...\n\nagent = Agent[UserContext](\n    ...,\n)\n</code></pre>"},{"location":"agents/#output-types","title":"Output types","text":"<p>By default, agents produce plain text (i.e. <code>str</code>) outputs. If you want the agent to produce a particular type of output, you can use the <code>output_type</code> parameter. A common choice is to use Pydantic objects, but we support any type that can be wrapped in a Pydantic TypeAdapter - dataclasses, lists, TypedDict, etc.</p> <pre><code>from pydantic import BaseModel\nfrom agents import Agent\n\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = Agent(\n    name=\"Calendar extractor\",\n    instructions=\"Extract calendar events from text\",\n    output_type=CalendarEvent,\n)\n</code></pre> <p>Note</p> <p>When you pass an <code>output_type</code>, that tells the model to use structured outputs instead of regular plain text responses.</p>"},{"location":"agents/#handoffs","title":"Handoffs","text":"<p>Handoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant. This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task. Read more in the handoffs documentation.</p> <pre><code>from agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=(\n        \"Help the user with their questions.\"\n        \"If they ask about booking, handoff to the booking agent.\"\n        \"If they ask about refunds, handoff to the refund agent.\"\n    ),\n    handoffs=[booking_agent, refund_agent],\n)\n</code></pre>"},{"location":"agents/#dynamic-instructions","title":"Dynamic instructions","text":"<p>In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and <code>async</code> functions are accepted.</p> <pre><code>def dynamic_instructions(\n    context: RunContextWrapper[UserContext], agent: Agent[UserContext]\n) -&gt; str:\n    return f\"The user's name is {context.context.name}. Help them with their questions.\"\n\n\nagent = Agent[UserContext](\n    name=\"Triage agent\",\n    instructions=dynamic_instructions,\n)\n</code></pre>"},{"location":"agents/#lifecycle-events-hooks","title":"Lifecycle events (hooks)","text":"<p>Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the <code>hooks</code> property. Subclass the <code>AgentHooks</code> class, and override the methods you're interested in.</p>"},{"location":"agents/#guardrails","title":"Guardrails","text":"<p>Guardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance. Read more in the guardrails documentation.</p>"},{"location":"agents/#cloningcopying-agents","title":"Cloning/copying agents","text":"<p>By using the <code>clone()</code> method on an agent, you can duplicate an Agent, and optionally change any properties you like.</p> <pre><code>pirate_agent = Agent(\n    name=\"Pirate\",\n    instructions=\"Write like a pirate\",\n    model=\"o3-mini\",\n)\n\nrobot_agent = pirate_agent.clone(\n    name=\"Robot\",\n    instructions=\"Write like a robot\",\n)\n</code></pre>"},{"location":"agents/#forcing-tool-use","title":"Forcing tool use","text":"<p>Supplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting <code>ModelSettings.tool_choice</code>. Valid values are:</p> <ol> <li><code>auto</code>, which allows the LLM to decide whether or not to use a tool.</li> <li><code>required</code>, which requires the LLM to use a tool (but it can intelligently decide which tool).</li> <li><code>none</code>, which requires the LLM to not use a tool.</li> <li>Setting a specific string e.g. <code>my_tool</code>, which requires the LLM to use that specific tool.</li> </ol> <p>Note</p> <p>To prevent infinite loops, the framework automatically resets <code>tool_choice</code> to \"auto\" after a tool call. This behavior is configurable via <code>agent.reset_tool_choice</code>. The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of <code>tool_choice</code>, ad infinitum.</p> <p>If you want the Agent to completely stop after a tool call (rather than continuing with auto mode), you can set [<code>Agent.tool_use_behavior=\"stop_on_first_tool\"</code>] which will directly use the tool output as the final response without further LLM processing.</p>"},{"location":"config/","title":"Configuring the SDK","text":""},{"location":"config/#api-keys-and-clients","title":"API keys and clients","text":"<p>By default, the SDK looks for the <code>OPENAI_API_KEY</code> environment variable for LLM requests and tracing, as soon as it is imported. If you are unable to set that environment variable before your app starts, you can use the set_default_openai_key() function to set the key.</p> <pre><code>from agents import set_default_openai_key\n\nset_default_openai_key(\"sk-...\")\n</code></pre> <p>Alternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an <code>AsyncOpenAI</code> instance, using the API key from the environment variable or the default key set above. You can change this by using the set_default_openai_client() function.</p> <pre><code>from openai import AsyncOpenAI\nfrom agents import set_default_openai_client\n\ncustom_client = AsyncOpenAI(base_url=\"...\", api_key=\"...\")\nset_default_openai_client(custom_client)\n</code></pre> <p>Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the set_default_openai_api() function.</p> <pre><code>from agents import set_default_openai_api\n\nset_default_openai_api(\"chat_completions\")\n</code></pre>"},{"location":"config/#tracing","title":"Tracing","text":"<p>Tracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set). You can specifically set the API key used for tracing by using the <code>set_tracing_export_api_key</code> function.</p> <pre><code>from agents import set_tracing_export_api_key\n\nset_tracing_export_api_key(\"sk-...\")\n</code></pre> <p>You can also disable tracing entirely by using the <code>set_tracing_disabled()</code> function.</p> <pre><code>from agents import set_tracing_disabled\n\nset_tracing_disabled(True)\n</code></pre>"},{"location":"config/#debug-logging","title":"Debug logging","text":"<p>The SDK has two Python loggers without any handlers set. By default, this means that warnings and errors are sent to <code>stdout</code>, but other logs are suppressed.</p> <p>To enable verbose logging, use the <code>enable_verbose_stdout_logging()</code> function.</p> <pre><code>from agents import enable_verbose_stdout_logging\n\nenable_verbose_stdout_logging()\n</code></pre> <p>Alternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the Python logging guide.</p> <pre><code>import logging\n\nlogger = logging.getLogger(\"openai.agents\") # or openai.agents.tracing for the Tracing logger\n\n# To make all logs show up\nlogger.setLevel(logging.DEBUG)\n# To make info and above show up\nlogger.setLevel(logging.INFO)\n# To make warning and above show up\nlogger.setLevel(logging.WARNING)\n# etc\n\n# You can customize this as needed, but this will output to `stderr` by default\nlogger.addHandler(logging.StreamHandler())\n</code></pre>"},{"location":"config/#sensitive-data-in-logs","title":"Sensitive data in logs","text":"<p>Certain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables.</p> <p>To disable logging LLM inputs and outputs:</p> <pre><code>export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1\n</code></pre> <p>To disable logging tool inputs and outputs:</p> <pre><code>export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1\n</code></pre>"},{"location":"context/","title":"Context management","text":"<p>Context is an overloaded term. There are two main classes of context you might care about:</p> <ol> <li>Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like <code>on_handoff</code>, in lifecycle hooks, etc.</li> <li>Context available to LLMs: this is data the LLM sees when generating a response.</li> </ol>"},{"location":"context/#local-context","title":"Local context","text":"<p>This is represented via the <code>RunContextWrapper</code> class and the <code>context</code> property within it. The way this works is:</p> <ol> <li>You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.</li> <li>You pass that object to the various run methods (e.g. <code>Runner.run(..., **context=whatever**))</code>.</li> <li>All your tool calls, lifecycle hooks etc will be passed a wrapper object, <code>RunContextWrapper[T]</code>, where <code>T</code> represents your context object type which you can access via <code>wrapper.context</code>.</li> </ol> <p>The most important thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same type of context.</p> <p>You can use the context for things like:</p> <ul> <li>Contextual data for your run (e.g. things like a username/uid or other information about the user)</li> <li>Dependencies (e.g. logger objects, data fetchers, etc)</li> <li>Helper functions</li> </ul> <p>Note</p> <p>The context object is not sent to the LLM. It is purely a local object that you can read from, write to and call methods on it.</p> <pre><code>import asyncio\nfrom dataclasses import dataclass\n\nfrom agents import Agent, RunContextWrapper, Runner, function_tool\n\n@dataclass\nclass UserInfo:  # (1)!\n    name: str\n    uid: int\n\n@function_tool\nasync def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -&gt; str:  # (2)!\n    \"\"\"Fetch the age of the user. Call this function to get user's age information.\"\"\"\n    return f\"The user {wrapper.context.name} is 47 years old\"\n\nasync def main():\n    user_info = UserInfo(name=\"John\", uid=123)\n\n    agent = Agent[UserInfo](  # (3)!\n        name=\"Assistant\",\n        tools=[fetch_user_age],\n    )\n\n    result = await Runner.run(  # (4)!\n        starting_agent=agent,\n        input=\"What is the age of the user?\",\n        context=user_info,\n    )\n\n    print(result.final_output)  # (5)!\n    # The user John is 47 years old.\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ol> <li>This is the context object. We've used a dataclass here, but you can use any type.</li> <li>This is a tool. You can see it takes a <code>RunContextWrapper[UserInfo]</code>. The tool implementation reads from the context.</li> <li>We mark the agent with the generic <code>UserInfo</code>, so that the typechecker can catch errors (for example, if we tried to pass a tool that took a different context type).</li> <li>The context is passed to the <code>run</code> function.</li> <li>The agent correctly calls the tool and gets the age.</li> </ol>"},{"location":"context/#agentllm-context","title":"Agent/LLM context","text":"<p>When an LLM is called, the only data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:</p> <ol> <li>You can add it to the Agent <code>instructions</code>. This is also known as a \"system prompt\" or \"developer message\". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).</li> <li>Add it to the <code>input</code> when calling the <code>Runner.run</code> functions. This is similar to the <code>instructions</code> tactic, but allows you to have messages that are lower in the chain of command.</li> <li>Expose it via function tools. This is useful for on-demand context - the LLM decides when it needs some data, and can call the tool to fetch that data.</li> <li>Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for \"grounding\" the response in relevant contextual data.</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>Check out a variety of sample implementations of the SDK in the examples section of the repo. The examples are organized into several categories that demonstrate different patterns and capabilities.</p>"},{"location":"examples/#categories","title":"Categories","text":"<ul> <li> <p>agent_patterns:   Examples in this category illustrate common agent design patterns, such as</p> <ul> <li>Deterministic workflows</li> <li>Agents as tools</li> <li>Parallel agent execution</li> </ul> </li> <li> <p>basic:   These examples showcase foundational capabilities of the SDK, such as</p> <ul> <li>Dynamic system prompts</li> <li>Streaming outputs</li> <li>Lifecycle events</li> </ul> </li> <li> <p>tool examples:   Learn how to implement OAI hosted tools such as web search and file search,    and integrate them into your agents.</p> </li> <li> <p>model providers:   Explore how to use non-OpenAI models with the SDK.</p> </li> <li> <p>handoffs:   See practical examples of agent handoffs.</p> </li> <li> <p>mcp:   Learn how to build agents with MCP.</p> </li> <li> <p>customer_service and research_bot:   Two more built-out examples that illustrate real-world applications</p> <ul> <li>customer_service: Example customer service system for an airline.</li> <li>research_bot: Simple deep research clone.</li> </ul> </li> <li> <p>voice:   See examples of voice agents, using our TTS and STT models.</p> </li> <li> <p>realtime:   Examples showing how to build realtime experiences using the SDK.</p> </li> </ul>"},{"location":"guardrails/","title":"Guardrails","text":"<p>Guardrails run in parallel to your agents, enabling you to do checks and validations of user input. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money.</p> <p>There are two kinds of guardrails:</p> <ol> <li>Input guardrails run on the initial user input</li> <li>Output guardrails run on the final agent output</li> </ol>"},{"location":"guardrails/#input-guardrails","title":"Input guardrails","text":"<p>Input guardrails run in 3 steps:</p> <ol> <li>First, the guardrail receives the same input passed to the agent.</li> <li>Next, the guardrail function runs to produce a <code>GuardrailFunctionOutput</code>, which is then wrapped in an <code>InputGuardrailResult</code></li> <li>Finally, we check if <code>.tripwire_triggered</code> is true. If true, an <code>InputGuardrailTripwireTriggered</code> exception is raised, so you can appropriately respond to the user or handle the exception.</li> </ol> <p>Note</p> <p>Input guardrails are intended to run on user input, so an agent's guardrails only run if the agent is the first agent. You might wonder, why is the <code>guardrails</code> property on the agent instead of passed to <code>Runner.run</code>? It's because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.</p>"},{"location":"guardrails/#output-guardrails","title":"Output guardrails","text":"<p>Output guardrails run in 3 steps:</p> <ol> <li>First, the guardrail receives the output produced by the agent.</li> <li>Next, the guardrail function runs to produce a <code>GuardrailFunctionOutput</code>, which is then wrapped in an <code>OutputGuardrailResult</code></li> <li>Finally, we check if <code>.tripwire_triggered</code> is true. If true, an <code>OutputGuardrailTripwireTriggered</code> exception is raised, so you can appropriately respond to the user or handle the exception.</li> </ol> <p>Note</p> <p>Output guardrails are intended to run on the final agent output, so an agent's guardrails only run if the agent is the last agent. Similar to the input guardrails, we do this because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.</p>"},{"location":"guardrails/#tripwires","title":"Tripwires","text":"<p>If the input or output fails the guardrail, the Guardrail can signal this with a tripwire. As soon as we see a guardrail that has triggered the tripwires, we immediately raise a <code>{Input,Output}GuardrailTripwireTriggered</code> exception and halt the Agent execution.</p>"},{"location":"guardrails/#implementing-a-guardrail","title":"Implementing a guardrail","text":"<p>You need to provide a function that receives input, and returns a <code>GuardrailFunctionOutput</code>. In this example, we'll do this by running an Agent under the hood.</p> <pre><code>from pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    InputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    TResponseInputItem,\n    input_guardrail,\n)\n\nclass MathHomeworkOutput(BaseModel):\n    is_math_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent( # (1)!\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking you to do their math homework.\",\n    output_type=MathHomeworkOutput,\n)\n\n\n@input_guardrail\nasync def math_guardrail( # (2)!\n    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n) -&gt; GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output, # (3)!\n        tripwire_triggered=result.final_output.is_math_homework,\n    )\n\n\nagent = Agent(  # (4)!\n    name=\"Customer support agent\",\n    instructions=\"You are a customer support agent. You help customers with their questions.\",\n    input_guardrails=[math_guardrail],\n)\n\nasync def main():\n    # This should trip the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"Guardrail didn't trip - this is unexpected\")\n\n    except InputGuardrailTripwireTriggered:\n        print(\"Math homework guardrail tripped\")\n</code></pre> <ol> <li>We'll use this agent in our guardrail function.</li> <li>This is the guardrail function that receives the agent's input/context, and returns the result.</li> <li>We can include extra information in the guardrail result.</li> <li>This is the actual agent that defines the workflow.</li> </ol> <p>Output guardrails are similar.</p> <pre><code>from pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    OutputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    output_guardrail,\n)\nclass MessageOutput(BaseModel): # (1)!\n    response: str\n\nclass MathOutput(BaseModel): # (2)!\n    reasoning: str\n    is_math: bool\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the output includes any math.\",\n    output_type=MathOutput,\n)\n\n@output_guardrail\nasync def math_guardrail(  # (3)!\n    ctx: RunContextWrapper, agent: Agent, output: MessageOutput\n) -&gt; GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math,\n    )\n\nagent = Agent( # (4)!\n    name=\"Customer support agent\",\n    instructions=\"You are a customer support agent. You help customers with their questions.\",\n    output_guardrails=[math_guardrail],\n    output_type=MessageOutput,\n)\n\nasync def main():\n    # This should trip the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"Guardrail didn't trip - this is unexpected\")\n\n    except OutputGuardrailTripwireTriggered:\n        print(\"Math output guardrail tripped\")\n</code></pre> <ol> <li>This is the actual agent's output type.</li> <li>This is the guardrail's output type.</li> <li>This is the guardrail function that receives the agent's output, and returns the result.</li> <li>This is the actual agent that defines the workflow.</li> </ol>"},{"location":"handoffs/","title":"Handoffs","text":"<p>Handoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc.</p> <p>Handoffs are represented as tools to the LLM. So if there's a handoff to an agent named <code>Refund Agent</code>, the tool would be called <code>transfer_to_refund_agent</code>.</p>"},{"location":"handoffs/#creating-a-handoff","title":"Creating a handoff","text":"<p>All agents have a <code>handoffs</code> param, which can either take an <code>Agent</code> directly, or a <code>Handoff</code> object that customizes the Handoff.</p> <p>You can create a handoff using the <code>handoff()</code> function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters.</p>"},{"location":"handoffs/#basic-usage","title":"Basic Usage","text":"<p>Here's how you can create a simple handoff:</p> <pre><code>from agents import Agent, handoff\n\nbilling_agent = Agent(name=\"Billing agent\")\nrefund_agent = Agent(name=\"Refund agent\")\n\n# (1)!\ntriage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n</code></pre> <ol> <li>You can use the agent directly (as in <code>billing_agent</code>), or you can use the <code>handoff()</code> function.</li> </ol>"},{"location":"handoffs/#customizing-handoffs-via-the-handoff-function","title":"Customizing handoffs via the <code>handoff()</code> function","text":"<p>The <code>handoff()</code> function lets you customize things.</p> <ul> <li><code>agent</code>: This is the agent to which things will be handed off.</li> <li><code>tool_name_override</code>: By default, the <code>Handoff.default_tool_name()</code> function is used, which resolves to <code>transfer_to_&lt;agent_name&gt;</code>. You can override this.</li> <li><code>tool_description_override</code>: Override the default tool description from <code>Handoff.default_tool_description()</code></li> <li><code>on_handoff</code>: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context, and can optionally also receive LLM generated input. The input data is controlled by the <code>input_type</code> param.</li> <li><code>input_type</code>: The type of input expected by the handoff (optional).</li> <li><code>input_filter</code>: This lets you filter the input received by the next agent. See below for more.</li> </ul> <pre><code>from agents import Agent, handoff, RunContextWrapper\n\ndef on_handoff(ctx: RunContextWrapper[None]):\n    print(\"Handoff called\")\n\nagent = Agent(name=\"My agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    tool_name_override=\"custom_handoff_tool\",\n    tool_description_override=\"Custom description\",\n)\n</code></pre>"},{"location":"handoffs/#handoff-inputs","title":"Handoff inputs","text":"<p>In certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an \"Escalation agent\". You might want a reason to be provided, so you can log it.</p> <pre><code>from pydantic import BaseModel\n\nfrom agents import Agent, handoff, RunContextWrapper\n\nclass EscalationData(BaseModel):\n    reason: str\n\nasync def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n    print(f\"Escalation agent called with reason: {input_data.reason}\")\n\nagent = Agent(name=\"Escalation agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    input_type=EscalationData,\n)\n</code></pre>"},{"location":"handoffs/#input-filters","title":"Input filters","text":"<p>When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an <code>input_filter</code>. An input filter is a function that receives the existing input via a <code>HandoffInputData</code>, and must return a new <code>HandoffInputData</code>.</p> <p>There are some common patterns (for example removing all tool calls from the history), which are implemented for you in <code>agents.extensions.handoff_filters</code></p> <pre><code>from agents import Agent, handoff\nfrom agents.extensions import handoff_filters\n\nagent = Agent(name=\"FAQ agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    input_filter=handoff_filters.remove_all_tools, # (1)!\n)\n</code></pre> <ol> <li>This will automatically remove all tools from the history when <code>FAQ agent</code> is called.</li> </ol>"},{"location":"handoffs/#recommended-prompts","title":"Recommended prompts","text":"<p>To make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents. We have a suggested prefix in <code>agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX</code>, or you can call <code>agents.extensions.handoff_prompt.prompt_with_handoff_instructions</code> to automatically add recommended data to your prompts.</p> <pre><code>from agents import Agent\nfrom agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n\nbilling_agent = Agent(\n    name=\"Billing agent\",\n    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n    &lt;Fill in the rest of your prompt here&gt;.\"\"\",\n)\n</code></pre>"},{"location":"mcp/","title":"Model context protocol (MCP)","text":"<p>The Model context protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:</p> <p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p> <p>The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools and prompts to your Agents.</p>"},{"location":"mcp/#mcp-servers","title":"MCP servers","text":"<p>Currently, the MCP spec defines three kinds of servers, based on the transport mechanism they use:</p> <ol> <li>stdio servers run as a subprocess of your application. You can think of them as running \"locally\".</li> <li>HTTP over SSE servers run remotely. You connect to them via a URL.</li> <li>Streamable HTTP servers run remotely using the Streamable HTTP transport defined in the MCP spec.</li> </ol> <p>You can use the <code>MCPServerStdio</code>, <code>MCPServerSse</code>, and <code>MCPServerStreamableHttp</code> classes to connect to these servers.</p> <p>For example, this is how you'd use the official MCP filesystem server.</p> <pre><code>from agents.run_context import RunContextWrapper\n\nasync with MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    }\n) as server:\n    # Note: In practice, you typically add the server to an Agent\n    # and let the framework handle tool listing automatically.\n    # Direct calls to list_tools() require run_context and agent parameters.\n    run_context = RunContextWrapper(context=None)\n    agent = Agent(name=\"test\", instructions=\"test\")\n    tools = await server.list_tools(run_context, agent)\n</code></pre>"},{"location":"mcp/#using-mcp-servers","title":"Using MCP servers","text":"<p>MCP servers can be added to Agents. The Agents SDK will call <code>list_tools()</code> on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls <code>call_tool()</code> on that server.</p> <pre><code>agent=Agent(\n    name=\"Assistant\",\n    instructions=\"Use the tools to achieve the task\",\n    mcp_servers=[mcp_server_1, mcp_server_2]\n)\n</code></pre>"},{"location":"mcp/#tool-filtering","title":"Tool filtering","text":"<p>You can filter which tools are available to your Agent by configuring tool filters on MCP servers. The SDK supports both static and dynamic tool filtering.</p>"},{"location":"mcp/#static-tool-filtering","title":"Static tool filtering","text":"<p>For simple allow/block lists, you can use static filtering:</p> <pre><code>from agents.mcp import create_static_tool_filter\n\n# Only expose specific tools from this server\nserver = MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    },\n    tool_filter=create_static_tool_filter(\n        allowed_tool_names=[\"read_file\", \"write_file\"]\n    )\n)\n\n# Exclude specific tools from this server\nserver = MCPServerStdio(\n    params={\n        \"command\": \"npx\", \n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    },\n    tool_filter=create_static_tool_filter(\n        blocked_tool_names=[\"delete_file\"]\n    )\n)\n</code></pre> <p>When both <code>allowed_tool_names</code> and <code>blocked_tool_names</code> are configured, the processing order is: 1. First apply <code>allowed_tool_names</code> (allowlist) - only keep the specified tools 2. Then apply <code>blocked_tool_names</code> (blocklist) - exclude specified tools from the remaining tools</p> <p>For example, if you configure <code>allowed_tool_names=[\"read_file\", \"write_file\", \"delete_file\"]</code> and <code>blocked_tool_names=[\"delete_file\"]</code>, only <code>read_file</code> and <code>write_file</code> tools will be available.</p>"},{"location":"mcp/#dynamic-tool-filtering","title":"Dynamic tool filtering","text":"<p>For more complex filtering logic, you can use dynamic filters with functions:</p> <pre><code>from agents.mcp import ToolFilterContext\n\n# Simple synchronous filter\ndef custom_filter(context: ToolFilterContext, tool) -&gt; bool:\n    \"\"\"Example of a custom tool filter.\"\"\"\n    # Filter logic based on tool name patterns\n    return tool.name.startswith(\"allowed_prefix\")\n\n# Context-aware filter\ndef context_aware_filter(context: ToolFilterContext, tool) -&gt; bool:\n    \"\"\"Filter tools based on context information.\"\"\"\n    # Access agent information\n    agent_name = context.agent.name\n\n    # Access server information  \n    server_name = context.server_name\n\n    # Implement your custom filtering logic here\n    return some_filtering_logic(agent_name, server_name, tool)\n\n# Asynchronous filter\nasync def async_filter(context: ToolFilterContext, tool) -&gt; bool:\n    \"\"\"Example of an asynchronous filter.\"\"\"\n    # Perform async operations if needed\n    result = await some_async_check(context, tool)\n    return result\n\nserver = MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    },\n    tool_filter=custom_filter  # or context_aware_filter or async_filter\n)\n</code></pre> <p>The <code>ToolFilterContext</code> provides access to: - <code>run_context</code>: The current run context - <code>agent</code>: The agent requesting the tools  - <code>server_name</code>: The name of the MCP server</p>"},{"location":"mcp/#prompts","title":"Prompts","text":"<p>MCP servers can also provide prompts that can be used to dynamically generate agent instructions. This allows you to create reusable instruction templates that can be customized with parameters.</p>"},{"location":"mcp/#using-prompts","title":"Using prompts","text":"<p>MCP servers that support prompts provide two key methods:</p> <ul> <li><code>list_prompts()</code>: Lists all available prompts on the server</li> <li><code>get_prompt(name, arguments)</code>: Gets a specific prompt with optional parameters</li> </ul> <pre><code># List available prompts\nprompts_result = await server.list_prompts()\nfor prompt in prompts_result.prompts:\n    print(f\"Prompt: {prompt.name} - {prompt.description}\")\n\n# Get a specific prompt with parameters\nprompt_result = await server.get_prompt(\n    \"generate_code_review_instructions\",\n    {\"focus\": \"security vulnerabilities\", \"language\": \"python\"}\n)\ninstructions = prompt_result.messages[0].content.text\n\n# Use the prompt-generated instructions with an Agent\nagent = Agent(\n    name=\"Code Reviewer\",\n    instructions=instructions,  # Instructions from MCP prompt\n    mcp_servers=[server]\n)\n</code></pre>"},{"location":"mcp/#caching","title":"Caching","text":"<p>Every time an Agent runs, it calls <code>list_tools()</code> on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass <code>cache_tools_list=True</code> to <code>MCPServerStdio</code>, <code>MCPServerSse</code>, and <code>MCPServerStreamableHttp</code>. You should only do this if you're certain the tool list will not change.</p> <p>If you want to invalidate the cache, you can call <code>invalidate_tools_cache()</code> on the servers.</p>"},{"location":"mcp/#end-to-end-examples","title":"End-to-end examples","text":"<p>View complete working examples at examples/mcp.</p>"},{"location":"mcp/#tracing","title":"Tracing","text":"<p>Tracing automatically captures MCP operations, including:</p> <ol> <li>Calls to the MCP server to list tools</li> <li>MCP-related info on function calls</li> </ol> <p></p>"},{"location":"multi_agent/","title":"Orchestrating multiple agents","text":"<p>Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:</p> <ol> <li>Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.</li> <li>Orchestrating via code: determining the flow of agents via your code.</li> </ol> <p>You can mix and match these patterns. Each has their own tradeoffs, described below.</p>"},{"location":"multi_agent/#orchestrating-via-llm","title":"Orchestrating via LLM","text":"<p>An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:</p> <ul> <li>Web search to find information online</li> <li>File search and retrieval to search through proprietary data and connections</li> <li>Computer use to take actions on a computer</li> <li>Code execution to do data analysis</li> <li>Handoffs to specialized agents that are great at planning, report writing and more.</li> </ul> <p>This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:</p> <ol> <li>Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.</li> <li>Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.</li> <li>Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.</li> <li>Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.</li> <li>Invest in evals. This lets you train your agents to improve and get better at tasks.</li> </ol>"},{"location":"multi_agent/#orchestrating-via-code","title":"Orchestrating via code","text":"<p>While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:</p> <ul> <li>Using structured outputs to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.</li> <li>Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.</li> <li>Running the agent that performs the task in a <code>while</code> loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.</li> <li>Running multiple agents in parallel, e.g. via Python primitives like <code>asyncio.gather</code>. This is useful for speed when you have multiple tasks that don't depend on each other.</li> </ul> <p>We have a number of examples in <code>examples/agent_patterns</code>.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#create-a-project-and-virtual-environment","title":"Create a project and virtual environment","text":"<p>You'll only need to do this once.</p> <pre><code>mkdir my_project\ncd my_project\npython -m venv .venv\n</code></pre>"},{"location":"quickstart/#activate-the-virtual-environment","title":"Activate the virtual environment","text":"<p>Do this every time you start a new terminal session.</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"quickstart/#install-the-agents-sdk","title":"Install the Agents SDK","text":"<pre><code>pip install openai-agents # or `uv add openai-agents`, etc\n</code></pre>"},{"location":"quickstart/#set-an-openai-api-key","title":"Set an OpenAI API key","text":"<p>If you don't have one, follow these instructions to create an OpenAI API key.</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"quickstart/#create-your-first-agent","title":"Create your first agent","text":"<p>Agents are defined with instructions, a name, and optional config (such as <code>model_config</code>)</p> <pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"Math Tutor\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n</code></pre>"},{"location":"quickstart/#add-a-few-more-agents","title":"Add a few more agents","text":"<p>Additional agents can be defined in the same way. <code>handoff_descriptions</code> provide additional context for determining handoff routing</p> <pre><code>from agents import Agent\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n</code></pre>"},{"location":"quickstart/#define-your-handoffs","title":"Define your handoffs","text":"<p>On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task.</p> <pre><code>triage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"You determine which agent to use based on the user's homework question\",\n    handoffs=[history_tutor_agent, math_tutor_agent]\n)\n</code></pre>"},{"location":"quickstart/#run-the-agent-orchestration","title":"Run the agent orchestration","text":"<p>Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents.</p> <pre><code>from agents import Runner\n\nasync def main():\n    result = await Runner.run(triage_agent, \"What is the capital of France?\")\n    print(result.final_output)\n</code></pre>"},{"location":"quickstart/#add-a-guardrail","title":"Add a guardrail","text":"<p>You can define custom guardrails to run on the input or output.</p> <pre><code>from agents import GuardrailFunctionOutput, Agent, Runner\nfrom pydantic import BaseModel\n\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n    final_output = result.final_output_as(HomeworkOutput)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n</code></pre>"},{"location":"quickstart/#put-it-all-together","title":"Put it all together","text":"<p>Let's put it all together and run the entire workflow, using handoffs and the input guardrail.</p> <pre><code>from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\nfrom agents.exceptions import InputGuardrailTripwireTriggered\nfrom pydantic import BaseModel\nimport asyncio\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n    final_output = result.final_output_as(HomeworkOutput)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"You determine which agent to use based on the user's homework question\",\n    handoffs=[history_tutor_agent, math_tutor_agent],\n    input_guardrails=[\n        InputGuardrail(guardrail_function=homework_guardrail),\n    ],\n)\n\nasync def main():\n    # Example 1: History question\n    try:\n        result = await Runner.run(triage_agent, \"who was the first president of the united states?\")\n        print(result.final_output)\n    except InputGuardrailTripwireTriggered as e:\n        print(\"Guardrail blocked this input:\", e)\n\n    # Example 2: General/philosophical question\n    try:\n        result = await Runner.run(triage_agent, \"What is the meaning of life?\")\n        print(result.final_output)\n    except InputGuardrailTripwireTriggered as e:\n        print(\"Guardrail blocked this input:\", e)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#view-your-traces","title":"View your traces","text":"<p>To review what happened during your agent run, navigate to the Trace viewer in the OpenAI Dashboard to view traces of your agent runs.</p>"},{"location":"quickstart/#next-steps","title":"Next steps","text":"<p>Learn how to build more complex agentic flows:</p> <ul> <li>Learn about how to configure Agents.</li> <li>Learn about running agents.</li> <li>Learn about tools, guardrails and models.</li> </ul>"},{"location":"release/","title":"Release process/changelog","text":"<p>The project follows a slightly modified version of semantic versioning using the form <code>0.Y.Z</code>. The leading <code>0</code> indicates the SDK is still evolving rapidly. Increment the components as follows:</p>"},{"location":"release/#minor-y-versions","title":"Minor (<code>Y</code>) versions","text":"<p>We will increase minor versions <code>Y</code> for breaking changes to any public interfaces that are not marked as beta. For example, going from <code>0.0.x</code> to <code>0.1.x</code> might include breaking changes.</p> <p>If you don't want breaking changes, we recommend pinning to <code>0.0.x</code> versions in your project.</p>"},{"location":"release/#patch-z-versions","title":"Patch (<code>Z</code>) versions","text":"<p>We will increment <code>Z</code> for non-breaking changes:</p> <ul> <li>Bug fixes</li> <li>New features</li> <li>Changes to private interfaces</li> <li>Updates to beta features</li> </ul>"},{"location":"release/#breaking-change-changelog","title":"Breaking change changelog","text":""},{"location":"release/#020","title":"0.2.0","text":"<p>In this version, a few places that used to take <code>Agent</code> as an arg, now take <code>AgentBase</code> as an arg instead. For example, the <code>list_tools()</code> call in MCP servers. This is a purely typing change, you will still receive <code>Agent</code> objects. To update, just fix type errors by replacing <code>Agent</code> with <code>AgentBase</code>.</p>"},{"location":"release/#010","title":"0.1.0","text":"<p>In this version, <code>MCPServer.list_tools()</code> has two new params: <code>run_context</code> and <code>agent</code>. You'll need to add these params to any classes that subclass <code>MCPServer</code>.</p>"},{"location":"repl/","title":"REPL utility","text":"<p>The SDK provides <code>run_demo_loop</code> for quick interactive testing.</p> <pre><code>import asyncio\nfrom agents import Agent, run_demo_loop\n\nasync def main() -&gt; None:\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant.\")\n    await run_demo_loop(agent)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p><code>run_demo_loop</code> prompts for user input in a loop, keeping the conversation history between turns. By default it streams model output as it is produced. Type <code>quit</code> or <code>exit</code> (or press <code>Ctrl-D</code>) to leave the loop.</p>"},{"location":"results/","title":"Results","text":"<p>When you call the <code>Runner.run</code> methods, you either get a:</p> <ul> <li><code>RunResult</code> if you call <code>run</code> or <code>run_sync</code></li> <li><code>RunResultStreaming</code> if you call <code>run_streamed</code></li> </ul> <p>Both of these inherit from <code>RunResultBase</code>, which is where most useful information is present.</p>"},{"location":"results/#final-output","title":"Final output","text":"<p>The <code>final_output</code> property contains the final output of the last agent that ran. This is either:</p> <ul> <li>a <code>str</code>, if the last agent didn't have an <code>output_type</code> defined</li> <li>an object of type <code>last_agent.output_type</code>, if the agent had an output type defined.</li> </ul> <p>Note</p> <p><code>final_output</code> is of type <code>Any</code>. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.</p>"},{"location":"results/#inputs-for-the-next-turn","title":"Inputs for the next turn","text":"<p>You can use <code>result.to_input_list()</code> to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.</p>"},{"location":"results/#last-agent","title":"Last agent","text":"<p>The <code>last_agent</code> property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.</p>"},{"location":"results/#new-items","title":"New items","text":"<p>The <code>new_items</code> property contains the new items generated during the run. The items are <code>RunItem</code>s. A run item wraps the raw item generated by the LLM.</p> <ul> <li><code>MessageOutputItem</code> indicates a message from the LLM. The raw item is the message generated.</li> <li><code>HandoffCallItem</code> indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.</li> <li><code>HandoffOutputItem</code> indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.</li> <li><code>ToolCallItem</code> indicates that the LLM invoked a tool.</li> <li><code>ToolCallOutputItem</code> indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.</li> <li><code>ReasoningItem</code> indicates a reasoning item from the LLM. The raw item is the reasoning generated.</li> </ul>"},{"location":"results/#other-information","title":"Other information","text":""},{"location":"results/#guardrail-results","title":"Guardrail results","text":"<p>The <code>input_guardrail_results</code> and <code>output_guardrail_results</code> properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.</p>"},{"location":"results/#raw-responses","title":"Raw responses","text":"<p>The <code>raw_responses</code> property contains the <code>ModelResponse</code>s generated by the LLM.</p>"},{"location":"results/#original-input","title":"Original input","text":"<p>The <code>input</code> property contains the original input you provided to the <code>run</code> method. In most cases you won't need this, but it's available in case you do.</p>"},{"location":"running_agents/","title":"Running agents","text":"<p>You can run agents via the <code>Runner</code> class. You have 3 options:</p> <ol> <li><code>Runner.run()</code>, which runs async and returns a <code>RunResult</code>.</li> <li><code>Runner.run_sync()</code>, which is a sync method and just runs <code>.run()</code> under the hood.</li> <li><code>Runner.run_streamed()</code>, which runs async and returns a <code>RunResultStreaming</code>. It calls the LLM in streaming mode, and streams those events to you as they are received.</li> </ol> <pre><code>from agents import Agent, Runner\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n\n    result = await Runner.run(agent, \"Write a haiku about recursion in programming.\")\n    print(result.final_output)\n    # Code within the code,\n    # Functions calling themselves,\n    # Infinite loop's dance\n</code></pre> <p>Read more in the results guide.</p>"},{"location":"running_agents/#the-agent-loop","title":"The agent loop","text":"<p>When you use the run method in <code>Runner</code>, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.</p> <p>The runner then runs a loop:</p> <ol> <li>We call the LLM for the current agent, with the current input.</li> <li>The LLM produces its output.<ol> <li>If the LLM returns a <code>final_output</code>, the loop ends and we return the result.</li> <li>If the LLM does a handoff, we update the current agent and input, and re-run the loop.</li> <li>If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.</li> </ol> </li> <li>If we exceed the <code>max_turns</code> passed, we raise a <code>MaxTurnsExceeded</code> exception.</li> </ol> <p>Note</p> <p>The rule for whether the LLM output is considered as a \"final output\" is that it produces text output with the desired type, and there are no tool calls.</p>"},{"location":"running_agents/#streaming","title":"Streaming","text":"<p>Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the <code>RunResultStreaming</code> will contain the complete information about the run, including all the new outputs produced. You can call <code>.stream_events()</code> for the streaming events. Read more in the streaming guide.</p>"},{"location":"running_agents/#run-config","title":"Run config","text":"<p>The <code>run_config</code> parameter lets you configure some global settings for the agent run:</p> <ul> <li><code>model</code>: Allows setting a global LLM model to use, irrespective of what <code>model</code> each Agent has.</li> <li><code>model_provider</code>: A model provider for looking up model names, which defaults to OpenAI.</li> <li><code>model_settings</code>: Overrides agent-specific settings. For example, you can set a global <code>temperature</code> or <code>top_p</code>.</li> <li><code>input_guardrails</code>, <code>output_guardrails</code>: A list of input or output guardrails to include on all runs.</li> <li><code>handoff_input_filter</code>: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in <code>Handoff.input_filter</code> for more details.</li> <li><code>tracing_disabled</code>: Allows you to disable tracing for the entire run.</li> <li><code>trace_include_sensitive_data</code>: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.</li> <li><code>workflow_name</code>, <code>trace_id</code>, <code>group_id</code>: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting <code>workflow_name</code>. The group ID is an optional field that lets you link traces across multiple runs.</li> <li><code>trace_metadata</code>: Metadata to include on all traces.</li> </ul>"},{"location":"running_agents/#conversationschat-threads","title":"Conversations/chat threads","text":"<p>Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:</p> <ol> <li>User turn: user enter text</li> <li>Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.</li> </ol> <p>At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.</p>"},{"location":"running_agents/#manual-conversation-management","title":"Manual conversation management","text":"<p>You can manually manage conversation history using the <code>RunResultBase.to_input_list()</code> method to get the inputs for the next turn:</p> <pre><code>async def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    thread_id = \"thread_123\"  # Example thread ID\n    with trace(workflow_name=\"Conversation\", group_id=thread_id):\n        # First turn\n        result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\")\n        print(result.final_output)\n        # San Francisco\n\n        # Second turn\n        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n        result = await Runner.run(agent, new_input)\n        print(result.final_output)\n        # California\n</code></pre>"},{"location":"running_agents/#automatic-conversation-management-with-sessions","title":"Automatic conversation management with Sessions","text":"<p>For a simpler approach, you can use Sessions to automatically handle conversation history without manually calling <code>.to_input_list()</code>:</p> <pre><code>from agents import Agent, Runner, SQLiteSession\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    # Create session instance\n    session = SQLiteSession(\"conversation_123\")\n\n    with trace(workflow_name=\"Conversation\", group_id=thread_id):\n        # First turn\n        result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\", session=session)\n        print(result.final_output)\n        # San Francisco\n\n        # Second turn - agent automatically remembers previous context\n        result = await Runner.run(agent, \"What state is it in?\", session=session)\n        print(result.final_output)\n        # California\n</code></pre> <p>Sessions automatically:</p> <ul> <li>Retrieves conversation history before each run</li> <li>Stores new messages after each run</li> <li>Maintains separate conversations for different session IDs</li> </ul> <p>See the Sessions documentation for more details.</p>"},{"location":"running_agents/#exceptions","title":"Exceptions","text":"<p>The SDK raises exceptions in certain cases. The full list is in <code>agents.exceptions</code>. As an overview:</p> <ul> <li><code>AgentsException</code> is the base class for all exceptions raised in the SDK.</li> <li><code>MaxTurnsExceeded</code> is raised when the run exceeds the <code>max_turns</code> passed to the run methods.</li> <li><code>ModelBehaviorError</code> is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.</li> <li><code>UserError</code> is raised when you (the person writing code using the SDK) make an error using the SDK.</li> <li><code>InputGuardrailTripwireTriggered</code>, <code>OutputGuardrailTripwireTriggered</code> is raised when a guardrail is tripped.</li> </ul>"},{"location":"sessions/","title":"Sessions","text":"<p>The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle <code>.to_input_list()</code> between turns.</p> <p>Sessions stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management. This is particularly useful for building chat applications or multi-turn conversations where you want the agent to remember previous interactions.</p>"},{"location":"sessions/#quick-start","title":"Quick start","text":"<pre><code>from agents import Agent, Runner, SQLiteSession\n\n# Create agent\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"Reply very concisely.\",\n)\n\n# Create a session instance with a session ID\nsession = SQLiteSession(\"conversation_123\")\n\n# First turn\nresult = await Runner.run(\n    agent,\n    \"What city is the Golden Gate Bridge in?\",\n    session=session\n)\nprint(result.final_output)  # \"San Francisco\"\n\n# Second turn - agent automatically remembers previous context\nresult = await Runner.run(\n    agent,\n    \"What state is it in?\",\n    session=session\n)\nprint(result.final_output)  # \"California\"\n\n# Also works with synchronous runner\nresult = Runner.run_sync(\n    agent,\n    \"What's the population?\",\n    session=session\n)\nprint(result.final_output)  # \"Approximately 39 million\"\n</code></pre>"},{"location":"sessions/#how-it-works","title":"How it works","text":"<p>When session memory is enabled:</p> <ol> <li>Before each run: The runner automatically retrieves the conversation history for the session and prepends it to the input items.</li> <li>After each run: All new items generated during the run (user input, assistant responses, tool calls, etc.) are automatically stored in the session.</li> <li>Context preservation: Each subsequent run with the same session includes the full conversation history, allowing the agent to maintain context.</li> </ol> <p>This eliminates the need to manually call <code>.to_input_list()</code> and manage conversation state between runs.</p>"},{"location":"sessions/#memory-operations","title":"Memory operations","text":""},{"location":"sessions/#basic-operations","title":"Basic operations","text":"<p>Sessions supports several operations for managing conversation history:</p> <pre><code>from agents import SQLiteSession\n\nsession = SQLiteSession(\"user_123\", \"conversations.db\")\n\n# Get all items in a session\nitems = await session.get_items()\n\n# Add new items to a session\nnew_items = [\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n]\nawait session.add_items(new_items)\n\n# Remove and return the most recent item\nlast_item = await session.pop_item()\nprint(last_item)  # {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n\n# Clear all items from a session\nawait session.clear_session()\n</code></pre>"},{"location":"sessions/#using-pop_item-for-corrections","title":"Using pop_item for corrections","text":"<p>The <code>pop_item</code> method is particularly useful when you want to undo or modify the last item in a conversation:</p> <pre><code>from agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\")\nsession = SQLiteSession(\"correction_example\")\n\n# Initial conversation\nresult = await Runner.run(\n    agent,\n    \"What's 2 + 2?\",\n    session=session\n)\nprint(f\"Agent: {result.final_output}\")\n\n# User wants to correct their question\nassistant_item = await session.pop_item()  # Remove agent's response\nuser_item = await session.pop_item()  # Remove user's question\n\n# Ask a corrected question\nresult = await Runner.run(\n    agent,\n    \"What's 2 + 3?\",\n    session=session\n)\nprint(f\"Agent: {result.final_output}\")\n</code></pre>"},{"location":"sessions/#memory-options","title":"Memory options","text":""},{"location":"sessions/#no-memory-default","title":"No memory (default)","text":"<pre><code># Default behavior - no session memory\nresult = await Runner.run(agent, \"Hello\")\n</code></pre>"},{"location":"sessions/#sqlite-memory","title":"SQLite memory","text":"<pre><code>from agents import SQLiteSession\n\n# In-memory database (lost when process ends)\nsession = SQLiteSession(\"user_123\")\n\n# Persistent file-based database\nsession = SQLiteSession(\"user_123\", \"conversations.db\")\n\n# Use the session\nresult = await Runner.run(\n    agent,\n    \"Hello\",\n    session=session\n)\n</code></pre>"},{"location":"sessions/#multiple-sessions","title":"Multiple sessions","text":"<pre><code>from agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\")\n\n# Different sessions maintain separate conversation histories\nsession_1 = SQLiteSession(\"user_123\", \"conversations.db\")\nsession_2 = SQLiteSession(\"user_456\", \"conversations.db\")\n\nresult1 = await Runner.run(\n    agent,\n    \"Hello\",\n    session=session_1\n)\nresult2 = await Runner.run(\n    agent,\n    \"Hello\",\n    session=session_2\n)\n</code></pre>"},{"location":"sessions/#custom-memory-implementations","title":"Custom memory implementations","text":"<p>You can implement your own session memory by creating a class that follows the <code>Session</code> protocol:</p> <pre><code>from agents.memory import Session\nfrom typing import List\n\nclass MyCustomSession:\n    \"\"\"Custom session implementation following the Session protocol.\"\"\"\n\n    def __init__(self, session_id: str):\n        self.session_id = session_id\n        # Your initialization here\n\n    async def get_items(self, limit: int | None = None) -&gt; List[dict]:\n        \"\"\"Retrieve conversation history for this session.\"\"\"\n        # Your implementation here\n        pass\n\n    async def add_items(self, items: List[dict]) -&gt; None:\n        \"\"\"Store new items for this session.\"\"\"\n        # Your implementation here\n        pass\n\n    async def pop_item(self) -&gt; dict | None:\n        \"\"\"Remove and return the most recent item from this session.\"\"\"\n        # Your implementation here\n        pass\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        # Your implementation here\n        pass\n\n# Use your custom session\nagent = Agent(name=\"Assistant\")\nresult = await Runner.run(\n    agent,\n    \"Hello\",\n    session=MyCustomSession(\"my_session\")\n)\n\n## Session management\n\n### Session ID naming\n\nUse meaningful session IDs that help you organize conversations:\n\n-   User-based: `\"user_12345\"`\n-   Thread-based: `\"thread_abc123\"`\n-   Context-based: `\"support_ticket_456\"`\n\n### Memory persistence\n\n-   Use in-memory SQLite (`SQLiteSession(\"session_id\")`) for temporary conversations\n-   Use file-based SQLite (`SQLiteSession(\"session_id\", \"path/to/db.sqlite\")`) for persistent conversations\n-   Consider implementing custom session backends for production systems (Redis, PostgreSQL, etc.)\n\n### Session management\n\n```python\n# Clear a session when conversation should start fresh\nawait session.clear_session()\n\n# Different agents can share the same session\nsupport_agent = Agent(name=\"Support\")\nbilling_agent = Agent(name=\"Billing\")\nsession = SQLiteSession(\"user_123\")\n\n# Both agents will see the same conversation history\nresult1 = await Runner.run(\n    support_agent,\n    \"Help me with my account\",\n    session=session\n)\nresult2 = await Runner.run(\n    billing_agent,\n    \"What are my charges?\",\n    session=session\n)\n</code></pre>"},{"location":"sessions/#complete-example","title":"Complete example","text":"<p>Here's a complete example showing session memory in action:</p> <pre><code>import asyncio\nfrom agents import Agent, Runner, SQLiteSession\n\n\nasync def main():\n    # Create an agent\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"Reply very concisely.\",\n    )\n\n    # Create a session instance that will persist across runs\n    session = SQLiteSession(\"conversation_123\", \"conversation_history.db\")\n\n    print(\"=== Sessions Example ===\")\n    print(\"The agent will remember previous messages automatically.\\n\")\n\n    # First turn\n    print(\"First turn:\")\n    print(\"User: What city is the Golden Gate Bridge in?\")\n    result = await Runner.run(\n        agent,\n        \"What city is the Golden Gate Bridge in?\",\n        session=session\n    )\n    print(f\"Assistant: {result.final_output}\")\n    print()\n\n    # Second turn - the agent will remember the previous conversation\n    print(\"Second turn:\")\n    print(\"User: What state is it in?\")\n    result = await Runner.run(\n        agent,\n        \"What state is it in?\",\n        session=session\n    )\n    print(f\"Assistant: {result.final_output}\")\n    print()\n\n    # Third turn - continuing the conversation\n    print(\"Third turn:\")\n    print(\"User: What's the population of that state?\")\n    result = await Runner.run(\n        agent,\n        \"What's the population of that state?\",\n        session=session\n    )\n    print(f\"Assistant: {result.final_output}\")\n    print()\n\n    print(\"=== Conversation Complete ===\")\n    print(\"Notice how the agent remembered the context from previous turns!\")\n    print(\"Sessions automatically handles conversation history.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"sessions/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see:</p> <ul> <li><code>Session</code> - Protocol interface</li> <li><code>SQLiteSession</code> - SQLite implementation</li> </ul>"},{"location":"streaming/","title":"Streaming","text":"<p>Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.</p> <p>To stream, you can call <code>Runner.run_streamed()</code>, which will give you a <code>RunResultStreaming</code>. Calling <code>result.stream_events()</code> gives you an async stream of <code>StreamEvent</code> objects, which are described below.</p>"},{"location":"streaming/#raw-response-events","title":"Raw response events","text":"<p><code>RawResponsesStreamEvent</code> are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like <code>response.created</code>, <code>response.output_text.delta</code>, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.</p> <p>For example, this will output the text generated by the LLM token-by-token.</p> <pre><code>import asyncio\nfrom openai.types.responses import ResponseTextDeltaEvent\nfrom agents import Agent, Runner\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n            print(event.data.delta, end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"streaming/#run-item-events-and-agent-events","title":"Run item events and agent events","text":"<p><code>RunItemStreamEvent</code>s are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of \"message generated\", \"tool ran\", etc, instead of each token. Similarly, <code>AgentUpdatedStreamEvent</code> gives you updates when the current agent changes (e.g. as the result of a handoff).</p> <p>For example, this will ignore raw events and stream updates to the user.</p> <pre><code>import asyncio\nimport random\nfrom agents import Agent, ItemHelpers, Runner, function_tool\n\n@function_tool\ndef how_many_jokes() -&gt; int:\n    return random.randint(1, 10)\n\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n        tools=[how_many_jokes],\n    )\n\n    result = Runner.run_streamed(\n        agent,\n        input=\"Hello\",\n    )\n    print(\"=== Run starting ===\")\n\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        # When the agent updates, print that\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        # When items are generated, print them\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n            else:\n                pass  # Ignore other event types\n\n    print(\"=== Run complete ===\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p>Tools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK:</p> <ul> <li>Hosted tools: these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools.</li> <li>Function calling: these allow you to use any Python function as a tool.</li> <li>Agents as tools: this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them.</li> </ul>"},{"location":"tools/#hosted-tools","title":"Hosted tools","text":"<p>OpenAI offers a few built-in tools when using the <code>OpenAIResponsesModel</code>:</p> <ul> <li>The <code>WebSearchTool</code> lets an agent search the web.</li> <li>The <code>FileSearchTool</code> allows retrieving information from your OpenAI Vector Stores.</li> <li>The <code>ComputerTool</code> allows automating computer use tasks.</li> <li>The <code>CodeInterpreterTool</code> lets the LLM execute code in a sandboxed environment.</li> <li>The <code>HostedMCPTool</code> exposes a remote MCP server's tools to the model.</li> <li>The <code>ImageGenerationTool</code> generates images from a prompt.</li> <li>The <code>LocalShellTool</code> runs shell commands on your machine.</li> </ul> <pre><code>from agents import Agent, FileSearchTool, Runner, WebSearchTool\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[\n        WebSearchTool(),\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[\"VECTOR_STORE_ID\"],\n        ),\n    ],\n)\n\nasync def main():\n    result = await Runner.run(agent, \"Which coffee shop should I go to, taking into account my preferences and the weather today in SF?\")\n    print(result.final_output)\n</code></pre>"},{"location":"tools/#function-tools","title":"Function tools","text":"<p>You can use any Python function as a tool. The Agents SDK will setup the tool automatically:</p> <ul> <li>The name of the tool will be the name of the Python function (or you can provide a name)</li> <li>Tool description will be taken from the docstring of the function (or you can provide a description)</li> <li>The schema for the function inputs is automatically created from the function's arguments</li> <li>Descriptions for each input are taken from the docstring of the function, unless disabled</li> </ul> <p>We use Python's <code>inspect</code> module to extract the function signature, along with <code>griffe</code> to parse docstrings and <code>pydantic</code> for schema creation.</p> <pre><code>import json\n\nfrom typing_extensions import TypedDict, Any\n\nfrom agents import Agent, FunctionTool, RunContextWrapper, function_tool\n\n\nclass Location(TypedDict):\n    lat: float\n    long: float\n\n@function_tool  # (1)!\nasync def fetch_weather(location: Location) -&gt; str:\n    # (2)!\n    \"\"\"Fetch the weather for a given location.\n\n    Args:\n        location: The location to fetch the weather for.\n    \"\"\"\n    # In real life, we'd fetch the weather from a weather API\n    return \"sunny\"\n\n\n@function_tool(name_override=\"fetch_data\")  # (3)!\ndef read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -&gt; str:\n    \"\"\"Read the contents of a file.\n\n    Args:\n        path: The path to the file to read.\n        directory: The directory to read the file from.\n    \"\"\"\n    # In real life, we'd read the file from the file system\n    return \"&lt;file contents&gt;\"\n\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[fetch_weather, read_file],  # (4)!\n)\n\nfor tool in agent.tools:\n    if isinstance(tool, FunctionTool):\n        print(tool.name)\n        print(tool.description)\n        print(json.dumps(tool.params_json_schema, indent=2))\n        print()\n</code></pre> <ol> <li>You can use any Python types as arguments to your functions, and the function can be sync or async.</li> <li>Docstrings, if present, are used to capture descriptions and argument descriptions</li> <li>Functions can optionally take the <code>context</code> (must be the first argument). You can also set overrides, like the name of the tool, description, which docstring style to use, etc.</li> <li>You can pass the decorated functions to the list of tools.</li> </ol> Expand to see output <pre><code>fetch_weather\nFetch the weather for a given location.\n{\n\"$defs\": {\n  \"Location\": {\n    \"properties\": {\n      \"lat\": {\n        \"title\": \"Lat\",\n        \"type\": \"number\"\n      },\n      \"long\": {\n        \"title\": \"Long\",\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\n      \"lat\",\n      \"long\"\n    ],\n    \"title\": \"Location\",\n    \"type\": \"object\"\n  }\n},\n\"properties\": {\n  \"location\": {\n    \"$ref\": \"#/$defs/Location\",\n    \"description\": \"The location to fetch the weather for.\"\n  }\n},\n\"required\": [\n  \"location\"\n],\n\"title\": \"fetch_weather_args\",\n\"type\": \"object\"\n}\n\nfetch_data\nRead the contents of a file.\n{\n\"properties\": {\n  \"path\": {\n    \"description\": \"The path to the file to read.\",\n    \"title\": \"Path\",\n    \"type\": \"string\"\n  },\n  \"directory\": {\n    \"anyOf\": [\n      {\n        \"type\": \"string\"\n      },\n      {\n        \"type\": \"null\"\n      }\n    ],\n    \"default\": null,\n    \"description\": \"The directory to read the file from.\",\n    \"title\": \"Directory\"\n  }\n},\n\"required\": [\n  \"path\"\n],\n\"title\": \"fetch_data_args\",\n\"type\": \"object\"\n}\n</code></pre>"},{"location":"tools/#custom-function-tools","title":"Custom function tools","text":"<p>Sometimes, you don't want to use a Python function as a tool. You can directly create a <code>FunctionTool</code> if you prefer. You'll need to provide:</p> <ul> <li><code>name</code></li> <li><code>description</code></li> <li><code>params_json_schema</code>, which is the JSON schema for the arguments</li> <li><code>on_invoke_tool</code>, which is an async function that receives a [<code>ToolContext</code>][agents.tool_context.ToolContext] and the arguments as a JSON string, and must return the tool output as a string.</li> </ul> <pre><code>from typing import Any\n\nfrom pydantic import BaseModel\n\nfrom agents import RunContextWrapper, FunctionTool\n\n\n\ndef do_some_work(data: str) -&gt; str:\n    return \"done\"\n\n\nclass FunctionArgs(BaseModel):\n    username: str\n    age: int\n\n\nasync def run_function(ctx: RunContextWrapper[Any], args: str) -&gt; str:\n    parsed = FunctionArgs.model_validate_json(args)\n    return do_some_work(data=f\"{parsed.username} is {parsed.age} years old\")\n\n\ntool = FunctionTool(\n    name=\"process_user\",\n    description=\"Processes extracted user data\",\n    params_json_schema=FunctionArgs.model_json_schema(),\n    on_invoke_tool=run_function,\n)\n</code></pre>"},{"location":"tools/#automatic-argument-and-docstring-parsing","title":"Automatic argument and docstring parsing","text":"<p>As mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:</p> <ol> <li>The signature parsing is done via the <code>inspect</code> module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.</li> <li>We use <code>griffe</code> to parse docstrings. Supported docstring formats are <code>google</code>, <code>sphinx</code> and <code>numpy</code>. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling <code>function_tool</code>. You can also disable docstring parsing by setting <code>use_docstring_info</code> to <code>False</code>.</li> </ol> <p>The code for the schema extraction lives in <code>agents.function_schema</code>.</p>"},{"location":"tools/#agents-as-tools","title":"Agents as tools","text":"<p>In some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.</p> <pre><code>from agents import Agent, Runner\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You translate the user's message to Spanish\",\n)\n\nfrench_agent = Agent(\n    name=\"French agent\",\n    instructions=\"You translate the user's message to French\",\n)\n\norchestrator_agent = Agent(\n    name=\"orchestrator_agent\",\n    instructions=(\n        \"You are a translation agent. You use the tools given to you to translate.\"\n        \"If asked for multiple translations, you call the relevant tools.\"\n    ),\n    tools=[\n        spanish_agent.as_tool(\n            tool_name=\"translate_to_spanish\",\n            tool_description=\"Translate the user's message to Spanish\",\n        ),\n        french_agent.as_tool(\n            tool_name=\"translate_to_french\",\n            tool_description=\"Translate the user's message to French\",\n        ),\n    ],\n)\n\nasync def main():\n    result = await Runner.run(orchestrator_agent, input=\"Say 'Hello, how are you?' in Spanish.\")\n    print(result.final_output)\n</code></pre>"},{"location":"tools/#customizing-tool-agents","title":"Customizing tool-agents","text":"<p>The <code>agent.as_tool</code> function is a convenience method to make it easy to turn an agent into a tool. It doesn't support all configuration though; for example, you can't set <code>max_turns</code>. For advanced use cases, use <code>Runner.run</code> directly in your tool implementation:</p> <pre><code>@function_tool\nasync def run_my_agent() -&gt; str:\n    \"\"\"A tool that runs the agent with custom configs\"\"\"\n\n    agent = Agent(name=\"My agent\", instructions=\"...\")\n\n    result = await Runner.run(\n        agent,\n        input=\"...\",\n        max_turns=5,\n        run_config=...\n    )\n\n    return str(result.final_output)\n</code></pre>"},{"location":"tools/#custom-output-extraction","title":"Custom output extraction","text":"<p>In certain cases, you might want to modify the output of the tool-agents before returning it to the central agent. This may be useful if you want to:</p> <ul> <li>Extract a specific piece of information (e.g., a JSON payload) from the sub-agent's chat history.</li> <li>Convert or reformat the agent\u2019s final answer (e.g., transform Markdown into plain text or CSV).</li> <li>Validate the output or provide a fallback value when the agent\u2019s response is missing or malformed.</li> </ul> <p>You can do this by supplying the <code>custom_output_extractor</code> argument to the <code>as_tool</code> method:</p> <pre><code>async def extract_json_payload(run_result: RunResult) -&gt; str:\n    # Scan the agent\u2019s outputs in reverse order until we find a JSON-like message from a tool call.\n    for item in reversed(run_result.new_items):\n        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith(\"{\"):\n            return item.output.strip()\n    # Fallback to an empty JSON object if nothing was found\n    return \"{}\"\n\n\njson_tool = data_agent.as_tool(\n    tool_name=\"get_data_json\",\n    tool_description=\"Run the data agent and return only its JSON payload\",\n    custom_output_extractor=extract_json_payload,\n)\n</code></pre>"},{"location":"tools/#handling-errors-in-function-tools","title":"Handling errors in function tools","text":"<p>When you create a function tool via <code>@function_tool</code>, you can pass a <code>failure_error_function</code>. This is a function that provides an error response to the LLM in case the tool call crashes.</p> <ul> <li>By default (i.e. if you don't pass anything), it runs a <code>default_tool_error_function</code> which tells the LLM an error occurred.</li> <li>If you pass your own error function, it runs that instead, and sends the response to the LLM.</li> <li>If you explicitly pass <code>None</code>, then any tool call errors will be re-raised for you to handle. This could be a <code>ModelBehaviorError</code> if the model produced invalid JSON, or a <code>UserError</code> if your code crashed, etc.</li> </ul> <p>If you are manually creating a <code>FunctionTool</code> object, then you must handle errors inside the <code>on_invoke_tool</code> function.</p>"},{"location":"tracing/","title":"Tracing","text":"<p>The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during development and in production.</p> <p>Note</p> <p>Tracing is enabled by default. There are two ways to disable tracing:</p> <ol> <li>You can globally disable tracing by setting the env var <code>OPENAI_AGENTS_DISABLE_TRACING=1</code></li> <li>You can disable tracing for a single run by setting <code>agents.run.RunConfig.tracing_disabled</code> to <code>True</code></li> </ol> <p>For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable.</p>"},{"location":"tracing/#traces-and-spans","title":"Traces and spans","text":"<ul> <li>Traces represent a single end-to-end operation of a \"workflow\". They're composed of Spans. Traces have the following properties:<ul> <li><code>workflow_name</code>: This is the logical workflow or app. For example \"Code generation\" or \"Customer service\".</li> <li><code>trace_id</code>: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format <code>trace_&lt;32_alphanumeric&gt;</code>.</li> <li><code>group_id</code>: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID.</li> <li><code>disabled</code>: If True, the trace will not be recorded.</li> <li><code>metadata</code>: Optional metadata for the trace.</li> </ul> </li> <li>Spans represent operations that have a start and end time. Spans have:<ul> <li><code>started_at</code> and <code>ended_at</code> timestamps.</li> <li><code>trace_id</code>, to represent the trace they belong to</li> <li><code>parent_id</code>, which points to the parent Span of this Span (if any)</li> <li><code>span_data</code>, which is information about the Span. For example, <code>AgentSpanData</code> contains information about the Agent, <code>GenerationSpanData</code> contains information about the LLM generation, etc.</li> </ul> </li> </ul>"},{"location":"tracing/#default-tracing","title":"Default tracing","text":"<p>By default, the SDK traces the following:</p> <ul> <li>The entire <code>Runner.{run, run_sync, run_streamed}()</code> is wrapped in a <code>trace()</code>.</li> <li>Each time an agent runs, it is wrapped in <code>agent_span()</code></li> <li>LLM generations are wrapped in <code>generation_span()</code></li> <li>Function tool calls are each wrapped in <code>function_span()</code></li> <li>Guardrails are wrapped in <code>guardrail_span()</code></li> <li>Handoffs are wrapped in <code>handoff_span()</code></li> <li>Audio inputs (speech-to-text) are wrapped in a <code>transcription_span()</code></li> <li>Audio outputs (text-to-speech) are wrapped in a <code>speech_span()</code></li> <li>Related audio spans may be parented under a <code>speech_group_span()</code></li> </ul> <p>By default, the trace is named \"Agent trace\". You can set this name if you use <code>trace</code>, or you can can configure the name and other properties with the <code>RunConfig</code>.</p> <p>In addition, you can set up custom trace processors to push traces to other destinations (as a replacement, or secondary destination).</p>"},{"location":"tracing/#higher-level-traces","title":"Higher level traces","text":"<p>Sometimes, you might want multiple calls to <code>run()</code> to be part of a single trace. You can do this by wrapping the entire code in a <code>trace()</code>.</p> <pre><code>from agents import Agent, Runner, trace\n\nasync def main():\n    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n\n    with trace(\"Joke workflow\"): # (1)!\n        first_result = await Runner.run(agent, \"Tell me a joke\")\n        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\")\n        print(f\"Joke: {first_result.final_output}\")\n        print(f\"Rating: {second_result.final_output}\")\n</code></pre> <ol> <li>Because the two calls to <code>Runner.run</code> are wrapped in a <code>with trace()</code>, the individual runs will be part of the overall trace rather than creating two traces.</li> </ol>"},{"location":"tracing/#creating-traces","title":"Creating traces","text":"<p>You can use the <code>trace()</code> function to create a trace. Traces need to be started and finished. You have two options to do so:</p> <ol> <li>Recommended: use the trace as a context manager, i.e. <code>with trace(...) as my_trace</code>. This will automatically start and end the trace at the right time.</li> <li>You can also manually call <code>trace.start()</code> and <code>trace.finish()</code>.</li> </ol> <p>The current trace is tracked via a Python <code>contextvar</code>. This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass <code>mark_as_current</code> and <code>reset_current</code> to <code>start()</code>/<code>finish()</code> to update the current trace.</p>"},{"location":"tracing/#creating-spans","title":"Creating spans","text":"<p>You can use the various <code>*_span()</code> methods to create a span. In general, you don't need to manually create spans. A <code>custom_span()</code> function is available for tracking custom span information.</p> <p>Spans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python <code>contextvar</code>.</p>"},{"location":"tracing/#sensitive-data","title":"Sensitive data","text":"<p>Certain spans may capture potentially sensitive data.</p> <p>The <code>generation_span()</code> stores the inputs/outputs of the LLM generation, and <code>function_span()</code> stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via <code>RunConfig.trace_include_sensitive_data</code>.</p> <p>Similarly, Audio spans include base64-encoded PCM data for input and output audio by default. You can disable capturing this audio data by configuring <code>VoicePipelineConfig.trace_include_sensitive_audio_data</code>.</p>"},{"location":"tracing/#custom-tracing-processors","title":"Custom tracing processors","text":"<p>The high level architecture for tracing is:</p> <ul> <li>At initialization, we create a global <code>TraceProvider</code>, which is responsible for creating traces.</li> <li>We configure the <code>TraceProvider</code> with a <code>BatchTraceProcessor</code> that sends traces/spans in batches to a <code>BackendSpanExporter</code>, which exports the spans and traces to the OpenAI backend in batches.</li> </ul> <p>To customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options:</p> <ol> <li><code>add_trace_processor()</code> lets you add an additional trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend.</li> <li><code>set_trace_processors()</code> lets you replace the default processors with your own trace processors. This means traces will not be sent to the OpenAI backend unless you include a <code>TracingProcessor</code> that does so.</li> </ol>"},{"location":"tracing/#external-tracing-processors-list","title":"External tracing processors list","text":"<ul> <li>Weights &amp; Biases</li> <li>Arize-Phoenix</li> <li>Future AGI</li> <li>MLflow (self-hosted/OSS</li> <li>MLflow (Databricks hosted</li> <li>Braintrust</li> <li>Pydantic Logfire</li> <li>AgentOps</li> <li>Scorecard</li> <li>Keywords AI</li> <li>LangSmith</li> <li>Maxim AI</li> <li>Comet Opik</li> <li>Langfuse</li> <li>Langtrace</li> <li>Okahu-Monocle</li> <li>Galileo</li> <li>Portkey AI</li> </ul>"},{"location":"visualization/","title":"Agent Visualization","text":"<p>Agent visualization allows you to generate a structured graphical representation of agents and their relationships using Graphviz. This is useful for understanding how agents, tools, and handoffs interact within an application.</p>"},{"location":"visualization/#installation","title":"Installation","text":"<p>Install the optional <code>viz</code> dependency group:</p> <pre><code>pip install \"openai-agents[viz]\"\n</code></pre>"},{"location":"visualization/#generating-a-graph","title":"Generating a Graph","text":"<p>You can generate an agent visualization using the <code>draw_graph</code> function. This function creates a directed graph where:</p> <ul> <li>Agents are represented as yellow boxes.</li> <li>Tools are represented as green ellipses.</li> <li>Handoffs are directed edges from one agent to another.</li> </ul>"},{"location":"visualization/#example-usage","title":"Example Usage","text":"<pre><code>from agents import Agent, function_tool\nfrom agents.extensions.visualization import draw_graph\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny.\"\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    tools=[get_weather],\n)\n\ndraw_graph(triage_agent)\n</code></pre> <p>This generates a graph that visually represents the structure of the triage agent and its connections to sub-agents and tools.</p>"},{"location":"visualization/#understanding-the-visualization","title":"Understanding the Visualization","text":"<p>The generated graph includes:</p> <ul> <li>A start node (<code>__start__</code>) indicating the entry point.</li> <li>Agents represented as rectangles with yellow fill.</li> <li>Tools represented as ellipses with green fill.</li> <li>Directed edges indicating interactions:</li> <li>Solid arrows for agent-to-agent handoffs.</li> <li>Dotted arrows for tool invocations.</li> <li>An end node (<code>__end__</code>) indicating where execution terminates.</li> </ul>"},{"location":"visualization/#customizing-the-graph","title":"Customizing the Graph","text":""},{"location":"visualization/#showing-the-graph","title":"Showing the Graph","text":"<p>By default, <code>draw_graph</code> displays the graph inline. To show the graph in a separate window, write the following:</p> <pre><code>draw_graph(triage_agent).view()\n</code></pre>"},{"location":"visualization/#saving-the-graph","title":"Saving the Graph","text":"<p>By default, <code>draw_graph</code> displays the graph inline. To save it as a file, specify a filename:</p> <pre><code>draw_graph(triage_agent, filename=\"agent_graph\")\n</code></pre> <p>This will generate <code>agent_graph.png</code> in the working directory.</p>"},{"location":"models/","title":"Models","text":"<p>The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:</p> <ul> <li>Recommended: the <code>OpenAIResponsesModel</code>, which calls OpenAI APIs using the new Responses API.</li> <li>The <code>OpenAIChatCompletionsModel</code>, which calls OpenAI APIs using the Chat Completions API.</li> </ul>"},{"location":"models/#non-openai-models","title":"Non-OpenAI models","text":"<p>You can use most other non-OpenAI models via the LiteLLM integration. First, install the litellm dependency group:</p> <pre><code>pip install \"openai-agents[litellm]\"\n</code></pre> <p>Then, use any of the supported models with the <code>litellm/</code> prefix:</p> <pre><code>claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\ngemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)\n</code></pre>"},{"location":"models/#other-ways-to-use-non-openai-models","title":"Other ways to use non-OpenAI models","text":"<p>You can integrate other LLM providers in 3 more ways (examples here):</p> <ol> <li><code>set_default_openai_client</code> is useful in cases where you want to globally use an instance of <code>AsyncOpenAI</code> as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the <code>base_url</code> and <code>api_key</code>. See a configurable example in examples/model_providers/custom_example_global.py.</li> <li><code>ModelProvider</code> is at the <code>Runner.run</code> level. This lets you say \"use a custom model provider for all agents in this run\". See a configurable example in examples/model_providers/custom_example_provider.py.</li> <li><code>Agent.model</code> lets you specify the model on a specific Agent instance. This enables you to mix and match different providers for different agents. See a configurable example in examples/model_providers/custom_example_agent.py. An easy way to use most available models is via the LiteLLM integration.</li> </ol> <p>In cases where you do not have an API key from <code>platform.openai.com</code>, we recommend disabling tracing via <code>set_tracing_disabled()</code>, or setting up a different tracing processor.</p> <p>Note</p> <p>In these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.</p>"},{"location":"models/#mixing-and-matching-models","title":"Mixing and matching models","text":"<p>Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an <code>Agent</code>, you can select a specific model by either:</p> <ol> <li>Passing the name of a model.</li> <li>Passing any model name + a <code>ModelProvider</code> that can map that name to a Model instance.</li> <li>Directly providing a <code>Model</code> implementation.</li> </ol> <p>Note</p> <p>While our SDK supports both the <code>OpenAIResponsesModel</code> and the <code>OpenAIChatCompletionsModel</code> shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.</p> <pre><code>from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=\"o3-mini\", # (1)!\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=OpenAIChatCompletionsModel( # (2)!\n        model=\"gpt-4o\",\n        openai_client=AsyncOpenAI()\n    ),\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    model=\"gpt-3.5-turbo\",\n)\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Hola, \u00bfc\u00f3mo est\u00e1s?\")\n    print(result.final_output)\n</code></pre> <ol> <li>Sets the name of an OpenAI model directly.</li> <li>Provides a <code>Model</code> implementation.</li> </ol> <p>When you want to further configure the model used for an agent, you can pass <code>ModelSettings</code>, which provides optional model configuration parameters such as temperature.</p> <pre><code>from agents import Agent, ModelSettings\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=\"gpt-4o\",\n    model_settings=ModelSettings(temperature=0.1),\n)\n</code></pre> <p>Also, when you use OpenAI's Responses API, there are a few other optional parameters (e.g., <code>user</code>, <code>service_tier</code>, and so on). If they are not available at the top level, you can use <code>extra_args</code> to pass them as well.</p> <pre><code>from agents import Agent, ModelSettings\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=\"gpt-4o\",\n    model_settings=ModelSettings(\n        temperature=0.1,\n        extra_args={\"service_tier\": \"flex\", \"user\": \"user_12345\"},\n    ),\n)\n</code></pre>"},{"location":"models/#common-issues-with-using-other-llm-providers","title":"Common issues with using other LLM providers","text":""},{"location":"models/#tracing-client-error-401","title":"Tracing client error 401","text":"<p>If you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:</p> <ol> <li>Disable tracing entirely: <code>set_tracing_disabled(True)</code>.</li> <li>Set an OpenAI key for tracing: <code>set_tracing_export_api_key(...)</code>. This API key will only be used for uploading traces, and must be from platform.openai.com.</li> <li>Use a non-OpenAI trace processor. See the tracing docs.</li> </ol>"},{"location":"models/#responses-api-support","title":"Responses API support","text":"<p>The SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:</p> <ol> <li>Call <code>set_default_openai_api(\"chat_completions\")</code>. This works if you are setting <code>OPENAI_API_KEY</code> and <code>OPENAI_BASE_URL</code> via environment vars.</li> <li>Use <code>OpenAIChatCompletionsModel</code>. There are examples here.</li> </ol>"},{"location":"models/#structured-outputs-support","title":"Structured outputs support","text":"<p>Some model providers don't have support for structured outputs. This sometimes results in an error that looks something like this:</p> <pre><code>BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n</code></pre> <p>This is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the <code>json_schema</code> to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.</p>"},{"location":"models/#mixing-models-across-providers","title":"Mixing models across providers","text":"<p>You need to be aware of feature differences between model providers, or you may run into errors. For example, OpenAI supports structured outputs, multimodal input, and hosted file search and web search, but many other providers don't support these features. Be aware of these limitations:</p> <ul> <li>Don't send unsupported <code>tools</code> to providers that don't understand them</li> <li>Filter out multimodal inputs before calling models that are text-only</li> <li>Be aware that providers that don't support structured JSON outputs will occasionally produce invalid JSON.</li> </ul>"},{"location":"models/litellm/","title":"Using any model via LiteLLM","text":"<p>Note</p> <p>The LiteLLM integration is in beta. You may run into issues with some model providers, especially smaller ones. Please report any issues via Github issues and we'll fix quickly.</p> <p>LiteLLM is a library that allows you to use 100+ models via a single interface. We've added a LiteLLM integration to allow you to use any AI model in the Agents SDK.</p>"},{"location":"models/litellm/#setup","title":"Setup","text":"<p>You'll need to ensure <code>litellm</code> is available. You can do this by installing the optional <code>litellm</code> dependency group:</p> <pre><code>pip install \"openai-agents[litellm]\"\n</code></pre> <p>Once done, you can use <code>LitellmModel</code> in any agent.</p>"},{"location":"models/litellm/#example","title":"Example","text":"<p>This is a fully working example. When you run it, you'll be prompted for a model name and API key. For example, you could enter:</p> <ul> <li><code>openai/gpt-4.1</code> for the model, and your OpenAI API key</li> <li><code>anthropic/claude-3-5-sonnet-20240620</code> for the model, and your Anthropic API key</li> <li>etc</li> </ul> <p>For a full list of models supported in LiteLLM, see the litellm providers docs.</p> <pre><code>from __future__ import annotations\n\nimport asyncio\n\nfrom agents import Agent, Runner, function_tool, set_tracing_disabled\nfrom agents.extensions.models.litellm_model import LitellmModel\n\n@function_tool\ndef get_weather(city: str):\n    print(f\"[debug] getting weather for {city}\")\n    return f\"The weather in {city} is sunny.\"\n\n\nasync def main(model: str, api_key: str):\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You only respond in haikus.\",\n        model=LitellmModel(model=model, api_key=api_key),\n        tools=[get_weather],\n    )\n\n    result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    # First try to get model/api key from args\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, required=False)\n    parser.add_argument(\"--api-key\", type=str, required=False)\n    args = parser.parse_args()\n\n    model = args.model\n    if not model:\n        model = input(\"Enter a model name for Litellm: \")\n\n    api_key = args.api_key\n    if not api_key:\n        api_key = input(\"Enter an API key for Litellm: \")\n\n    asyncio.run(main(model, api_key))\n</code></pre>"},{"location":"realtime/guide/","title":"Guide","text":"<p>This guide provides an in-depth look at building voice-enabled AI agents using the OpenAI Agents SDK's realtime capabilities.</p> <p>Beta feature</p> <p>Realtime agents are in beta. Expect some breaking changes as we improve the implementation.</p>"},{"location":"realtime/guide/#overview","title":"Overview","text":"<p>Realtime agents allow for conversational flows, processing audio and text inputs in real time and responding with realtime audio. They maintain persistent connections with OpenAI's Realtime API, enabling natural voice conversations with low latency and the ability to handle interruptions gracefully.</p>"},{"location":"realtime/guide/#architecture","title":"Architecture","text":""},{"location":"realtime/guide/#core-components","title":"Core Components","text":"<p>The realtime system consists of several key components:</p> <ul> <li>RealtimeAgent: An agent, configured wiht instructions, tools and handoffs.</li> <li>RealtimeRunner: Manages configuration. You can call <code>runner.run()</code> to get a session.</li> <li>RealtimeSession: A single interaction session. You typically create one each time a user starts a conversation, and keep it alive until the conversation is done.</li> <li>RealtimeModel: The underlying model interface (typically OpenAI's WebSocket implementation)</li> </ul>"},{"location":"realtime/guide/#session-flow","title":"Session flow","text":"<p>A typical realtime session follows this flow:</p> <ol> <li>Create your RealtimeAgent(s) with instructions, tools and handoffs.</li> <li>Set up a RealtimeRunner with the agent and configuration options</li> <li>Start the session using <code>await runner.run()</code> which returns a RealtimeSession.</li> <li>Send audio or text messages to the session using <code>send_audio()</code> or <code>send_message()</code></li> <li>Listen for events by iterating over the session - events include audio output, transcripts, tool calls, handoffs, and errors</li> <li>Handle interruptions when users speak over the agent, which automatically stops current audio generation</li> </ol> <p>The session maintains the conversation history and manages the persistent connection with the realtime model.</p>"},{"location":"realtime/guide/#agent-configuration","title":"Agent configuration","text":"<p>RealtimeAgent works similarly to the regular Agent class with some key differences. For full API details, see the <code>RealtimeAgent</code> API reference.</p> <p>Key differences from regular agents:</p> <ul> <li>Model choice is configured at the session level, not the agent level.</li> <li>No structured output support (<code>outputType</code> is not supported).</li> <li>Voice can be configured per agent but cannot be changed after the first agent speaks.</li> <li>All other features like tools, handoffs, and instructions work the same way.</li> </ul>"},{"location":"realtime/guide/#session-configuration","title":"Session configuration","text":""},{"location":"realtime/guide/#model-settings","title":"Model settings","text":"<p>The session configuration allows you to control the underlying realtime model behavior. You can configure the model name (such as <code>gpt-4o-realtime-preview</code>), voice selection (alloy, echo, fable, onyx, nova, shimmer), and supported modalities (text and/or audio). Audio formats can be set for both input and output, with PCM16 being the default.</p>"},{"location":"realtime/guide/#audio-configuration","title":"Audio configuration","text":"<p>Audio settings control how the session handles voice input and output. You can configure input audio transcription using models like Whisper, set language preferences, and provide transcription prompts to improve accuracy for domain-specific terms. Turn detection settings control when the agent should start and stop responding, with options for voice activity detection thresholds, silence duration, and padding around detected speech.</p>"},{"location":"realtime/guide/#tools-and-functions","title":"Tools and Functions","text":""},{"location":"realtime/guide/#adding-tools","title":"Adding Tools","text":"<p>Just like regular agents, realtime agents support function tools that execute during conversations:</p> <pre><code>from agents import function_tool\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    # Your weather API logic here\n    return f\"The weather in {city} is sunny, 72\u00b0F\"\n\n@function_tool\ndef book_appointment(date: str, time: str, service: str) -&gt; str:\n    \"\"\"Book an appointment.\"\"\"\n    # Your booking logic here\n    return f\"Appointment booked for {service} on {date} at {time}\"\n\nagent = RealtimeAgent(\n    name=\"Assistant\",\n    instructions=\"You can help with weather and appointments.\",\n    tools=[get_weather, book_appointment],\n)\n</code></pre>"},{"location":"realtime/guide/#handoffs","title":"Handoffs","text":""},{"location":"realtime/guide/#creating-handoffs","title":"Creating Handoffs","text":"<p>Handoffs allow transferring conversations between specialized agents.</p> <pre><code>from agents.realtime import realtime_handoff\n\n# Specialized agents\nbilling_agent = RealtimeAgent(\n    name=\"Billing Support\",\n    instructions=\"You specialize in billing and payment issues.\",\n)\n\ntechnical_agent = RealtimeAgent(\n    name=\"Technical Support\",\n    instructions=\"You handle technical troubleshooting.\",\n)\n\n# Main agent with handoffs\nmain_agent = RealtimeAgent(\n    name=\"Customer Service\",\n    instructions=\"You are the main customer service agent. Hand off to specialists when needed.\",\n    handoffs=[\n        realtime_handoff(billing_agent, tool_description=\"Transfer to billing support\"),\n        realtime_handoff(technical_agent, tool_description=\"Transfer to technical support\"),\n    ]\n)\n</code></pre>"},{"location":"realtime/guide/#event-handling","title":"Event handling","text":"<p>The session streams events that you can listen to by iterating over the session object. Events include audio output chunks, transcription results, tool execution start and end, agent handoffs, and errors. Key events to handle include:</p> <ul> <li>audio: Raw audio data from the agent's response</li> <li>audio_end: Agent finished speaking</li> <li>audio_interrupted: User interrupted the agent</li> <li>tool_start/tool_end: Tool execution lifecycle</li> <li>handoff: Agent handoff occurred</li> <li>error: Error occurred during processing</li> </ul> <p>For complete event details, see <code>RealtimeSessionEvent</code>.</p>"},{"location":"realtime/guide/#guardrails","title":"Guardrails","text":"<p>Only output guardrails are supported for realtime agents. These guardrails are debounced and run periodically (not on every word) to avoid performance issues during real-time generation. The default debounce length is 100 characters, but this is configurable.</p> <p>When a guardrail is triggered, it generates a <code>guardrail_tripped</code> event and can interrupt the agent's current response. The debounce behavior helps balance safety with real-time performance requirements. Unlike text agents, realtime agents do not raise an Exception when guardrails are tripped.</p>"},{"location":"realtime/guide/#audio-processing","title":"Audio processing","text":"<p>Send audio to the session using <code>session.send_audio(audio_bytes)</code> or send text using <code>session.send_message()</code>.</p> <p>For audio output, listen for <code>audio</code> events and play the audio data through your preferred audio library. Make sure to listen for <code>audio_interrupted</code> events to stop playback immediately and clear any queued audio when the user interrupts the agent.</p>"},{"location":"realtime/guide/#direct-model-access","title":"Direct model access","text":"<p>You can access the underlying model to add custom listeners or perform advanced operations:</p> <pre><code># Add a custom listener to the model\nsession.model.add_listener(my_custom_listener)\n</code></pre> <p>This gives you direct access to the [<code>RealtimeModel</code>][agents.realtime.model.RealtimeModel] interface for advanced use cases where you need lower-level control over the connection.</p>"},{"location":"realtime/guide/#examples","title":"Examples","text":"<p>For complete working examples, check out the examples/realtime directory which includes demos with and without UI components.</p>"},{"location":"realtime/quickstart/","title":"Quickstart","text":"<p>Realtime agents enable voice conversations with your AI agents using OpenAI's Realtime API. This guide walks you through creating your first realtime voice agent.</p> <p>Beta feature</p> <p>Realtime agents are in beta. Expect some breaking changes as we improve the implementation.</p>"},{"location":"realtime/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>OpenAI API key</li> <li>Basic familiarity with the OpenAI Agents SDK</li> </ul>"},{"location":"realtime/quickstart/#installation","title":"Installation","text":"<p>If you haven't already, install the OpenAI Agents SDK:</p> <pre><code>pip install openai-agents\n</code></pre>"},{"location":"realtime/quickstart/#creating-your-first-realtime-agent","title":"Creating your first realtime agent","text":""},{"location":"realtime/quickstart/#1-import-required-components","title":"1. Import required components","text":"<pre><code>import asyncio\nfrom agents.realtime import RealtimeAgent, RealtimeRunner\n</code></pre>"},{"location":"realtime/quickstart/#2-create-a-realtime-agent","title":"2. Create a realtime agent","text":"<pre><code>agent = RealtimeAgent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful voice assistant. Keep your responses conversational and friendly.\",\n)\n</code></pre>"},{"location":"realtime/quickstart/#3-set-up-the-runner","title":"3. Set up the runner","text":"<pre><code>runner = RealtimeRunner(\n    starting_agent=agent,\n    config={\n        \"model_settings\": {\n            \"model_name\": \"gpt-4o-realtime-preview\",\n            \"voice\": \"alloy\",\n            \"modalities\": [\"text\", \"audio\"],\n        }\n    }\n)\n</code></pre>"},{"location":"realtime/quickstart/#4-start-a-session","title":"4. Start a session","text":"<pre><code>async def main():\n    # Start the realtime session\n    session = await runner.run()\n\n    async with session:\n        # Send a text message to start the conversation\n        await session.send_message(\"Hello! How are you today?\")\n\n        # The agent will stream back audio in real-time (not shown in this example)\n        # Listen for events from the session\n        async for event in session:\n            if event.type == \"response.audio_transcript.done\":\n                print(f\"Assistant: {event.transcript}\")\n            elif event.type == \"conversation.item.input_audio_transcription.completed\":\n                print(f\"User: {event.transcript}\")\n\n# Run the session\nasyncio.run(main())\n</code></pre>"},{"location":"realtime/quickstart/#complete-example","title":"Complete example","text":"<p>Here's a complete working example:</p> <pre><code>import asyncio\nfrom agents.realtime import RealtimeAgent, RealtimeRunner\n\nasync def main():\n    # Create the agent\n    agent = RealtimeAgent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful voice assistant. Keep responses brief and conversational.\",\n    )\n\n    # Set up the runner with configuration\n    runner = RealtimeRunner(\n        starting_agent=agent,\n        config={\n            \"model_settings\": {\n                \"model_name\": \"gpt-4o-realtime-preview\",\n                \"voice\": \"alloy\",\n                \"modalities\": [\"text\", \"audio\"],\n                \"input_audio_transcription\": {\n                    \"model\": \"whisper-1\"\n                },\n                \"turn_detection\": {\n                    \"type\": \"server_vad\",\n                    \"threshold\": 0.5,\n                    \"prefix_padding_ms\": 300,\n                    \"silence_duration_ms\": 200\n                }\n            }\n        }\n    )\n\n    # Start the session\n    session = await runner.run()\n\n    async with session:\n        print(\"Session started! The agent will stream audio responses in real-time.\")\n\n        # Process events\n        async for event in session:\n            if event.type == \"response.audio_transcript.done\":\n                print(f\"Assistant: {event.transcript}\")\n            elif event.type == \"conversation.item.input_audio_transcription.completed\":\n                print(f\"User: {event.transcript}\")\n            elif event.type == \"error\":\n                print(f\"Error: {event.error}\")\n                break\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"realtime/quickstart/#configuration-options","title":"Configuration options","text":""},{"location":"realtime/quickstart/#model-settings","title":"Model settings","text":"<ul> <li><code>model_name</code>: Choose from available realtime models (e.g., <code>gpt-4o-realtime-preview</code>)</li> <li><code>voice</code>: Select voice (<code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, <code>shimmer</code>)</li> <li><code>modalities</code>: Enable text and/or audio (<code>[\"text\", \"audio\"]</code>)</li> </ul>"},{"location":"realtime/quickstart/#audio-settings","title":"Audio settings","text":"<ul> <li><code>input_audio_format</code>: Format for input audio (<code>pcm16</code>, <code>g711_ulaw</code>, <code>g711_alaw</code>)</li> <li><code>output_audio_format</code>: Format for output audio</li> <li><code>input_audio_transcription</code>: Transcription configuration</li> </ul>"},{"location":"realtime/quickstart/#turn-detection","title":"Turn detection","text":"<ul> <li><code>type</code>: Detection method (<code>server_vad</code>, <code>semantic_vad</code>)</li> <li><code>threshold</code>: Voice activity threshold (0.0-1.0)</li> <li><code>silence_duration_ms</code>: Silence duration to detect turn end</li> <li><code>prefix_padding_ms</code>: Audio padding before speech</li> </ul>"},{"location":"realtime/quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Learn more about realtime agents</li> <li>Check out working examples in the examples/realtime folder</li> <li>Add tools to your agent</li> <li>Implement handoffs between agents</li> <li>Set up guardrails for safety</li> </ul>"},{"location":"realtime/quickstart/#authentication","title":"Authentication","text":"<p>Make sure your OpenAI API key is set in your environment:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Or pass it directly when creating the session:</p> <pre><code>session = await runner.run(model_config={\"api_key\": \"your-api-key\"})\n</code></pre>"},{"location":"ref/","title":"Agents module","text":""},{"location":"ref/#agents.set_default_openai_key","title":"set_default_openai_key","text":"<pre><code>set_default_openai_key(\n    key: str, use_for_tracing: bool = True\n) -&gt; None\n</code></pre> <p>Set the default OpenAI API key to use for LLM requests (and optionally tracing(). This is only necessary if the OPENAI_API_KEY environment variable is not already set.</p> <p>If provided, this key will be used instead of the OPENAI_API_KEY environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The OpenAI key to use.</p> required <code>use_for_tracing</code> <code>bool</code> <p>Whether to also use this key to send traces to OpenAI. Defaults to True If False, you'll either need to set the OPENAI_API_KEY environment variable or call set_tracing_export_api_key() with the API key you want to use for tracing.</p> <code>True</code> Source code in <code>src/agents/__init__.py</code> <pre><code>def set_default_openai_key(key: str, use_for_tracing: bool = True) -&gt; None:\n    \"\"\"Set the default OpenAI API key to use for LLM requests (and optionally tracing(). This is\n    only necessary if the OPENAI_API_KEY environment variable is not already set.\n\n    If provided, this key will be used instead of the OPENAI_API_KEY environment variable.\n\n    Args:\n        key: The OpenAI key to use.\n        use_for_tracing: Whether to also use this key to send traces to OpenAI. Defaults to True\n            If False, you'll either need to set the OPENAI_API_KEY environment variable or call\n            set_tracing_export_api_key() with the API key you want to use for tracing.\n    \"\"\"\n    _config.set_default_openai_key(key, use_for_tracing)\n</code></pre>"},{"location":"ref/#agents.set_default_openai_client","title":"set_default_openai_client","text":"<pre><code>set_default_openai_client(\n    client: AsyncOpenAI, use_for_tracing: bool = True\n) -&gt; None\n</code></pre> <p>Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this client will be used instead of the default OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use.</p> required <code>use_for_tracing</code> <code>bool</code> <p>Whether to use the API key from this client for uploading traces. If False, you'll either need to set the OPENAI_API_KEY environment variable or call set_tracing_export_api_key() with the API key you want to use for tracing.</p> <code>True</code> Source code in <code>src/agents/__init__.py</code> <pre><code>def set_default_openai_client(client: AsyncOpenAI, use_for_tracing: bool = True) -&gt; None:\n    \"\"\"Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this\n    client will be used instead of the default OpenAI client.\n\n    Args:\n        client: The OpenAI client to use.\n        use_for_tracing: Whether to use the API key from this client for uploading traces. If False,\n            you'll either need to set the OPENAI_API_KEY environment variable or call\n            set_tracing_export_api_key() with the API key you want to use for tracing.\n    \"\"\"\n    _config.set_default_openai_client(client, use_for_tracing)\n</code></pre>"},{"location":"ref/#agents.set_default_openai_api","title":"set_default_openai_api","text":"<pre><code>set_default_openai_api(\n    api: Literal[\"chat_completions\", \"responses\"],\n) -&gt; None\n</code></pre> <p>Set the default API to use for OpenAI LLM requests. By default, we will use the responses API but you can set this to use the chat completions API instead.</p> Source code in <code>src/agents/__init__.py</code> <pre><code>def set_default_openai_api(api: Literal[\"chat_completions\", \"responses\"]) -&gt; None:\n    \"\"\"Set the default API to use for OpenAI LLM requests. By default, we will use the responses API\n    but you can set this to use the chat completions API instead.\n    \"\"\"\n    _config.set_default_openai_api(api)\n</code></pre>"},{"location":"ref/#agents.set_tracing_export_api_key","title":"set_tracing_export_api_key","text":"<pre><code>set_tracing_export_api_key(api_key: str) -&gt; None\n</code></pre> <p>Set the OpenAI API key for the backend exporter.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_export_api_key(api_key: str) -&gt; None:\n    \"\"\"\n    Set the OpenAI API key for the backend exporter.\n    \"\"\"\n    default_exporter().set_api_key(api_key)\n</code></pre>"},{"location":"ref/#agents.set_tracing_disabled","title":"set_tracing_disabled","text":"<pre><code>set_tracing_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Set whether tracing is globally disabled.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_disabled(disabled: bool) -&gt; None:\n    \"\"\"\n    Set whether tracing is globally disabled.\n    \"\"\"\n    get_trace_provider().set_disabled(disabled)\n</code></pre>"},{"location":"ref/#agents.set_trace_processors","title":"set_trace_processors","text":"<pre><code>set_trace_processors(\n    processors: list[TracingProcessor],\n) -&gt; None\n</code></pre> <p>Set the list of trace processors. This will replace the current list of processors.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_trace_processors(processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"\n    Set the list of trace processors. This will replace the current list of processors.\n    \"\"\"\n    get_trace_provider().set_processors(processors)\n</code></pre>"},{"location":"ref/#agents.enable_verbose_stdout_logging","title":"enable_verbose_stdout_logging","text":"<pre><code>enable_verbose_stdout_logging()\n</code></pre> <p>Enables verbose logging to stdout. This is useful for debugging.</p> Source code in <code>src/agents/__init__.py</code> <pre><code>def enable_verbose_stdout_logging():\n    \"\"\"Enables verbose logging to stdout. This is useful for debugging.\"\"\"\n    logger = logging.getLogger(\"openai.agents\")\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n</code></pre>"},{"location":"ref/agent/","title":"<code>Agents</code>","text":""},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputFunction","title":"ToolsToFinalOutputFunction  <code>module-attribute</code>","text":"<pre><code>ToolsToFinalOutputFunction: TypeAlias = Callable[\n    [RunContextWrapper[TContext], list[FunctionToolResult]],\n    MaybeAwaitable[ToolsToFinalOutputResult],\n]\n</code></pre> <p>A function that takes a run context and a list of tool results, and returns a <code>ToolsToFinalOutputResult</code>.</p>"},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputResult","title":"ToolsToFinalOutputResult  <code>dataclass</code>","text":"Source code in <code>src/agents/agent.py</code> <pre><code>@dataclass\nclass ToolsToFinalOutputResult:\n    is_final_output: bool\n    \"\"\"Whether this is the final output. If False, the LLM will run again and receive the tool call\n    output.\n    \"\"\"\n\n    final_output: Any | None = None\n    \"\"\"The final output. Can be None if `is_final_output` is False, otherwise must match the\n    `output_type` of the agent.\n    \"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputResult.is_final_output","title":"is_final_output  <code>instance-attribute</code>","text":"<pre><code>is_final_output: bool\n</code></pre> <p>Whether this is the final output. If False, the LLM will run again and receive the tool call output.</p>"},{"location":"ref/agent/#agents.agent.ToolsToFinalOutputResult.final_output","title":"final_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>final_output: Any | None = None\n</code></pre> <p>The final output. Can be None if <code>is_final_output</code> is False, otherwise must match the <code>output_type</code> of the agent.</p>"},{"location":"ref/agent/#agents.agent.StopAtTools","title":"StopAtTools","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/agents/agent.py</code> <pre><code>class StopAtTools(TypedDict):\n    stop_at_tool_names: list[str]\n    \"\"\"A list of tool names, any of which will stop the agent from running further.\"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.StopAtTools.stop_at_tool_names","title":"stop_at_tool_names  <code>instance-attribute</code>","text":"<pre><code>stop_at_tool_names: list[str]\n</code></pre> <p>A list of tool names, any of which will stop the agent from running further.</p>"},{"location":"ref/agent/#agents.agent.MCPConfig","title":"MCPConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>class MCPConfig(TypedDict):\n    \"\"\"Configuration for MCP servers.\"\"\"\n\n    convert_schemas_to_strict: NotRequired[bool]\n    \"\"\"If True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a\n    best-effort conversion, so some schemas may not be convertible. Defaults to False.\n    \"\"\"\n</code></pre>"},{"location":"ref/agent/#agents.agent.MCPConfig.convert_schemas_to_strict","title":"convert_schemas_to_strict  <code>instance-attribute</code>","text":"<pre><code>convert_schemas_to_strict: NotRequired[bool]\n</code></pre> <p>If True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a best-effort conversion, so some schemas may not be convertible. Defaults to False.</p>"},{"location":"ref/agent/#agents.agent.AgentBase","title":"AgentBase  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Base class for <code>Agent</code> and <code>RealtimeAgent</code>.</p> Source code in <code>src/agents/agent.py</code> <pre><code>@dataclass\nclass AgentBase(Generic[TContext]):\n    \"\"\"Base class for `Agent` and `RealtimeAgent`.\"\"\"\n\n    name: str\n    \"\"\"The name of the agent.\"\"\"\n\n    handoff_description: str | None = None\n    \"\"\"A description of the agent. This is used when the agent is used as a handoff, so that an\n    LLM knows what it does and when to invoke it.\n    \"\"\"\n\n    tools: list[Tool] = field(default_factory=list)\n    \"\"\"A list of tools that the agent can use.\"\"\"\n\n    mcp_servers: list[MCPServer] = field(default_factory=list)\n    \"\"\"A list of [Model Context Protocol](https://modelcontextprotocol.io/) servers that\n    the agent can use. Every time the agent runs, it will include tools from these servers in the\n    list of available tools.\n\n    NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call\n    `server.connect()` before passing it to the agent, and `server.cleanup()` when the server is no\n    longer needed.\n    \"\"\"\n\n    mcp_config: MCPConfig = field(default_factory=lambda: MCPConfig())\n    \"\"\"Configuration for MCP servers.\"\"\"\n\n    async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n        \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n        convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n        return await MCPUtil.get_all_function_tools(\n            self.mcp_servers, convert_schemas_to_strict, run_context, self\n        )\n\n    async def get_all_tools(self, run_context: RunContextWrapper[Any]) -&gt; list[Tool]:\n        \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n        mcp_tools = await self.get_mcp_tools(run_context)\n\n        async def _check_tool_enabled(tool: Tool) -&gt; bool:\n            if not isinstance(tool, FunctionTool):\n                return True\n\n            attr = tool.is_enabled\n            if isinstance(attr, bool):\n                return attr\n            res = attr(run_context, self)\n            if inspect.isawaitable(res):\n                return bool(await res)\n            return bool(res)\n\n        results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n        enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n        return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent/#agents.agent.AgentBase.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the agent.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.handoff_description","title":"handoff_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_description: str | None = None\n</code></pre> <p>A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[Tool] = field(default_factory=list)\n</code></pre> <p>A list of tools that the agent can use.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: list[MCPServer] = field(default_factory=list)\n</code></pre> <p>A list of Model Context Protocol servers that the agent can use. Every time the agent runs, it will include tools from these servers in the list of available tools.</p> <p>NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call <code>server.connect()</code> before passing it to the agent, and <code>server.cleanup()</code> when the server is no longer needed.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.mcp_config","title":"mcp_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_config: MCPConfig = field(\n    default_factory=lambda: MCPConfig()\n)\n</code></pre> <p>Configuration for MCP servers.</p>"},{"location":"ref/agent/#agents.agent.AgentBase.get_mcp_tools","title":"get_mcp_tools  <code>async</code>","text":"<pre><code>get_mcp_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>Fetches the available tools from the MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n    convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n    return await MCPUtil.get_all_function_tools(\n        self.mcp_servers, convert_schemas_to_strict, run_context, self\n    )\n</code></pre>"},{"location":"ref/agent/#agents.agent.AgentBase.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    run_context: RunContextWrapper[Any],\n) -&gt; list[Tool]\n</code></pre> <p>All agent tools, including MCP tools and function tools.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_all_tools(self, run_context: RunContextWrapper[Any]) -&gt; list[Tool]:\n    \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n    mcp_tools = await self.get_mcp_tools(run_context)\n\n    async def _check_tool_enabled(tool: Tool) -&gt; bool:\n        if not isinstance(tool, FunctionTool):\n            return True\n\n        attr = tool.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(run_context, self)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n    enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n    return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent","title":"Agent  <code>dataclass</code>","text":"<p>               Bases: <code>AgentBase</code>, <code>Generic[TContext]</code></p> <p>An agent is an AI model configured with instructions, tools, guardrails, handoffs and more.</p> <p>We strongly recommend passing <code>instructions</code>, which is the \"system prompt\" for the agent. In addition, you can pass <code>handoff_description</code>, which is a human-readable description of the agent, used when the agent is used inside tools/handoffs.</p> <p>Agents are generic on the context type. The context is a (mutable) object you create. It is passed to tool functions, handoffs, guardrails, etc.</p> <p>See <code>AgentBase</code> for base parameters that are shared with <code>RealtimeAgent</code>s.</p> Source code in <code>src/agents/agent.py</code> <pre><code>@dataclass\nclass Agent(AgentBase, Generic[TContext]):\n    \"\"\"An agent is an AI model configured with instructions, tools, guardrails, handoffs and more.\n\n    We strongly recommend passing `instructions`, which is the \"system prompt\" for the agent. In\n    addition, you can pass `handoff_description`, which is a human-readable description of the\n    agent, used when the agent is used inside tools/handoffs.\n\n    Agents are generic on the context type. The context is a (mutable) object you create. It is\n    passed to tool functions, handoffs, guardrails, etc.\n\n    See `AgentBase` for base parameters that are shared with `RealtimeAgent`s.\n    \"\"\"\n\n    instructions: (\n        str\n        | Callable[\n            [RunContextWrapper[TContext], Agent[TContext]],\n            MaybeAwaitable[str],\n        ]\n        | None\n    ) = None\n    \"\"\"The instructions for the agent. Will be used as the \"system prompt\" when this agent is\n    invoked. Describes what the agent should do, and how it responds.\n\n    Can either be a string, or a function that dynamically generates instructions for the agent. If\n    you provide a function, it will be called with the context and the agent instance. It must\n    return a string.\n    \"\"\"\n\n    prompt: Prompt | DynamicPromptFunction | None = None\n    \"\"\"A prompt object (or a function that returns a Prompt). Prompts allow you to dynamically\n    configure the instructions, tools and other config for an agent outside of your code. Only\n    usable with OpenAI models, using the Responses API.\n    \"\"\"\n\n    handoffs: list[Agent[Any] | Handoff[TContext, Any]] = field(default_factory=list)\n    \"\"\"Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs,\n    and the agent can choose to delegate to them if relevant. Allows for separation of concerns and\n    modularity.\n    \"\"\"\n\n    model: str | Model | None = None\n    \"\"\"The model implementation to use when invoking the LLM.\n\n    By default, if not set, the agent will use the default model configured in\n    `openai_provider.DEFAULT_MODEL` (currently \"gpt-4o\").\n    \"\"\"\n\n    model_settings: ModelSettings = field(default_factory=ModelSettings)\n    \"\"\"Configures model-specific tuning parameters (e.g. temperature, top_p).\n    \"\"\"\n\n    input_guardrails: list[InputGuardrail[TContext]] = field(default_factory=list)\n    \"\"\"A list of checks that run in parallel to the agent's execution, before generating a\n    response. Runs only if the agent is the first agent in the chain.\n    \"\"\"\n\n    output_guardrails: list[OutputGuardrail[TContext]] = field(default_factory=list)\n    \"\"\"A list of checks that run on the final output of the agent, after generating a response.\n    Runs only if the agent produces a final output.\n    \"\"\"\n\n    output_type: type[Any] | AgentOutputSchemaBase | None = None\n    \"\"\"The type of the output object. If not provided, the output will be `str`. In most cases,\n    you should pass a regular Python type (e.g. a dataclass, Pydantic model, TypedDict, etc).\n    You can customize this in two ways:\n    1. If you want non-strict schemas, pass `AgentOutputSchema(MyClass, strict_json_schema=False)`.\n    2. If you want to use a custom JSON schema (i.e. without using the SDK's automatic schema)\n       creation, subclass and pass an `AgentOutputSchemaBase` subclass.\n    \"\"\"\n\n    hooks: AgentHooks[TContext] | None = None\n    \"\"\"A class that receives callbacks on various lifecycle events for this agent.\n    \"\"\"\n\n    tool_use_behavior: (\n        Literal[\"run_llm_again\", \"stop_on_first_tool\"] | StopAtTools | ToolsToFinalOutputFunction\n    ) = \"run_llm_again\"\n    \"\"\"This lets you configure how tool use is handled.\n    - \"run_llm_again\": The default behavior. Tools are run, and then the LLM receives the results\n        and gets to respond.\n    - \"stop_on_first_tool\": The output of the first tool call is used as the final output. This\n        means that the LLM does not process the result of the tool call.\n    - A list of tool names: The agent will stop running if any of the tools in the list are called.\n        The final output will be the output of the first matching tool call. The LLM does not\n        process the result of the tool call.\n    - A function: If you pass a function, it will be called with the run context and the list of\n      tool results. It must return a `ToolsToFinalOutputResult`, which determines whether the tool\n      calls result in a final output.\n\n      NOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search,\n      web search, etc are always processed by the LLM.\n    \"\"\"\n\n    reset_tool_choice: bool = True\n    \"\"\"Whether to reset the tool choice to the default value after a tool has been called. Defaults\n    to True. This ensures that the agent doesn't enter an infinite loop of tool usage.\"\"\"\n\n    def clone(self, **kwargs: Any) -&gt; Agent[TContext]:\n        \"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n        ```\n        new_agent = agent.clone(instructions=\"New instructions\")\n        ```\n        \"\"\"\n        return dataclasses.replace(self, **kwargs)\n\n    def as_tool(\n        self,\n        tool_name: str | None,\n        tool_description: str | None,\n        custom_output_extractor: Callable[[RunResult], Awaitable[str]] | None = None,\n    ) -&gt; Tool:\n        \"\"\"Transform this agent into a tool, callable by other agents.\n\n        This is different from handoffs in two ways:\n        1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\n           receives generated input.\n        2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\n           called as a tool, and the conversation is continued by the original agent.\n\n        Args:\n            tool_name: The name of the tool. If not provided, the agent's name will be used.\n            tool_description: The description of the tool, which should indicate what it does and\n                when to use it.\n            custom_output_extractor: A function that extracts the output from the agent. If not\n                provided, the last message from the agent will be used.\n        \"\"\"\n\n        @function_tool(\n            name_override=tool_name or _transforms.transform_string_function_style(self.name),\n            description_override=tool_description or \"\",\n        )\n        async def run_agent(context: RunContextWrapper, input: str) -&gt; str:\n            from .run import Runner\n\n            output = await Runner.run(\n                starting_agent=self,\n                input=input,\n                context=context.context,\n            )\n            if custom_output_extractor:\n                return await custom_output_extractor(output)\n\n            return ItemHelpers.text_message_outputs(output.new_items)\n\n        return run_agent\n\n    async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n        \"\"\"Get the system prompt for the agent.\"\"\"\n        if isinstance(self.instructions, str):\n            return self.instructions\n        elif callable(self.instructions):\n            if inspect.iscoroutinefunction(self.instructions):\n                return await cast(Awaitable[str], self.instructions(run_context, self))\n            else:\n                return cast(str, self.instructions(run_context, self))\n        elif self.instructions is not None:\n            logger.error(f\"Instructions must be a string or a function, got {self.instructions}\")\n\n        return None\n\n    async def get_prompt(\n        self, run_context: RunContextWrapper[TContext]\n    ) -&gt; ResponsePromptParam | None:\n        \"\"\"Get the prompt for the agent.\"\"\"\n        return await PromptUtil.to_model_input(self.prompt, run_context, self)\n\n    async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n        \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n        convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n        return await MCPUtil.get_all_function_tools(\n            self.mcp_servers, convert_schemas_to_strict, run_context, self\n        )\n\n    async def get_all_tools(self, run_context: RunContextWrapper[Any]) -&gt; list[Tool]:\n        \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n        mcp_tools = await self.get_mcp_tools(run_context)\n\n        async def _check_tool_enabled(tool: Tool) -&gt; bool:\n            if not isinstance(tool, FunctionTool):\n                return True\n\n            attr = tool.is_enabled\n            if isinstance(attr, bool):\n                return attr\n            res = attr(run_context, self)\n            if inspect.isawaitable(res):\n                return bool(await res)\n            return bool(res)\n\n        results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n        enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n        return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: (\n    str\n    | Callable[\n        [RunContextWrapper[TContext], Agent[TContext]],\n        MaybeAwaitable[str],\n    ]\n    | None\n) = None\n</code></pre> <p>The instructions for the agent. Will be used as the \"system prompt\" when this agent is invoked. Describes what the agent should do, and how it responds.</p> <p>Can either be a string, or a function that dynamically generates instructions for the agent. If you provide a function, it will be called with the context and the agent instance. It must return a string.</p>"},{"location":"ref/agent/#agents.agent.Agent.prompt","title":"prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt: Prompt | DynamicPromptFunction | None = None\n</code></pre> <p>A prompt object (or a function that returns a Prompt). Prompts allow you to dynamically configure the instructions, tools and other config for an agent outside of your code. Only usable with OpenAI models, using the Responses API.</p>"},{"location":"ref/agent/#agents.agent.Agent.handoffs","title":"handoffs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoffs: list[Agent[Any] | Handoff[TContext, Any]] = field(\n    default_factory=list\n)\n</code></pre> <p>Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs, and the agent can choose to delegate to them if relevant. Allows for separation of concerns and modularity.</p>"},{"location":"ref/agent/#agents.agent.Agent.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str | Model | None = None\n</code></pre> <p>The model implementation to use when invoking the LLM.</p> <p>By default, if not set, the agent will use the default model configured in <code>openai_provider.DEFAULT_MODEL</code> (currently \"gpt-4o\").</p>"},{"location":"ref/agent/#agents.agent.Agent.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettings = field(\n    default_factory=ModelSettings\n)\n</code></pre> <p>Configures model-specific tuning parameters (e.g. temperature, top_p).</p>"},{"location":"ref/agent/#agents.agent.Agent.input_guardrails","title":"input_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_guardrails: list[InputGuardrail[TContext]] = field(\n    default_factory=list\n)\n</code></pre> <p>A list of checks that run in parallel to the agent's execution, before generating a response. Runs only if the agent is the first agent in the chain.</p>"},{"location":"ref/agent/#agents.agent.Agent.output_guardrails","title":"output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrails: list[OutputGuardrail[TContext]] = field(\n    default_factory=list\n)\n</code></pre> <p>A list of checks that run on the final output of the agent, after generating a response. Runs only if the agent produces a final output.</p>"},{"location":"ref/agent/#agents.agent.Agent.output_type","title":"output_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_type: type[Any] | AgentOutputSchemaBase | None = None\n</code></pre> <p>The type of the output object. If not provided, the output will be <code>str</code>. In most cases, you should pass a regular Python type (e.g. a dataclass, Pydantic model, TypedDict, etc). You can customize this in two ways: 1. If you want non-strict schemas, pass <code>AgentOutputSchema(MyClass, strict_json_schema=False)</code>. 2. If you want to use a custom JSON schema (i.e. without using the SDK's automatic schema)    creation, subclass and pass an <code>AgentOutputSchemaBase</code> subclass.</p>"},{"location":"ref/agent/#agents.agent.Agent.hooks","title":"hooks  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hooks: AgentHooks[TContext] | None = None\n</code></pre> <p>A class that receives callbacks on various lifecycle events for this agent.</p>"},{"location":"ref/agent/#agents.agent.Agent.tool_use_behavior","title":"tool_use_behavior  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_use_behavior: (\n    Literal[\"run_llm_again\", \"stop_on_first_tool\"]\n    | StopAtTools\n    | ToolsToFinalOutputFunction\n) = \"run_llm_again\"\n</code></pre> <p>This lets you configure how tool use is handled. - \"run_llm_again\": The default behavior. Tools are run, and then the LLM receives the results     and gets to respond. - \"stop_on_first_tool\": The output of the first tool call is used as the final output. This     means that the LLM does not process the result of the tool call. - A list of tool names: The agent will stop running if any of the tools in the list are called.     The final output will be the output of the first matching tool call. The LLM does not     process the result of the tool call. - A function: If you pass a function, it will be called with the run context and the list of   tool results. It must return a <code>ToolsToFinalOutputResult</code>, which determines whether the tool   calls result in a final output.</p> <p>NOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search,   web search, etc are always processed by the LLM.</p>"},{"location":"ref/agent/#agents.agent.Agent.reset_tool_choice","title":"reset_tool_choice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reset_tool_choice: bool = True\n</code></pre> <p>Whether to reset the tool choice to the default value after a tool has been called. Defaults to True. This ensures that the agent doesn't enter an infinite loop of tool usage.</p>"},{"location":"ref/agent/#agents.agent.Agent.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the agent.</p>"},{"location":"ref/agent/#agents.agent.Agent.handoff_description","title":"handoff_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_description: str | None = None\n</code></pre> <p>A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it.</p>"},{"location":"ref/agent/#agents.agent.Agent.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[Tool] = field(default_factory=list)\n</code></pre> <p>A list of tools that the agent can use.</p>"},{"location":"ref/agent/#agents.agent.Agent.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: list[MCPServer] = field(default_factory=list)\n</code></pre> <p>A list of Model Context Protocol servers that the agent can use. Every time the agent runs, it will include tools from these servers in the list of available tools.</p> <p>NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call <code>server.connect()</code> before passing it to the agent, and <code>server.cleanup()</code> when the server is no longer needed.</p>"},{"location":"ref/agent/#agents.agent.Agent.mcp_config","title":"mcp_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_config: MCPConfig = field(\n    default_factory=lambda: MCPConfig()\n)\n</code></pre> <p>Configuration for MCP servers.</p>"},{"location":"ref/agent/#agents.agent.Agent.clone","title":"clone","text":"<pre><code>clone(**kwargs: Any) -&gt; Agent[TContext]\n</code></pre> <p>Make a copy of the agent, with the given arguments changed. For example, you could do: <pre><code>new_agent = agent.clone(instructions=\"New instructions\")\n</code></pre></p> Source code in <code>src/agents/agent.py</code> <pre><code>def clone(self, **kwargs: Any) -&gt; Agent[TContext]:\n    \"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n    ```\n    new_agent = agent.clone(instructions=\"New instructions\")\n    ```\n    \"\"\"\n    return dataclasses.replace(self, **kwargs)\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.as_tool","title":"as_tool","text":"<pre><code>as_tool(\n    tool_name: str | None,\n    tool_description: str | None,\n    custom_output_extractor: Callable[\n        [RunResult], Awaitable[str]\n    ]\n    | None = None,\n) -&gt; Tool\n</code></pre> <p>Transform this agent into a tool, callable by other agents.</p> <p>This is different from handoffs in two ways: 1. In handoffs, the new agent receives the conversation history. In this tool, the new agent    receives generated input. 2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is    called as a tool, and the conversation is continued by the original agent.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str | None</code> <p>The name of the tool. If not provided, the agent's name will be used.</p> required <code>tool_description</code> <code>str | None</code> <p>The description of the tool, which should indicate what it does and when to use it.</p> required <code>custom_output_extractor</code> <code>Callable[[RunResult], Awaitable[str]] | None</code> <p>A function that extracts the output from the agent. If not provided, the last message from the agent will be used.</p> <code>None</code> Source code in <code>src/agents/agent.py</code> <pre><code>def as_tool(\n    self,\n    tool_name: str | None,\n    tool_description: str | None,\n    custom_output_extractor: Callable[[RunResult], Awaitable[str]] | None = None,\n) -&gt; Tool:\n    \"\"\"Transform this agent into a tool, callable by other agents.\n\n    This is different from handoffs in two ways:\n    1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\n       receives generated input.\n    2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\n       called as a tool, and the conversation is continued by the original agent.\n\n    Args:\n        tool_name: The name of the tool. If not provided, the agent's name will be used.\n        tool_description: The description of the tool, which should indicate what it does and\n            when to use it.\n        custom_output_extractor: A function that extracts the output from the agent. If not\n            provided, the last message from the agent will be used.\n    \"\"\"\n\n    @function_tool(\n        name_override=tool_name or _transforms.transform_string_function_style(self.name),\n        description_override=tool_description or \"\",\n    )\n    async def run_agent(context: RunContextWrapper, input: str) -&gt; str:\n        from .run import Runner\n\n        output = await Runner.run(\n            starting_agent=self,\n            input=input,\n            context=context.context,\n        )\n        if custom_output_extractor:\n            return await custom_output_extractor(output)\n\n        return ItemHelpers.text_message_outputs(output.new_items)\n\n    return run_agent\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_system_prompt","title":"get_system_prompt  <code>async</code>","text":"<pre><code>get_system_prompt(\n    run_context: RunContextWrapper[TContext],\n) -&gt; str | None\n</code></pre> <p>Get the system prompt for the agent.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n    \"\"\"Get the system prompt for the agent.\"\"\"\n    if isinstance(self.instructions, str):\n        return self.instructions\n    elif callable(self.instructions):\n        if inspect.iscoroutinefunction(self.instructions):\n            return await cast(Awaitable[str], self.instructions(run_context, self))\n        else:\n            return cast(str, self.instructions(run_context, self))\n    elif self.instructions is not None:\n        logger.error(f\"Instructions must be a string or a function, got {self.instructions}\")\n\n    return None\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    run_context: RunContextWrapper[TContext],\n) -&gt; ResponsePromptParam | None\n</code></pre> <p>Get the prompt for the agent.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_prompt(\n    self, run_context: RunContextWrapper[TContext]\n) -&gt; ResponsePromptParam | None:\n    \"\"\"Get the prompt for the agent.\"\"\"\n    return await PromptUtil.to_model_input(self.prompt, run_context, self)\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_mcp_tools","title":"get_mcp_tools  <code>async</code>","text":"<pre><code>get_mcp_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>Fetches the available tools from the MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n    convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n    return await MCPUtil.get_all_function_tools(\n        self.mcp_servers, convert_schemas_to_strict, run_context, self\n    )\n</code></pre>"},{"location":"ref/agent/#agents.agent.Agent.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    run_context: RunContextWrapper[Any],\n) -&gt; list[Tool]\n</code></pre> <p>All agent tools, including MCP tools and function tools.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_all_tools(self, run_context: RunContextWrapper[Any]) -&gt; list[Tool]:\n    \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n    mcp_tools = await self.get_mcp_tools(run_context)\n\n    async def _check_tool_enabled(tool: Tool) -&gt; bool:\n        if not isinstance(tool, FunctionTool):\n            return True\n\n        attr = tool.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(run_context, self)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n    enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n    return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/agent_output/","title":"<code>Agent output</code>","text":""},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase","title":"AgentOutputSchemaBase","text":"<p>               Bases: <code>ABC</code></p> <p>An object that captures the JSON schema of the output, as well as validating/parsing JSON produced by the LLM into the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>class AgentOutputSchemaBase(abc.ABC):\n    \"\"\"An object that captures the JSON schema of the output, as well as validating/parsing JSON\n    produced by the LLM into the output type.\n    \"\"\"\n\n    @abc.abstractmethod\n    def is_plain_text(self) -&gt; bool:\n        \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"The name of the output type.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def json_schema(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the JSON schema of the output. Will only be called if the output type is not\n        plain text.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def is_strict_json_schema(self) -&gt; bool:\n        \"\"\"Whether the JSON schema is in strict mode. Strict mode constrains the JSON schema\n        features, but guarantees valid JSON. See here for details:\n        https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def validate_json(self, json_str: str) -&gt; Any:\n        \"\"\"Validate a JSON string against the output type. You must return the validated object,\n        or raise a `ModelBehaviorError` if the JSON is invalid.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.is_plain_text","title":"is_plain_text  <code>abstractmethod</code>","text":"<pre><code>is_plain_text() -&gt; bool\n</code></pre> <p>Whether the output type is plain text (versus a JSON object).</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef is_plain_text(self) -&gt; bool:\n    \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.name","title":"name  <code>abstractmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>The name of the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef name(self) -&gt; str:\n    \"\"\"The name of the output type.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.json_schema","title":"json_schema  <code>abstractmethod</code>","text":"<pre><code>json_schema() -&gt; dict[str, Any]\n</code></pre> <p>Returns the JSON schema of the output. Will only be called if the output type is not plain text.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef json_schema(self) -&gt; dict[str, Any]:\n    \"\"\"Returns the JSON schema of the output. Will only be called if the output type is not\n    plain text.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.is_strict_json_schema","title":"is_strict_json_schema  <code>abstractmethod</code>","text":"<pre><code>is_strict_json_schema() -&gt; bool\n</code></pre> <p>Whether the JSON schema is in strict mode. Strict mode constrains the JSON schema features, but guarantees valid JSON. See here for details: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef is_strict_json_schema(self) -&gt; bool:\n    \"\"\"Whether the JSON schema is in strict mode. Strict mode constrains the JSON schema\n    features, but guarantees valid JSON. See here for details:\n    https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchemaBase.validate_json","title":"validate_json  <code>abstractmethod</code>","text":"<pre><code>validate_json(json_str: str) -&gt; Any\n</code></pre> <p>Validate a JSON string against the output type. You must return the validated object, or raise a <code>ModelBehaviorError</code> if the JSON is invalid.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@abc.abstractmethod\ndef validate_json(self, json_str: str) -&gt; Any:\n    \"\"\"Validate a JSON string against the output type. You must return the validated object,\n    or raise a `ModelBehaviorError` if the JSON is invalid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema","title":"AgentOutputSchema  <code>dataclass</code>","text":"<p>               Bases: <code>AgentOutputSchemaBase</code></p> <p>An object that captures the JSON schema of the output, as well as validating/parsing JSON produced by the LLM into the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>@dataclass(init=False)\nclass AgentOutputSchema(AgentOutputSchemaBase):\n    \"\"\"An object that captures the JSON schema of the output, as well as validating/parsing JSON\n    produced by the LLM into the output type.\n    \"\"\"\n\n    output_type: type[Any]\n    \"\"\"The type of the output.\"\"\"\n\n    _type_adapter: TypeAdapter[Any]\n    \"\"\"A type adapter that wraps the output type, so that we can validate JSON.\"\"\"\n\n    _is_wrapped: bool\n    \"\"\"Whether the output type is wrapped in a dictionary. This is generally done if the base\n    output type cannot be represented as a JSON Schema object.\n    \"\"\"\n\n    _output_schema: dict[str, Any]\n    \"\"\"The JSON schema of the output.\"\"\"\n\n    _strict_json_schema: bool\n    \"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\n    as it increases the likelihood of correct JSON input.\n    \"\"\"\n\n    def __init__(self, output_type: type[Any], strict_json_schema: bool = True):\n        \"\"\"\n        Args:\n            output_type: The type of the output.\n            strict_json_schema: Whether the JSON schema is in strict mode. We **strongly** recommend\n                setting this to True, as it increases the likelihood of correct JSON input.\n        \"\"\"\n        self.output_type = output_type\n        self._strict_json_schema = strict_json_schema\n\n        if output_type is None or output_type is str:\n            self._is_wrapped = False\n            self._type_adapter = TypeAdapter(output_type)\n            self._output_schema = self._type_adapter.json_schema()\n            return\n\n        # We should wrap for things that are not plain text, and for things that would definitely\n        # not be a JSON Schema object.\n        self._is_wrapped = not _is_subclass_of_base_model_or_dict(output_type)\n\n        if self._is_wrapped:\n            OutputType = TypedDict(\n                \"OutputType\",\n                {\n                    _WRAPPER_DICT_KEY: output_type,  # type: ignore\n                },\n            )\n            self._type_adapter = TypeAdapter(OutputType)\n            self._output_schema = self._type_adapter.json_schema()\n        else:\n            self._type_adapter = TypeAdapter(output_type)\n            self._output_schema = self._type_adapter.json_schema()\n\n        if self._strict_json_schema:\n            try:\n                self._output_schema = ensure_strict_json_schema(self._output_schema)\n            except UserError as e:\n                raise UserError(\n                    \"Strict JSON schema is enabled, but the output type is not valid. \"\n                    \"Either make the output type strict, \"\n                    \"or wrap your type with AgentOutputSchema(your_type, strict_json_schema=False)\"\n                ) from e\n\n    def is_plain_text(self) -&gt; bool:\n        \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n        return self.output_type is None or self.output_type is str\n\n    def is_strict_json_schema(self) -&gt; bool:\n        \"\"\"Whether the JSON schema is in strict mode.\"\"\"\n        return self._strict_json_schema\n\n    def json_schema(self) -&gt; dict[str, Any]:\n        \"\"\"The JSON schema of the output type.\"\"\"\n        if self.is_plain_text():\n            raise UserError(\"Output type is plain text, so no JSON schema is available\")\n        return self._output_schema\n\n    def validate_json(self, json_str: str) -&gt; Any:\n        \"\"\"Validate a JSON string against the output type. Returns the validated object, or raises\n        a `ModelBehaviorError` if the JSON is invalid.\n        \"\"\"\n        validated = _json.validate_json(json_str, self._type_adapter, partial=False)\n        if self._is_wrapped:\n            if not isinstance(validated, dict):\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Invalid JSON\",\n                        data={\"details\": f\"Expected a dict, got {type(validated)}\"},\n                    )\n                )\n                raise ModelBehaviorError(\n                    f\"Expected a dict, got {type(validated)} for JSON: {json_str}\"\n                )\n\n            if _WRAPPER_DICT_KEY not in validated:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Invalid JSON\",\n                        data={\"details\": f\"Could not find key {_WRAPPER_DICT_KEY} in JSON\"},\n                    )\n                )\n                raise ModelBehaviorError(\n                    f\"Could not find key {_WRAPPER_DICT_KEY} in JSON: {json_str}\"\n                )\n            return validated[_WRAPPER_DICT_KEY]\n        return validated\n\n    def name(self) -&gt; str:\n        \"\"\"The name of the output type.\"\"\"\n        return _type_to_str(self.output_type)\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.output_type","title":"output_type  <code>instance-attribute</code>","text":"<pre><code>output_type: type[Any] = output_type\n</code></pre> <p>The type of the output.</p>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.__init__","title":"__init__","text":"<pre><code>__init__(\n    output_type: type[Any], strict_json_schema: bool = True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>type[Any]</code> <p>The type of the output.</p> required <code>strict_json_schema</code> <code>bool</code> <p>Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p> <code>True</code> Source code in <code>src/agents/agent_output.py</code> <pre><code>def __init__(self, output_type: type[Any], strict_json_schema: bool = True):\n    \"\"\"\n    Args:\n        output_type: The type of the output.\n        strict_json_schema: Whether the JSON schema is in strict mode. We **strongly** recommend\n            setting this to True, as it increases the likelihood of correct JSON input.\n    \"\"\"\n    self.output_type = output_type\n    self._strict_json_schema = strict_json_schema\n\n    if output_type is None or output_type is str:\n        self._is_wrapped = False\n        self._type_adapter = TypeAdapter(output_type)\n        self._output_schema = self._type_adapter.json_schema()\n        return\n\n    # We should wrap for things that are not plain text, and for things that would definitely\n    # not be a JSON Schema object.\n    self._is_wrapped = not _is_subclass_of_base_model_or_dict(output_type)\n\n    if self._is_wrapped:\n        OutputType = TypedDict(\n            \"OutputType\",\n            {\n                _WRAPPER_DICT_KEY: output_type,  # type: ignore\n            },\n        )\n        self._type_adapter = TypeAdapter(OutputType)\n        self._output_schema = self._type_adapter.json_schema()\n    else:\n        self._type_adapter = TypeAdapter(output_type)\n        self._output_schema = self._type_adapter.json_schema()\n\n    if self._strict_json_schema:\n        try:\n            self._output_schema = ensure_strict_json_schema(self._output_schema)\n        except UserError as e:\n            raise UserError(\n                \"Strict JSON schema is enabled, but the output type is not valid. \"\n                \"Either make the output type strict, \"\n                \"or wrap your type with AgentOutputSchema(your_type, strict_json_schema=False)\"\n            ) from e\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.is_plain_text","title":"is_plain_text","text":"<pre><code>is_plain_text() -&gt; bool\n</code></pre> <p>Whether the output type is plain text (versus a JSON object).</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def is_plain_text(self) -&gt; bool:\n    \"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\n    return self.output_type is None or self.output_type is str\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.is_strict_json_schema","title":"is_strict_json_schema","text":"<pre><code>is_strict_json_schema() -&gt; bool\n</code></pre> <p>Whether the JSON schema is in strict mode.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def is_strict_json_schema(self) -&gt; bool:\n    \"\"\"Whether the JSON schema is in strict mode.\"\"\"\n    return self._strict_json_schema\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.json_schema","title":"json_schema","text":"<pre><code>json_schema() -&gt; dict[str, Any]\n</code></pre> <p>The JSON schema of the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def json_schema(self) -&gt; dict[str, Any]:\n    \"\"\"The JSON schema of the output type.\"\"\"\n    if self.is_plain_text():\n        raise UserError(\"Output type is plain text, so no JSON schema is available\")\n    return self._output_schema\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.validate_json","title":"validate_json","text":"<pre><code>validate_json(json_str: str) -&gt; Any\n</code></pre> <p>Validate a JSON string against the output type. Returns the validated object, or raises a <code>ModelBehaviorError</code> if the JSON is invalid.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def validate_json(self, json_str: str) -&gt; Any:\n    \"\"\"Validate a JSON string against the output type. Returns the validated object, or raises\n    a `ModelBehaviorError` if the JSON is invalid.\n    \"\"\"\n    validated = _json.validate_json(json_str, self._type_adapter, partial=False)\n    if self._is_wrapped:\n        if not isinstance(validated, dict):\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Invalid JSON\",\n                    data={\"details\": f\"Expected a dict, got {type(validated)}\"},\n                )\n            )\n            raise ModelBehaviorError(\n                f\"Expected a dict, got {type(validated)} for JSON: {json_str}\"\n            )\n\n        if _WRAPPER_DICT_KEY not in validated:\n            _error_tracing.attach_error_to_current_span(\n                SpanError(\n                    message=\"Invalid JSON\",\n                    data={\"details\": f\"Could not find key {_WRAPPER_DICT_KEY} in JSON\"},\n                )\n            )\n            raise ModelBehaviorError(\n                f\"Could not find key {_WRAPPER_DICT_KEY} in JSON: {json_str}\"\n            )\n        return validated[_WRAPPER_DICT_KEY]\n    return validated\n</code></pre>"},{"location":"ref/agent_output/#agents.agent_output.AgentOutputSchema.name","title":"name","text":"<pre><code>name() -&gt; str\n</code></pre> <p>The name of the output type.</p> Source code in <code>src/agents/agent_output.py</code> <pre><code>def name(self) -&gt; str:\n    \"\"\"The name of the output type.\"\"\"\n    return _type_to_str(self.output_type)\n</code></pre>"},{"location":"ref/exceptions/","title":"<code>Exceptions</code>","text":""},{"location":"ref/exceptions/#agents.exceptions.RunErrorDetails","title":"RunErrorDetails  <code>dataclass</code>","text":"<p>Data collected from an agent run when an exception occurs.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>@dataclass\nclass RunErrorDetails:\n    \"\"\"Data collected from an agent run when an exception occurs.\"\"\"\n\n    input: str | list[TResponseInputItem]\n    new_items: list[RunItem]\n    raw_responses: list[ModelResponse]\n    last_agent: Agent[Any]\n    context_wrapper: RunContextWrapper[Any]\n    input_guardrail_results: list[InputGuardrailResult]\n    output_guardrail_results: list[OutputGuardrailResult]\n\n    def __str__(self) -&gt; str:\n        return pretty_print_run_error_details(self)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.AgentsException","title":"AgentsException","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions in the Agents SDK.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class AgentsException(Exception):\n    \"\"\"Base class for all exceptions in the Agents SDK.\"\"\"\n\n    run_data: RunErrorDetails | None\n\n    def __init__(self, *args: object) -&gt; None:\n        super().__init__(*args)\n        self.run_data = None\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.MaxTurnsExceeded","title":"MaxTurnsExceeded","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the maximum number of turns is exceeded.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class MaxTurnsExceeded(AgentsException):\n    \"\"\"Exception raised when the maximum number of turns is exceeded.\"\"\"\n\n    message: str\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.ModelBehaviorError","title":"ModelBehaviorError","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the model does something unexpected, e.g. calling a tool that doesn't exist, or providing malformed JSON.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class ModelBehaviorError(AgentsException):\n    \"\"\"Exception raised when the model does something unexpected, e.g. calling a tool that doesn't\n    exist, or providing malformed JSON.\n    \"\"\"\n\n    message: str\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.UserError","title":"UserError","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the user makes an error using the SDK.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class UserError(AgentsException):\n    \"\"\"Exception raised when the user makes an error using the SDK.\"\"\"\n\n    message: str\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.InputGuardrailTripwireTriggered","title":"InputGuardrailTripwireTriggered","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when a guardrail tripwire is triggered.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class InputGuardrailTripwireTriggered(AgentsException):\n    \"\"\"Exception raised when a guardrail tripwire is triggered.\"\"\"\n\n    guardrail_result: InputGuardrailResult\n    \"\"\"The result data of the guardrail that was triggered.\"\"\"\n\n    def __init__(self, guardrail_result: InputGuardrailResult):\n        self.guardrail_result = guardrail_result\n        super().__init__(\n            f\"Guardrail {guardrail_result.guardrail.__class__.__name__} triggered tripwire\"\n        )\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.InputGuardrailTripwireTriggered.guardrail_result","title":"guardrail_result  <code>instance-attribute</code>","text":"<pre><code>guardrail_result: InputGuardrailResult = guardrail_result\n</code></pre> <p>The result data of the guardrail that was triggered.</p>"},{"location":"ref/exceptions/#agents.exceptions.OutputGuardrailTripwireTriggered","title":"OutputGuardrailTripwireTriggered","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when a guardrail tripwire is triggered.</p> Source code in <code>src/agents/exceptions.py</code> <pre><code>class OutputGuardrailTripwireTriggered(AgentsException):\n    \"\"\"Exception raised when a guardrail tripwire is triggered.\"\"\"\n\n    guardrail_result: OutputGuardrailResult\n    \"\"\"The result data of the guardrail that was triggered.\"\"\"\n\n    def __init__(self, guardrail_result: OutputGuardrailResult):\n        self.guardrail_result = guardrail_result\n        super().__init__(\n            f\"Guardrail {guardrail_result.guardrail.__class__.__name__} triggered tripwire\"\n        )\n</code></pre>"},{"location":"ref/exceptions/#agents.exceptions.OutputGuardrailTripwireTriggered.guardrail_result","title":"guardrail_result  <code>instance-attribute</code>","text":"<pre><code>guardrail_result: OutputGuardrailResult = guardrail_result\n</code></pre> <p>The result data of the guardrail that was triggered.</p>"},{"location":"ref/function_schema/","title":"<code>Function schema</code>","text":""},{"location":"ref/function_schema/#agents.function_schema.FuncSchema","title":"FuncSchema  <code>dataclass</code>","text":"<p>Captures the schema for a python function, in preparation for sending it to an LLM as a tool.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>@dataclass\nclass FuncSchema:\n    \"\"\"\n    Captures the schema for a python function, in preparation for sending it to an LLM as a tool.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the function.\"\"\"\n    description: str | None\n    \"\"\"The description of the function.\"\"\"\n    params_pydantic_model: type[BaseModel]\n    \"\"\"A Pydantic model that represents the function's parameters.\"\"\"\n    params_json_schema: dict[str, Any]\n    \"\"\"The JSON schema for the function's parameters, derived from the Pydantic model.\"\"\"\n    signature: inspect.Signature\n    \"\"\"The signature of the function.\"\"\"\n    takes_context: bool = False\n    \"\"\"Whether the function takes a RunContextWrapper argument (must be the first argument).\"\"\"\n    strict_json_schema: bool = True\n    \"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\n    as it increases the likelihood of correct JSON input.\"\"\"\n\n    def to_call_args(self, data: BaseModel) -&gt; tuple[list[Any], dict[str, Any]]:\n        \"\"\"\n        Converts validated data from the Pydantic model into (args, kwargs), suitable for calling\n        the original function.\n        \"\"\"\n        positional_args: list[Any] = []\n        keyword_args: dict[str, Any] = {}\n        seen_var_positional = False\n\n        # Use enumerate() so we can skip the first parameter if it's context.\n        for idx, (name, param) in enumerate(self.signature.parameters.items()):\n            # If the function takes a RunContextWrapper and this is the first parameter, skip it.\n            if self.takes_context and idx == 0:\n                continue\n\n            value = getattr(data, name, None)\n            if param.kind == param.VAR_POSITIONAL:\n                # e.g. *args: extend positional args and mark that *args is now seen\n                positional_args.extend(value or [])\n                seen_var_positional = True\n            elif param.kind == param.VAR_KEYWORD:\n                # e.g. **kwargs handling\n                keyword_args.update(value or {})\n            elif param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n                # Before *args, add to positional args. After *args, add to keyword args.\n                if not seen_var_positional:\n                    positional_args.append(value)\n                else:\n                    keyword_args[name] = value\n            else:\n                # For KEYWORD_ONLY parameters, always use keyword args.\n                keyword_args[name] = value\n        return positional_args, keyword_args\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str | None\n</code></pre> <p>The description of the function.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.params_pydantic_model","title":"params_pydantic_model  <code>instance-attribute</code>","text":"<pre><code>params_pydantic_model: type[BaseModel]\n</code></pre> <p>A Pydantic model that represents the function's parameters.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.params_json_schema","title":"params_json_schema  <code>instance-attribute</code>","text":"<pre><code>params_json_schema: dict[str, Any]\n</code></pre> <p>The JSON schema for the function's parameters, derived from the Pydantic model.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.signature","title":"signature  <code>instance-attribute</code>","text":"<pre><code>signature: Signature\n</code></pre> <p>The signature of the function.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.takes_context","title":"takes_context  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>takes_context: bool = False\n</code></pre> <p>Whether the function takes a RunContextWrapper argument (must be the first argument).</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.strict_json_schema","title":"strict_json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strict_json_schema: bool = True\n</code></pre> <p>Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncSchema.to_call_args","title":"to_call_args","text":"<pre><code>to_call_args(\n    data: BaseModel,\n) -&gt; tuple[list[Any], dict[str, Any]]\n</code></pre> <p>Converts validated data from the Pydantic model into (args, kwargs), suitable for calling the original function.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>def to_call_args(self, data: BaseModel) -&gt; tuple[list[Any], dict[str, Any]]:\n    \"\"\"\n    Converts validated data from the Pydantic model into (args, kwargs), suitable for calling\n    the original function.\n    \"\"\"\n    positional_args: list[Any] = []\n    keyword_args: dict[str, Any] = {}\n    seen_var_positional = False\n\n    # Use enumerate() so we can skip the first parameter if it's context.\n    for idx, (name, param) in enumerate(self.signature.parameters.items()):\n        # If the function takes a RunContextWrapper and this is the first parameter, skip it.\n        if self.takes_context and idx == 0:\n            continue\n\n        value = getattr(data, name, None)\n        if param.kind == param.VAR_POSITIONAL:\n            # e.g. *args: extend positional args and mark that *args is now seen\n            positional_args.extend(value or [])\n            seen_var_positional = True\n        elif param.kind == param.VAR_KEYWORD:\n            # e.g. **kwargs handling\n            keyword_args.update(value or {})\n        elif param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n            # Before *args, add to positional args. After *args, add to keyword args.\n            if not seen_var_positional:\n                positional_args.append(value)\n            else:\n                keyword_args[name] = value\n        else:\n            # For KEYWORD_ONLY parameters, always use keyword args.\n            keyword_args[name] = value\n    return positional_args, keyword_args\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation","title":"FuncDocumentation  <code>dataclass</code>","text":"<p>Contains metadata about a python function, extracted from its docstring.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>@dataclass\nclass FuncDocumentation:\n    \"\"\"Contains metadata about a python function, extracted from its docstring.\"\"\"\n\n    name: str\n    \"\"\"The name of the function, via `__name__`.\"\"\"\n    description: str | None\n    \"\"\"The description of the function, derived from the docstring.\"\"\"\n    param_descriptions: dict[str, str] | None\n    \"\"\"The parameter descriptions of the function, derived from the docstring.\"\"\"\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the function, via <code>__name__</code>.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str | None\n</code></pre> <p>The description of the function, derived from the docstring.</p>"},{"location":"ref/function_schema/#agents.function_schema.FuncDocumentation.param_descriptions","title":"param_descriptions  <code>instance-attribute</code>","text":"<pre><code>param_descriptions: dict[str, str] | None\n</code></pre> <p>The parameter descriptions of the function, derived from the docstring.</p>"},{"location":"ref/function_schema/#agents.function_schema.generate_func_documentation","title":"generate_func_documentation","text":"<pre><code>generate_func_documentation(\n    func: Callable[..., Any],\n    style: DocstringStyle | None = None,\n) -&gt; FuncDocumentation\n</code></pre> <p>Extracts metadata from a function docstring, in preparation for sending it to an LLM as a tool.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to extract documentation from.</p> required <code>style</code> <code>DocstringStyle | None</code> <p>The style of the docstring to use for parsing. If not provided, we will attempt to auto-detect the style.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncDocumentation</code> <p>A FuncDocumentation object containing the function's name, description, and parameter</p> <code>FuncDocumentation</code> <p>descriptions.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>def generate_func_documentation(\n    func: Callable[..., Any], style: DocstringStyle | None = None\n) -&gt; FuncDocumentation:\n    \"\"\"\n    Extracts metadata from a function docstring, in preparation for sending it to an LLM as a tool.\n\n    Args:\n        func: The function to extract documentation from.\n        style: The style of the docstring to use for parsing. If not provided, we will attempt to\n            auto-detect the style.\n\n    Returns:\n        A FuncDocumentation object containing the function's name, description, and parameter\n        descriptions.\n    \"\"\"\n    name = func.__name__\n    doc = inspect.getdoc(func)\n    if not doc:\n        return FuncDocumentation(name=name, description=None, param_descriptions=None)\n\n    with _suppress_griffe_logging():\n        docstring = Docstring(doc, lineno=1, parser=style or _detect_docstring_style(doc))\n        parsed = docstring.parse()\n\n    description: str | None = next(\n        (section.value for section in parsed if section.kind == DocstringSectionKind.text), None\n    )\n\n    param_descriptions: dict[str, str] = {\n        param.name: param.description\n        for section in parsed\n        if section.kind == DocstringSectionKind.parameters\n        for param in section.value\n    }\n\n    return FuncDocumentation(\n        name=func.__name__,\n        description=description,\n        param_descriptions=param_descriptions or None,\n    )\n</code></pre>"},{"location":"ref/function_schema/#agents.function_schema.function_schema","title":"function_schema","text":"<pre><code>function_schema(\n    func: Callable[..., Any],\n    docstring_style: DocstringStyle | None = None,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    use_docstring_info: bool = True,\n    strict_json_schema: bool = True,\n) -&gt; FuncSchema\n</code></pre> <p>Given a python function, extracts a <code>FuncSchema</code> from it, capturing the name, description, parameter descriptions, and other metadata.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to extract the schema from.</p> required <code>docstring_style</code> <code>DocstringStyle | None</code> <p>The style of the docstring to use for parsing. If not provided, we will attempt to auto-detect the style.</p> <code>None</code> <code>name_override</code> <code>str | None</code> <p>If provided, use this name instead of the function's <code>__name__</code>.</p> <code>None</code> <code>description_override</code> <code>str | None</code> <p>If provided, use this description instead of the one derived from the docstring.</p> <code>None</code> <code>use_docstring_info</code> <code>bool</code> <p>If True, uses the docstring to generate the description and parameter descriptions.</p> <code>True</code> <code>strict_json_schema</code> <code>bool</code> <p>Whether the JSON schema is in strict mode. If True, we'll ensure that the schema adheres to the \"strict\" standard the OpenAI API expects. We strongly recommend setting this to True, as it increases the likelihood of the LLM providing correct JSON input.</p> <code>True</code> <p>Returns:</p> Type Description <code>FuncSchema</code> <p>A <code>FuncSchema</code> object containing the function's name, description, parameter descriptions,</p> <code>FuncSchema</code> <p>and other metadata.</p> Source code in <code>src/agents/function_schema.py</code> <pre><code>def function_schema(\n    func: Callable[..., Any],\n    docstring_style: DocstringStyle | None = None,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    use_docstring_info: bool = True,\n    strict_json_schema: bool = True,\n) -&gt; FuncSchema:\n    \"\"\"\n    Given a python function, extracts a `FuncSchema` from it, capturing the name, description,\n    parameter descriptions, and other metadata.\n\n    Args:\n        func: The function to extract the schema from.\n        docstring_style: The style of the docstring to use for parsing. If not provided, we will\n            attempt to auto-detect the style.\n        name_override: If provided, use this name instead of the function's `__name__`.\n        description_override: If provided, use this description instead of the one derived from the\n            docstring.\n        use_docstring_info: If True, uses the docstring to generate the description and parameter\n            descriptions.\n        strict_json_schema: Whether the JSON schema is in strict mode. If True, we'll ensure that\n            the schema adheres to the \"strict\" standard the OpenAI API expects. We **strongly**\n            recommend setting this to True, as it increases the likelihood of the LLM providing\n            correct JSON input.\n\n    Returns:\n        A `FuncSchema` object containing the function's name, description, parameter descriptions,\n        and other metadata.\n    \"\"\"\n\n    # 1. Grab docstring info\n    if use_docstring_info:\n        doc_info = generate_func_documentation(func, docstring_style)\n        param_descs = doc_info.param_descriptions or {}\n    else:\n        doc_info = None\n        param_descs = {}\n\n    # Ensure name_override takes precedence even if docstring info is disabled.\n    func_name = name_override or (doc_info.name if doc_info else func.__name__)\n\n    # 2. Inspect function signature and get type hints\n    sig = inspect.signature(func)\n    type_hints = get_type_hints(func)\n    params = list(sig.parameters.items())\n    takes_context = False\n    filtered_params = []\n\n    if params:\n        first_name, first_param = params[0]\n        # Prefer the evaluated type hint if available\n        ann = type_hints.get(first_name, first_param.annotation)\n        if ann != inspect._empty:\n            origin = get_origin(ann) or ann\n            if origin is RunContextWrapper or origin is ToolContext:\n                takes_context = True  # Mark that the function takes context\n            else:\n                filtered_params.append((first_name, first_param))\n        else:\n            filtered_params.append((first_name, first_param))\n\n    # For parameters other than the first, raise error if any use RunContextWrapper or ToolContext.\n    for name, param in params[1:]:\n        ann = type_hints.get(name, param.annotation)\n        if ann != inspect._empty:\n            origin = get_origin(ann) or ann\n            if origin is RunContextWrapper or origin is ToolContext:\n                raise UserError(\n                    f\"RunContextWrapper/ToolContext param found at non-first position in function\"\n                    f\" {func.__name__}\"\n                )\n        filtered_params.append((name, param))\n\n    # We will collect field definitions for create_model as a dict:\n    #   field_name -&gt; (type_annotation, default_value_or_Field(...))\n    fields: dict[str, Any] = {}\n\n    for name, param in filtered_params:\n        ann = type_hints.get(name, param.annotation)\n        default = param.default\n\n        # If there's no type hint, assume `Any`\n        if ann == inspect._empty:\n            ann = Any\n\n        # If a docstring param description exists, use it\n        field_description = param_descs.get(name, None)\n\n        # Handle different parameter kinds\n        if param.kind == param.VAR_POSITIONAL:\n            # e.g. *args: extend positional args\n            if get_origin(ann) is tuple:\n                # e.g. def foo(*args: tuple[int, ...]) -&gt; treat as List[int]\n                args_of_tuple = get_args(ann)\n                if len(args_of_tuple) == 2 and args_of_tuple[1] is Ellipsis:\n                    ann = list[args_of_tuple[0]]  # type: ignore\n                else:\n                    ann = list[Any]\n            else:\n                # If user wrote *args: int, treat as List[int]\n                ann = list[ann]  # type: ignore\n\n            # Default factory to empty list\n            fields[name] = (\n                ann,\n                Field(default_factory=list, description=field_description),  # type: ignore\n            )\n\n        elif param.kind == param.VAR_KEYWORD:\n            # **kwargs handling\n            if get_origin(ann) is dict:\n                # e.g. def foo(**kwargs: dict[str, int])\n                dict_args = get_args(ann)\n                if len(dict_args) == 2:\n                    ann = dict[dict_args[0], dict_args[1]]  # type: ignore\n                else:\n                    ann = dict[str, Any]\n            else:\n                # e.g. def foo(**kwargs: int) -&gt; Dict[str, int]\n                ann = dict[str, ann]  # type: ignore\n\n            fields[name] = (\n                ann,\n                Field(default_factory=dict, description=field_description),  # type: ignore\n            )\n\n        else:\n            # Normal parameter\n            if default == inspect._empty:\n                # Required field\n                fields[name] = (\n                    ann,\n                    Field(..., description=field_description),\n                )\n            elif isinstance(default, FieldInfo):\n                # Parameter with a default value that is a Field(...)\n                fields[name] = (\n                    ann,\n                    FieldInfo.merge_field_infos(\n                        default, description=field_description or default.description\n                    ),\n                )\n            else:\n                # Parameter with a default value\n                fields[name] = (\n                    ann,\n                    Field(default=default, description=field_description),\n                )\n\n    # 3. Dynamically build a Pydantic model\n    dynamic_model = create_model(f\"{func_name}_args\", __base__=BaseModel, **fields)\n\n    # 4. Build JSON schema from that model\n    json_schema = dynamic_model.model_json_schema()\n    if strict_json_schema:\n        json_schema = ensure_strict_json_schema(json_schema)\n\n    # 5. Return as a FuncSchema dataclass\n    return FuncSchema(\n        name=func_name,\n        # Ensure description_override takes precedence even if docstring info is disabled.\n        description=description_override or (doc_info.description if doc_info else None),\n        params_pydantic_model=dynamic_model,\n        params_json_schema=json_schema,\n        signature=sig,\n        takes_context=takes_context,\n        strict_json_schema=strict_json_schema,\n    )\n</code></pre>"},{"location":"ref/guardrail/","title":"<code>Guardrails</code>","text":""},{"location":"ref/guardrail/#agents.guardrail.GuardrailFunctionOutput","title":"GuardrailFunctionOutput  <code>dataclass</code>","text":"<p>The output of a guardrail function.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass GuardrailFunctionOutput:\n    \"\"\"The output of a guardrail function.\"\"\"\n\n    output_info: Any\n    \"\"\"\n    Optional information about the guardrail's output. For example, the guardrail could include\n    information about the checks it performed and granular results.\n    \"\"\"\n\n    tripwire_triggered: bool\n    \"\"\"\n    Whether the tripwire was triggered. If triggered, the agent's execution will be halted.\n    \"\"\"\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.GuardrailFunctionOutput.output_info","title":"output_info  <code>instance-attribute</code>","text":"<pre><code>output_info: Any\n</code></pre> <p>Optional information about the guardrail's output. For example, the guardrail could include information about the checks it performed and granular results.</p>"},{"location":"ref/guardrail/#agents.guardrail.GuardrailFunctionOutput.tripwire_triggered","title":"tripwire_triggered  <code>instance-attribute</code>","text":"<pre><code>tripwire_triggered: bool\n</code></pre> <p>Whether the tripwire was triggered. If triggered, the agent's execution will be halted.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrailResult","title":"InputGuardrailResult  <code>dataclass</code>","text":"<p>The result of a guardrail run.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass InputGuardrailResult:\n    \"\"\"The result of a guardrail run.\"\"\"\n\n    guardrail: InputGuardrail[Any]\n    \"\"\"\n    The guardrail that was run.\n    \"\"\"\n\n    output: GuardrailFunctionOutput\n    \"\"\"The output of the guardrail function.\"\"\"\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrailResult.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: InputGuardrail[Any]\n</code></pre> <p>The guardrail that was run.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrailResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: GuardrailFunctionOutput\n</code></pre> <p>The output of the guardrail function.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult","title":"OutputGuardrailResult  <code>dataclass</code>","text":"<p>The result of a guardrail run.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass OutputGuardrailResult:\n    \"\"\"The result of a guardrail run.\"\"\"\n\n    guardrail: OutputGuardrail[Any]\n    \"\"\"\n    The guardrail that was run.\n    \"\"\"\n\n    agent_output: Any\n    \"\"\"\n    The output of the agent that was checked by the guardrail.\n    \"\"\"\n\n    agent: Agent[Any]\n    \"\"\"\n    The agent that was checked by the guardrail.\n    \"\"\"\n\n    output: GuardrailFunctionOutput\n    \"\"\"The output of the guardrail function.\"\"\"\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.guardrail","title":"guardrail  <code>instance-attribute</code>","text":"<pre><code>guardrail: OutputGuardrail[Any]\n</code></pre> <p>The guardrail that was run.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.agent_output","title":"agent_output  <code>instance-attribute</code>","text":"<pre><code>agent_output: Any\n</code></pre> <p>The output of the agent that was checked by the guardrail.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent that was checked by the guardrail.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrailResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: GuardrailFunctionOutput\n</code></pre> <p>The output of the guardrail function.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail","title":"InputGuardrail  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Input guardrails are checks that run in parallel to the agent's execution. They can be used to do things like: - Check if input messages are off-topic - Take over control of the agent's execution if an unexpected input is detected</p> <p>You can use the <code>@input_guardrail()</code> decorator to turn a function into an <code>InputGuardrail</code>, or create an <code>InputGuardrail</code> manually.</p> <p>Guardrails return a <code>GuardrailResult</code>. If <code>result.tripwire_triggered</code> is <code>True</code>, the agent execution will immediately stop and a <code>InputGuardrailTripwireTriggered</code> exception will be raised</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass InputGuardrail(Generic[TContext]):\n    \"\"\"Input guardrails are checks that run in parallel to the agent's execution.\n    They can be used to do things like:\n    - Check if input messages are off-topic\n    - Take over control of the agent's execution if an unexpected input is detected\n\n    You can use the `@input_guardrail()` decorator to turn a function into an `InputGuardrail`, or\n    create an `InputGuardrail` manually.\n\n    Guardrails return a `GuardrailResult`. If `result.tripwire_triggered` is `True`, the agent\n    execution will immediately stop and a `InputGuardrailTripwireTriggered` exception will be raised\n    \"\"\"\n\n    guardrail_function: Callable[\n        [RunContextWrapper[TContext], Agent[Any], str | list[TResponseInputItem]],\n        MaybeAwaitable[GuardrailFunctionOutput],\n    ]\n    \"\"\"A function that receives the agent input and the context, and returns a\n     `GuardrailResult`. The result marks whether the tripwire was triggered, and can optionally\n     include information about the guardrail's output.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"The name of the guardrail, used for tracing. If not provided, we'll use the guardrail\n    function's name.\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        if self.name:\n            return self.name\n\n        return self.guardrail_function.__name__\n\n    async def run(\n        self,\n        agent: Agent[Any],\n        input: str | list[TResponseInputItem],\n        context: RunContextWrapper[TContext],\n    ) -&gt; InputGuardrailResult:\n        if not callable(self.guardrail_function):\n            raise UserError(f\"Guardrail function must be callable, got {self.guardrail_function}\")\n\n        output = self.guardrail_function(context, agent, input)\n        if inspect.isawaitable(output):\n            return InputGuardrailResult(\n                guardrail=self,\n                output=await output,\n            )\n\n        return InputGuardrailResult(\n            guardrail=self,\n            output=output,\n        )\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail.guardrail_function","title":"guardrail_function  <code>instance-attribute</code>","text":"<pre><code>guardrail_function: Callable[\n    [\n        RunContextWrapper[TContext],\n        Agent[Any],\n        str | list[TResponseInputItem],\n    ],\n    MaybeAwaitable[GuardrailFunctionOutput],\n]\n</code></pre> <p>A function that receives the agent input and the context, and returns a <code>GuardrailResult</code>. The result marks whether the tripwire was triggered, and can optionally include information about the guardrail's output.</p>"},{"location":"ref/guardrail/#agents.guardrail.InputGuardrail.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>The name of the guardrail, used for tracing. If not provided, we'll use the guardrail function's name.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrail","title":"OutputGuardrail  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>Output guardrails are checks that run on the final output of an agent. They can be used to do check if the output passes certain validation criteria</p> <p>You can use the <code>@output_guardrail()</code> decorator to turn a function into an <code>OutputGuardrail</code>, or create an <code>OutputGuardrail</code> manually.</p> <p>Guardrails return a <code>GuardrailResult</code>. If <code>result.tripwire_triggered</code> is <code>True</code>, a <code>OutputGuardrailTripwireTriggered</code> exception will be raised.</p> Source code in <code>src/agents/guardrail.py</code> <pre><code>@dataclass\nclass OutputGuardrail(Generic[TContext]):\n    \"\"\"Output guardrails are checks that run on the final output of an agent.\n    They can be used to do check if the output passes certain validation criteria\n\n    You can use the `@output_guardrail()` decorator to turn a function into an `OutputGuardrail`,\n    or create an `OutputGuardrail` manually.\n\n    Guardrails return a `GuardrailResult`. If `result.tripwire_triggered` is `True`, a\n    `OutputGuardrailTripwireTriggered` exception will be raised.\n    \"\"\"\n\n    guardrail_function: Callable[\n        [RunContextWrapper[TContext], Agent[Any], Any],\n        MaybeAwaitable[GuardrailFunctionOutput],\n    ]\n    \"\"\"A function that receives the final agent, its output, and the context, and returns a\n     `GuardrailResult`. The result marks whether the tripwire was triggered, and can optionally\n     include information about the guardrail's output.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"The name of the guardrail, used for tracing. If not provided, we'll use the guardrail\n    function's name.\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        if self.name:\n            return self.name\n\n        return self.guardrail_function.__name__\n\n    async def run(\n        self, context: RunContextWrapper[TContext], agent: Agent[Any], agent_output: Any\n    ) -&gt; OutputGuardrailResult:\n        if not callable(self.guardrail_function):\n            raise UserError(f\"Guardrail function must be callable, got {self.guardrail_function}\")\n\n        output = self.guardrail_function(context, agent, agent_output)\n        if inspect.isawaitable(output):\n            return OutputGuardrailResult(\n                guardrail=self,\n                agent=agent,\n                agent_output=agent_output,\n                output=await output,\n            )\n\n        return OutputGuardrailResult(\n            guardrail=self,\n            agent=agent,\n            agent_output=agent_output,\n            output=output,\n        )\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrail.guardrail_function","title":"guardrail_function  <code>instance-attribute</code>","text":"<pre><code>guardrail_function: Callable[\n    [RunContextWrapper[TContext], Agent[Any], Any],\n    MaybeAwaitable[GuardrailFunctionOutput],\n]\n</code></pre> <p>A function that receives the final agent, its output, and the context, and returns a <code>GuardrailResult</code>. The result marks whether the tripwire was triggered, and can optionally include information about the guardrail's output.</p>"},{"location":"ref/guardrail/#agents.guardrail.OutputGuardrail.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre> <p>The name of the guardrail, used for tracing. If not provided, we'll use the guardrail function's name.</p>"},{"location":"ref/guardrail/#agents.guardrail.input_guardrail","title":"input_guardrail","text":"<pre><code>input_guardrail(\n    func: _InputGuardrailFuncSync[TContext_co],\n) -&gt; InputGuardrail[TContext_co]\n</code></pre><pre><code>input_guardrail(\n    func: _InputGuardrailFuncAsync[TContext_co],\n) -&gt; InputGuardrail[TContext_co]\n</code></pre><pre><code>input_guardrail(\n    *, name: str | None = None\n) -&gt; Callable[\n    [\n        _InputGuardrailFuncSync[TContext_co]\n        | _InputGuardrailFuncAsync[TContext_co]\n    ],\n    InputGuardrail[TContext_co],\n]\n</code></pre> <pre><code>input_guardrail(\n    func: _InputGuardrailFuncSync[TContext_co]\n    | _InputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    InputGuardrail[TContext_co]\n    | Callable[\n        [\n            _InputGuardrailFuncSync[TContext_co]\n            | _InputGuardrailFuncAsync[TContext_co]\n        ],\n        InputGuardrail[TContext_co],\n    ]\n)\n</code></pre> <p>Decorator that transforms a sync or async function into an <code>InputGuardrail</code>. It can be used directly (no parentheses) or with keyword args, e.g.:</p> <pre><code>@input_guardrail\ndef my_sync_guardrail(...): ...\n\n@input_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\n</code></pre> Source code in <code>src/agents/guardrail.py</code> <pre><code>def input_guardrail(\n    func: _InputGuardrailFuncSync[TContext_co]\n    | _InputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    InputGuardrail[TContext_co]\n    | Callable[\n        [_InputGuardrailFuncSync[TContext_co] | _InputGuardrailFuncAsync[TContext_co]],\n        InputGuardrail[TContext_co],\n    ]\n):\n    \"\"\"\n    Decorator that transforms a sync or async function into an `InputGuardrail`.\n    It can be used directly (no parentheses) or with keyword args, e.g.:\n\n        @input_guardrail\n        def my_sync_guardrail(...): ...\n\n        @input_guardrail(name=\"guardrail_name\")\n        async def my_async_guardrail(...): ...\n    \"\"\"\n\n    def decorator(\n        f: _InputGuardrailFuncSync[TContext_co] | _InputGuardrailFuncAsync[TContext_co],\n    ) -&gt; InputGuardrail[TContext_co]:\n        return InputGuardrail(\n            guardrail_function=f,\n            # If not set, guardrail name uses the function\u2019s name by default.\n            name=name if name else f.__name__,\n        )\n\n    if func is not None:\n        # Decorator was used without parentheses\n        return decorator(func)\n\n    # Decorator used with keyword arguments\n    return decorator\n</code></pre>"},{"location":"ref/guardrail/#agents.guardrail.output_guardrail","title":"output_guardrail","text":"<pre><code>output_guardrail(\n    func: _OutputGuardrailFuncSync[TContext_co],\n) -&gt; OutputGuardrail[TContext_co]\n</code></pre><pre><code>output_guardrail(\n    func: _OutputGuardrailFuncAsync[TContext_co],\n) -&gt; OutputGuardrail[TContext_co]\n</code></pre><pre><code>output_guardrail(\n    *, name: str | None = None\n) -&gt; Callable[\n    [\n        _OutputGuardrailFuncSync[TContext_co]\n        | _OutputGuardrailFuncAsync[TContext_co]\n    ],\n    OutputGuardrail[TContext_co],\n]\n</code></pre> <pre><code>output_guardrail(\n    func: _OutputGuardrailFuncSync[TContext_co]\n    | _OutputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    OutputGuardrail[TContext_co]\n    | Callable[\n        [\n            _OutputGuardrailFuncSync[TContext_co]\n            | _OutputGuardrailFuncAsync[TContext_co]\n        ],\n        OutputGuardrail[TContext_co],\n    ]\n)\n</code></pre> <p>Decorator that transforms a sync or async function into an <code>OutputGuardrail</code>. It can be used directly (no parentheses) or with keyword args, e.g.:</p> <pre><code>@output_guardrail\ndef my_sync_guardrail(...): ...\n\n@output_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\n</code></pre> Source code in <code>src/agents/guardrail.py</code> <pre><code>def output_guardrail(\n    func: _OutputGuardrailFuncSync[TContext_co]\n    | _OutputGuardrailFuncAsync[TContext_co]\n    | None = None,\n    *,\n    name: str | None = None,\n) -&gt; (\n    OutputGuardrail[TContext_co]\n    | Callable[\n        [_OutputGuardrailFuncSync[TContext_co] | _OutputGuardrailFuncAsync[TContext_co]],\n        OutputGuardrail[TContext_co],\n    ]\n):\n    \"\"\"\n    Decorator that transforms a sync or async function into an `OutputGuardrail`.\n    It can be used directly (no parentheses) or with keyword args, e.g.:\n\n        @output_guardrail\n        def my_sync_guardrail(...): ...\n\n        @output_guardrail(name=\"guardrail_name\")\n        async def my_async_guardrail(...): ...\n    \"\"\"\n\n    def decorator(\n        f: _OutputGuardrailFuncSync[TContext_co] | _OutputGuardrailFuncAsync[TContext_co],\n    ) -&gt; OutputGuardrail[TContext_co]:\n        return OutputGuardrail(\n            guardrail_function=f,\n            # Guardrail name defaults to function name when not specified (None).\n            name=name if name else f.__name__,\n        )\n\n    if func is not None:\n        # Decorator was used without parentheses\n        return decorator(func)\n\n    # Decorator used with keyword arguments\n    return decorator\n</code></pre>"},{"location":"ref/handoffs/","title":"<code>Handoffs</code>","text":""},{"location":"ref/handoffs/#agents.handoffs.HandoffInputFilter","title":"HandoffInputFilter  <code>module-attribute</code>","text":"<pre><code>HandoffInputFilter: TypeAlias = Callable[\n    [HandoffInputData], HandoffInputData\n]\n</code></pre> <p>A function that filters the input data passed to the next agent.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData","title":"HandoffInputData  <code>dataclass</code>","text":"Source code in <code>src/agents/handoffs.py</code> <pre><code>@dataclass(frozen=True)\nclass HandoffInputData:\n    input_history: str | tuple[TResponseInputItem, ...]\n    \"\"\"\n    The input history before `Runner.run()` was called.\n    \"\"\"\n\n    pre_handoff_items: tuple[RunItem, ...]\n    \"\"\"\n    The items generated before the agent turn where the handoff was invoked.\n    \"\"\"\n\n    new_items: tuple[RunItem, ...]\n    \"\"\"\n    The new items generated during the current agent turn, including the item that triggered the\n    handoff and the tool output message representing the response from the handoff output.\n    \"\"\"\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.input_history","title":"input_history  <code>instance-attribute</code>","text":"<pre><code>input_history: str | tuple[TResponseInputItem, ...]\n</code></pre> <p>The input history before <code>Runner.run()</code> was called.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.pre_handoff_items","title":"pre_handoff_items  <code>instance-attribute</code>","text":"<pre><code>pre_handoff_items: tuple[RunItem, ...]\n</code></pre> <p>The items generated before the agent turn where the handoff was invoked.</p>"},{"location":"ref/handoffs/#agents.handoffs.HandoffInputData.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: tuple[RunItem, ...]\n</code></pre> <p>The new items generated during the current agent turn, including the item that triggered the handoff and the tool output message representing the response from the handoff output.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff","title":"Handoff  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>A handoff is when an agent delegates a task to another agent. For example, in a customer support scenario you might have a \"triage agent\" that determines which agent should handle the user's request, and sub-agents that specialize in different areas like billing, account management, etc.</p> Source code in <code>src/agents/handoffs.py</code> <pre><code>@dataclass\nclass Handoff(Generic[TContext, TAgent]):\n    \"\"\"A handoff is when an agent delegates a task to another agent.\n    For example, in a customer support scenario you might have a \"triage agent\" that determines\n    which agent should handle the user's request, and sub-agents that specialize in different\n    areas like billing, account management, etc.\n    \"\"\"\n\n    tool_name: str\n    \"\"\"The name of the tool that represents the handoff.\"\"\"\n\n    tool_description: str\n    \"\"\"The description of the tool that represents the handoff.\"\"\"\n\n    input_json_schema: dict[str, Any]\n    \"\"\"The JSON schema for the handoff input. Can be empty if the handoff does not take an input.\n    \"\"\"\n\n    on_invoke_handoff: Callable[[RunContextWrapper[Any], str], Awaitable[TAgent]]\n    \"\"\"The function that invokes the handoff. The parameters passed are:\n    1. The handoff run context\n    2. The arguments from the LLM, as a JSON string. Empty string if input_json_schema is empty.\n\n    Must return an agent.\n    \"\"\"\n\n    agent_name: str\n    \"\"\"The name of the agent that is being handed off to.\"\"\"\n\n    input_filter: HandoffInputFilter | None = None\n    \"\"\"A function that filters the inputs that are passed to the next agent. By default, the new\n    agent sees the entire conversation history. In some cases, you may want to filter inputs e.g.\n    to remove older inputs, or remove tools from existing inputs.\n\n    The function will receive the entire conversation history so far, including the input item\n    that triggered the handoff and a tool call output item representing the handoff tool's output.\n\n    You are free to modify the input history or new items as you see fit. The next agent that\n    runs will receive `handoff_input_data.all_items`.\n\n    IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The\n    items generated before will already have been streamed.\n    \"\"\"\n\n    strict_json_schema: bool = True\n    \"\"\"Whether the input JSON schema is in strict mode. We **strongly** recommend setting this to\n    True, as it increases the likelihood of correct JSON input.\n    \"\"\"\n\n    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]] = (\n        True\n    )\n    \"\"\"Whether the handoff is enabled. Either a bool or a Callable that takes the run context and\n    agent and returns whether the handoff is enabled. You can use this to dynamically enable/disable\n    a handoff based on your context/state.\"\"\"\n\n    def get_transfer_message(self, agent: AgentBase[Any]) -&gt; str:\n        return json.dumps({\"assistant\": agent.name})\n\n    @classmethod\n    def default_tool_name(cls, agent: AgentBase[Any]) -&gt; str:\n        return _transforms.transform_string_function_style(f\"transfer_to_{agent.name}\")\n\n    @classmethod\n    def default_tool_description(cls, agent: AgentBase[Any]) -&gt; str:\n        return (\n            f\"Handoff to the {agent.name} agent to handle the request. \"\n            f\"{agent.handoff_description or ''}\"\n        )\n</code></pre>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.tool_name","title":"tool_name  <code>instance-attribute</code>","text":"<pre><code>tool_name: str\n</code></pre> <p>The name of the tool that represents the handoff.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.tool_description","title":"tool_description  <code>instance-attribute</code>","text":"<pre><code>tool_description: str\n</code></pre> <p>The description of the tool that represents the handoff.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.input_json_schema","title":"input_json_schema  <code>instance-attribute</code>","text":"<pre><code>input_json_schema: dict[str, Any]\n</code></pre> <p>The JSON schema for the handoff input. Can be empty if the handoff does not take an input.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.on_invoke_handoff","title":"on_invoke_handoff  <code>instance-attribute</code>","text":"<pre><code>on_invoke_handoff: Callable[\n    [RunContextWrapper[Any], str], Awaitable[TAgent]\n]\n</code></pre> <p>The function that invokes the handoff. The parameters passed are: 1. The handoff run context 2. The arguments from the LLM, as a JSON string. Empty string if input_json_schema is empty.</p> <p>Must return an agent.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.agent_name","title":"agent_name  <code>instance-attribute</code>","text":"<pre><code>agent_name: str\n</code></pre> <p>The name of the agent that is being handed off to.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.input_filter","title":"input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_filter: HandoffInputFilter | None = None\n</code></pre> <p>A function that filters the inputs that are passed to the next agent. By default, the new agent sees the entire conversation history. In some cases, you may want to filter inputs e.g. to remove older inputs, or remove tools from existing inputs.</p> <p>The function will receive the entire conversation history so far, including the input item that triggered the handoff and a tool call output item representing the handoff tool's output.</p> <p>You are free to modify the input history or new items as you see fit. The next agent that runs will receive <code>handoff_input_data.all_items</code>.</p> <p>IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The items generated before will already have been streamed.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.strict_json_schema","title":"strict_json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strict_json_schema: bool = True\n</code></pre> <p>Whether the input JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p>"},{"location":"ref/handoffs/#agents.handoffs.Handoff.is_enabled","title":"is_enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_enabled: (\n    bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase[Any]],\n        MaybeAwaitable[bool],\n    ]\n) = True\n</code></pre> <p>Whether the handoff is enabled. Either a bool or a Callable that takes the run context and agent and returns whether the handoff is enabled. You can use this to dynamically enable/disable a handoff based on your context/state.</p>"},{"location":"ref/handoffs/#agents.handoffs.handoff","title":"handoff","text":"<pre><code>handoff(\n    agent: Agent[TContext],\n    *,\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre><pre><code>handoff(\n    agent: Agent[TContext],\n    *,\n    on_handoff: OnHandoffWithInput[THandoffInput],\n    input_type: type[THandoffInput],\n    tool_description_override: str | None = None,\n    tool_name_override: str | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre><pre><code>handoff(\n    agent: Agent[TContext],\n    *,\n    on_handoff: OnHandoffWithoutInput,\n    tool_description_override: str | None = None,\n    tool_name_override: str | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[Any]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre> <pre><code>handoff(\n    agent: Agent[TContext],\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    on_handoff: OnHandoffWithInput[THandoffInput]\n    | OnHandoffWithoutInput\n    | None = None,\n    input_type: type[THandoffInput] | None = None,\n    input_filter: Callable[\n        [HandoffInputData], HandoffInputData\n    ]\n    | None = None,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], Agent[TContext]],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]\n</code></pre> <p>Create a handoff from an agent.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[TContext]</code> <p>The agent to handoff to, or a function that returns an agent.</p> required <code>tool_name_override</code> <code>str | None</code> <p>Optional override for the name of the tool that represents the handoff.</p> <code>None</code> <code>tool_description_override</code> <code>str | None</code> <p>Optional override for the description of the tool that represents the handoff.</p> <code>None</code> <code>on_handoff</code> <code>OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None</code> <p>A function that runs when the handoff is invoked.</p> <code>None</code> <code>input_type</code> <code>type[THandoffInput] | None</code> <p>the type of the input to the handoff. If provided, the input will be validated against this type. Only relevant if you pass a function that takes an input.</p> <code>None</code> <code>input_filter</code> <code>Callable[[HandoffInputData], HandoffInputData] | None</code> <p>a function that filters the inputs that are passed to the next agent.</p> <code>None</code> <code>is_enabled</code> <code>bool | Callable[[RunContextWrapper[Any], Agent[TContext]], MaybeAwaitable[bool]]</code> <p>Whether the handoff is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the handoff is enabled. Disabled handoffs are hidden from the LLM at runtime.</p> <code>True</code> Source code in <code>src/agents/handoffs.py</code> <pre><code>def handoff(\n    agent: Agent[TContext],\n    tool_name_override: str | None = None,\n    tool_description_override: str | None = None,\n    on_handoff: OnHandoffWithInput[THandoffInput] | OnHandoffWithoutInput | None = None,\n    input_type: type[THandoffInput] | None = None,\n    input_filter: Callable[[HandoffInputData], HandoffInputData] | None = None,\n    is_enabled: bool\n    | Callable[[RunContextWrapper[Any], Agent[TContext]], MaybeAwaitable[bool]] = True,\n) -&gt; Handoff[TContext, Agent[TContext]]:\n    \"\"\"Create a handoff from an agent.\n\n    Args:\n        agent: The agent to handoff to, or a function that returns an agent.\n        tool_name_override: Optional override for the name of the tool that represents the handoff.\n        tool_description_override: Optional override for the description of the tool that\n            represents the handoff.\n        on_handoff: A function that runs when the handoff is invoked.\n        input_type: the type of the input to the handoff. If provided, the input will be validated\n            against this type. Only relevant if you pass a function that takes an input.\n        input_filter: a function that filters the inputs that are passed to the next agent.\n        is_enabled: Whether the handoff is enabled. Can be a bool or a callable that takes the run\n            context and agent and returns whether the handoff is enabled. Disabled handoffs are\n            hidden from the LLM at runtime.\n    \"\"\"\n    assert (on_handoff and input_type) or not (on_handoff and input_type), (\n        \"You must provide either both on_handoff and input_type, or neither\"\n    )\n    type_adapter: TypeAdapter[Any] | None\n    if input_type is not None:\n        assert callable(on_handoff), \"on_handoff must be callable\"\n        sig = inspect.signature(on_handoff)\n        if len(sig.parameters) != 2:\n            raise UserError(\"on_handoff must take two arguments: context and input\")\n\n        type_adapter = TypeAdapter(input_type)\n        input_json_schema = type_adapter.json_schema()\n    else:\n        type_adapter = None\n        input_json_schema = {}\n        if on_handoff is not None:\n            sig = inspect.signature(on_handoff)\n            if len(sig.parameters) != 1:\n                raise UserError(\"on_handoff must take one argument: context\")\n\n    async def _invoke_handoff(\n        ctx: RunContextWrapper[Any], input_json: str | None = None\n    ) -&gt; Agent[TContext]:\n        if input_type is not None and type_adapter is not None:\n            if input_json is None:\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Handoff function expected non-null input, but got None\",\n                        data={\"details\": \"input_json is None\"},\n                    )\n                )\n                raise ModelBehaviorError(\"Handoff function expected non-null input, but got None\")\n\n            validated_input = _json.validate_json(\n                json_str=input_json,\n                type_adapter=type_adapter,\n                partial=False,\n            )\n            input_func = cast(OnHandoffWithInput[THandoffInput], on_handoff)\n            if inspect.iscoroutinefunction(input_func):\n                await input_func(ctx, validated_input)\n            else:\n                input_func(ctx, validated_input)\n        elif on_handoff is not None:\n            no_input_func = cast(OnHandoffWithoutInput, on_handoff)\n            if inspect.iscoroutinefunction(no_input_func):\n                await no_input_func(ctx)\n            else:\n                no_input_func(ctx)\n\n        return agent\n\n    tool_name = tool_name_override or Handoff.default_tool_name(agent)\n    tool_description = tool_description_override or Handoff.default_tool_description(agent)\n\n    # Always ensure the input JSON schema is in strict mode\n    # If there is a need, we can make this configurable in the future\n    input_json_schema = ensure_strict_json_schema(input_json_schema)\n\n    async def _is_enabled(ctx: RunContextWrapper[Any], agent_base: AgentBase[Any]) -&gt; bool:\n        from .agent import Agent\n\n        assert callable(is_enabled), \"is_enabled must be non-null here\"\n        assert isinstance(agent_base, Agent), \"Can't handoff to a non-Agent\"\n        result = is_enabled(ctx, agent_base)\n\n        if inspect.isawaitable(result):\n            return await result\n\n        return result\n\n    return Handoff(\n        tool_name=tool_name,\n        tool_description=tool_description,\n        input_json_schema=input_json_schema,\n        on_invoke_handoff=_invoke_handoff,\n        input_filter=input_filter,\n        agent_name=agent.name,\n        is_enabled=_is_enabled if callable(is_enabled) else is_enabled,\n    )\n</code></pre>"},{"location":"ref/items/","title":"<code>Items</code>","text":""},{"location":"ref/items/#agents.items.TResponse","title":"TResponse  <code>module-attribute</code>","text":"<pre><code>TResponse = Response\n</code></pre> <p>A type alias for the Response type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.TResponseInputItem","title":"TResponseInputItem  <code>module-attribute</code>","text":"<pre><code>TResponseInputItem = ResponseInputItemParam\n</code></pre> <p>A type alias for the ResponseInputItemParam type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.TResponseOutputItem","title":"TResponseOutputItem  <code>module-attribute</code>","text":"<pre><code>TResponseOutputItem = ResponseOutputItem\n</code></pre> <p>A type alias for the ResponseOutputItem type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.TResponseStreamEvent","title":"TResponseStreamEvent  <code>module-attribute</code>","text":"<pre><code>TResponseStreamEvent = ResponseStreamEvent\n</code></pre> <p>A type alias for the ResponseStreamEvent type from the OpenAI SDK.</p>"},{"location":"ref/items/#agents.items.ToolCallItemTypes","title":"ToolCallItemTypes  <code>module-attribute</code>","text":"<pre><code>ToolCallItemTypes: TypeAlias = Union[\n    ResponseFunctionToolCall,\n    ResponseComputerToolCall,\n    ResponseFileSearchToolCall,\n    ResponseFunctionWebSearch,\n    McpCall,\n    ResponseCodeInterpreterToolCall,\n    ImageGenerationCall,\n    LocalShellCall,\n]\n</code></pre> <p>A type that represents a tool call item.</p>"},{"location":"ref/items/#agents.items.RunItem","title":"RunItem  <code>module-attribute</code>","text":"<pre><code>RunItem: TypeAlias = Union[\n    MessageOutputItem,\n    HandoffCallItem,\n    HandoffOutputItem,\n    ToolCallItem,\n    ToolCallOutputItem,\n    ReasoningItem,\n    MCPListToolsItem,\n    MCPApprovalRequestItem,\n    MCPApprovalResponseItem,\n]\n</code></pre> <p>An item generated by an agent.</p>"},{"location":"ref/items/#agents.items.RunItemBase","title":"RunItemBase  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass RunItemBase(Generic[T], abc.ABC):\n    agent: Agent[Any]\n    \"\"\"The agent whose run caused this item to be generated.\"\"\"\n\n    raw_item: T\n    \"\"\"The raw Responses item from the run. This will always be a either an output item (i.e.\n    `openai.types.responses.ResponseOutputItem` or an input item\n    (i.e. `openai.types.responses.ResponseInputItemParam`).\n    \"\"\"\n\n    def to_input_item(self) -&gt; TResponseInputItem:\n        \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n        if isinstance(self.raw_item, dict):\n            # We know that input items are dicts, so we can ignore the type error\n            return self.raw_item  # type: ignore\n        elif isinstance(self.raw_item, BaseModel):\n            # All output items are Pydantic models that can be converted to input items.\n            return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n        else:\n            raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.RunItemBase.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.RunItemBase.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: T\n</code></pre> <p>The raw Responses item from the run. This will always be a either an output item (i.e. <code>openai.types.responses.ResponseOutputItem</code> or an input item (i.e. <code>openai.types.responses.ResponseInputItemParam</code>).</p>"},{"location":"ref/items/#agents.items.RunItemBase.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MessageOutputItem","title":"MessageOutputItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ResponseOutputMessage]</code></p> <p>Represents a message from the LLM.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MessageOutputItem(RunItemBase[ResponseOutputMessage]):\n    \"\"\"Represents a message from the LLM.\"\"\"\n\n    raw_item: ResponseOutputMessage\n    \"\"\"The raw response output message.\"\"\"\n\n    type: Literal[\"message_output_item\"] = \"message_output_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MessageOutputItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ResponseOutputMessage\n</code></pre> <p>The raw response output message.</p>"},{"location":"ref/items/#agents.items.MessageOutputItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MessageOutputItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffCallItem","title":"HandoffCallItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ResponseFunctionToolCall]</code></p> <p>Represents a tool call for a handoff from one agent to another.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass HandoffCallItem(RunItemBase[ResponseFunctionToolCall]):\n    \"\"\"Represents a tool call for a handoff from one agent to another.\"\"\"\n\n    raw_item: ResponseFunctionToolCall\n    \"\"\"The raw response function tool call that represents the handoff.\"\"\"\n\n    type: Literal[\"handoff_call_item\"] = \"handoff_call_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffCallItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ResponseFunctionToolCall\n</code></pre> <p>The raw response function tool call that represents the handoff.</p>"},{"location":"ref/items/#agents.items.HandoffCallItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.HandoffCallItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffOutputItem","title":"HandoffOutputItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[TResponseInputItem]</code></p> <p>Represents the output of a handoff.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass HandoffOutputItem(RunItemBase[TResponseInputItem]):\n    \"\"\"Represents the output of a handoff.\"\"\"\n\n    raw_item: TResponseInputItem\n    \"\"\"The raw input item that represents the handoff taking place.\"\"\"\n\n    source_agent: Agent[Any]\n    \"\"\"The agent that made the handoff.\"\"\"\n\n    target_agent: Agent[Any]\n    \"\"\"The agent that is being handed off to.\"\"\"\n\n    type: Literal[\"handoff_output_item\"] = \"handoff_output_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.HandoffOutputItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: TResponseInputItem\n</code></pre> <p>The raw input item that represents the handoff taking place.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.source_agent","title":"source_agent  <code>instance-attribute</code>","text":"<pre><code>source_agent: Agent[Any]\n</code></pre> <p>The agent that made the handoff.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.target_agent","title":"target_agent  <code>instance-attribute</code>","text":"<pre><code>target_agent: Agent[Any]\n</code></pre> <p>The agent that is being handed off to.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.HandoffOutputItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallItem","title":"ToolCallItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ToolCallItemTypes]</code></p> <p>Represents a tool call e.g. a function call or computer action call.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ToolCallItem(RunItemBase[ToolCallItemTypes]):\n    \"\"\"Represents a tool call e.g. a function call or computer action call.\"\"\"\n\n    raw_item: ToolCallItemTypes\n    \"\"\"The raw tool call item.\"\"\"\n\n    type: Literal[\"tool_call_item\"] = \"tool_call_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ToolCallItemTypes\n</code></pre> <p>The raw tool call item.</p>"},{"location":"ref/items/#agents.items.ToolCallItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ToolCallItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallOutputItem","title":"ToolCallOutputItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[Union[FunctionCallOutput, ComputerCallOutput, LocalShellCallOutput]]</code></p> <p>Represents the output of a tool call.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ToolCallOutputItem(\n    RunItemBase[Union[FunctionCallOutput, ComputerCallOutput, LocalShellCallOutput]]\n):\n    \"\"\"Represents the output of a tool call.\"\"\"\n\n    raw_item: FunctionCallOutput | ComputerCallOutput | LocalShellCallOutput\n    \"\"\"The raw item from the model.\"\"\"\n\n    output: Any\n    \"\"\"The output of the tool call. This is whatever the tool call returned; the `raw_item`\n    contains a string representation of the output.\n    \"\"\"\n\n    type: Literal[\"tool_call_output_item\"] = \"tool_call_output_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: (\n    FunctionCallOutput\n    | ComputerCallOutput\n    | LocalShellCallOutput\n)\n</code></pre> <p>The raw item from the model.</p>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output of the tool call. This is whatever the tool call returned; the <code>raw_item</code> contains a string representation of the output.</p>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ToolCallOutputItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ReasoningItem","title":"ReasoningItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[ResponseReasoningItem]</code></p> <p>Represents a reasoning item.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass ReasoningItem(RunItemBase[ResponseReasoningItem]):\n    \"\"\"Represents a reasoning item.\"\"\"\n\n    raw_item: ResponseReasoningItem\n    \"\"\"The raw reasoning item.\"\"\"\n\n    type: Literal[\"reasoning_item\"] = \"reasoning_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.ReasoningItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: ResponseReasoningItem\n</code></pre> <p>The raw reasoning item.</p>"},{"location":"ref/items/#agents.items.ReasoningItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.ReasoningItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MCPListToolsItem","title":"MCPListToolsItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[McpListTools]</code></p> <p>Represents a call to an MCP server to list tools.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MCPListToolsItem(RunItemBase[McpListTools]):\n    \"\"\"Represents a call to an MCP server to list tools.\"\"\"\n\n    raw_item: McpListTools\n    \"\"\"The raw MCP list tools call.\"\"\"\n\n    type: Literal[\"mcp_list_tools_item\"] = \"mcp_list_tools_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MCPListToolsItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: McpListTools\n</code></pre> <p>The raw MCP list tools call.</p>"},{"location":"ref/items/#agents.items.MCPListToolsItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MCPListToolsItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem","title":"MCPApprovalRequestItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[McpApprovalRequest]</code></p> <p>Represents a request for MCP approval.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MCPApprovalRequestItem(RunItemBase[McpApprovalRequest]):\n    \"\"\"Represents a request for MCP approval.\"\"\"\n\n    raw_item: McpApprovalRequest\n    \"\"\"The raw MCP approval request.\"\"\"\n\n    type: Literal[\"mcp_approval_request_item\"] = \"mcp_approval_request_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: McpApprovalRequest\n</code></pre> <p>The raw MCP approval request.</p>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MCPApprovalRequestItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem","title":"MCPApprovalResponseItem  <code>dataclass</code>","text":"<p>               Bases: <code>RunItemBase[McpApprovalResponse]</code></p> <p>Represents a response to an MCP approval request.</p> Source code in <code>src/agents/items.py</code> <pre><code>@dataclass\nclass MCPApprovalResponseItem(RunItemBase[McpApprovalResponse]):\n    \"\"\"Represents a response to an MCP approval request.\"\"\"\n\n    raw_item: McpApprovalResponse\n    \"\"\"The raw MCP approval response.\"\"\"\n\n    type: Literal[\"mcp_approval_response_item\"] = \"mcp_approval_response_item\"\n</code></pre>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.raw_item","title":"raw_item  <code>instance-attribute</code>","text":"<pre><code>raw_item: McpApprovalResponse\n</code></pre> <p>The raw MCP approval response.</p>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent whose run caused this item to be generated.</p>"},{"location":"ref/items/#agents.items.MCPApprovalResponseItem.to_input_item","title":"to_input_item","text":"<pre><code>to_input_item() -&gt; TResponseInputItem\n</code></pre> <p>Converts this item into an input item suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_item(self) -&gt; TResponseInputItem:\n    \"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\n    if isinstance(self.raw_item, dict):\n        # We know that input items are dicts, so we can ignore the type error\n        return self.raw_item  # type: ignore\n    elif isinstance(self.raw_item, BaseModel):\n        # All output items are Pydantic models that can be converted to input items.\n        return self.raw_item.model_dump(exclude_unset=True)  # type: ignore\n    else:\n        raise AgentsException(f\"Unexpected raw item type: {type(self.raw_item)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ModelResponse","title":"ModelResponse","text":"Source code in <code>src/agents/items.py</code> <pre><code>@pydantic.dataclasses.dataclass\nclass ModelResponse:\n    output: list[TResponseOutputItem]\n    \"\"\"A list of outputs (messages, tool calls, etc) generated by the model\"\"\"\n\n    usage: Usage\n    \"\"\"The usage information for the response.\"\"\"\n\n    response_id: str | None\n    \"\"\"An ID for the response which can be used to refer to the response in subsequent calls to the\n    model. Not supported by all model providers.\n    If using OpenAI models via the Responses API, this is the `response_id` parameter, and it can\n    be passed to `Runner.run`.\n    \"\"\"\n\n    def to_input_items(self) -&gt; list[TResponseInputItem]:\n        \"\"\"Convert the output into a list of input items suitable for passing to the model.\"\"\"\n        # We happen to know that the shape of the Pydantic output items are the same as the\n        # equivalent TypedDict input items, so we can just convert each one.\n        # This is also tested via unit tests.\n        return [it.model_dump(exclude_unset=True) for it in self.output]  # type: ignore\n</code></pre>"},{"location":"ref/items/#agents.items.ModelResponse.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: list[TResponseOutputItem]\n</code></pre> <p>A list of outputs (messages, tool calls, etc) generated by the model</p>"},{"location":"ref/items/#agents.items.ModelResponse.usage","title":"usage  <code>instance-attribute</code>","text":"<pre><code>usage: Usage\n</code></pre> <p>The usage information for the response.</p>"},{"location":"ref/items/#agents.items.ModelResponse.response_id","title":"response_id  <code>instance-attribute</code>","text":"<pre><code>response_id: str | None\n</code></pre> <p>An ID for the response which can be used to refer to the response in subsequent calls to the model. Not supported by all model providers. If using OpenAI models via the Responses API, this is the <code>response_id</code> parameter, and it can be passed to <code>Runner.run</code>.</p>"},{"location":"ref/items/#agents.items.ModelResponse.to_input_items","title":"to_input_items","text":"<pre><code>to_input_items() -&gt; list[TResponseInputItem]\n</code></pre> <p>Convert the output into a list of input items suitable for passing to the model.</p> Source code in <code>src/agents/items.py</code> <pre><code>def to_input_items(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Convert the output into a list of input items suitable for passing to the model.\"\"\"\n    # We happen to know that the shape of the Pydantic output items are the same as the\n    # equivalent TypedDict input items, so we can just convert each one.\n    # This is also tested via unit tests.\n    return [it.model_dump(exclude_unset=True) for it in self.output]  # type: ignore\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers","title":"ItemHelpers","text":"Source code in <code>src/agents/items.py</code> <pre><code>class ItemHelpers:\n    @classmethod\n    def extract_last_content(cls, message: TResponseOutputItem) -&gt; str:\n        \"\"\"Extracts the last text content or refusal from a message.\"\"\"\n        if not isinstance(message, ResponseOutputMessage):\n            return \"\"\n\n        last_content = message.content[-1]\n        if isinstance(last_content, ResponseOutputText):\n            return last_content.text\n        elif isinstance(last_content, ResponseOutputRefusal):\n            return last_content.refusal\n        else:\n            raise ModelBehaviorError(f\"Unexpected content type: {type(last_content)}\")\n\n    @classmethod\n    def extract_last_text(cls, message: TResponseOutputItem) -&gt; str | None:\n        \"\"\"Extracts the last text content from a message, if any. Ignores refusals.\"\"\"\n        if isinstance(message, ResponseOutputMessage):\n            last_content = message.content[-1]\n            if isinstance(last_content, ResponseOutputText):\n                return last_content.text\n\n        return None\n\n    @classmethod\n    def input_to_new_input_list(\n        cls, input: str | list[TResponseInputItem]\n    ) -&gt; list[TResponseInputItem]:\n        \"\"\"Converts a string or list of input items into a list of input items.\"\"\"\n        if isinstance(input, str):\n            return [\n                {\n                    \"content\": input,\n                    \"role\": \"user\",\n                }\n            ]\n        return copy.deepcopy(input)\n\n    @classmethod\n    def text_message_outputs(cls, items: list[RunItem]) -&gt; str:\n        \"\"\"Concatenates all the text content from a list of message output items.\"\"\"\n        text = \"\"\n        for item in items:\n            if isinstance(item, MessageOutputItem):\n                text += cls.text_message_output(item)\n        return text\n\n    @classmethod\n    def text_message_output(cls, message: MessageOutputItem) -&gt; str:\n        \"\"\"Extracts all the text content from a single message output item.\"\"\"\n        text = \"\"\n        for item in message.raw_item.content:\n            if isinstance(item, ResponseOutputText):\n                text += item.text\n        return text\n\n    @classmethod\n    def tool_call_output_item(\n        cls, tool_call: ResponseFunctionToolCall, output: str\n    ) -&gt; FunctionCallOutput:\n        \"\"\"Creates a tool call output item from a tool call and its output.\"\"\"\n        return {\n            \"call_id\": tool_call.call_id,\n            \"output\": output,\n            \"type\": \"function_call_output\",\n        }\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.extract_last_content","title":"extract_last_content  <code>classmethod</code>","text":"<pre><code>extract_last_content(message: TResponseOutputItem) -&gt; str\n</code></pre> <p>Extracts the last text content or refusal from a message.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef extract_last_content(cls, message: TResponseOutputItem) -&gt; str:\n    \"\"\"Extracts the last text content or refusal from a message.\"\"\"\n    if not isinstance(message, ResponseOutputMessage):\n        return \"\"\n\n    last_content = message.content[-1]\n    if isinstance(last_content, ResponseOutputText):\n        return last_content.text\n    elif isinstance(last_content, ResponseOutputRefusal):\n        return last_content.refusal\n    else:\n        raise ModelBehaviorError(f\"Unexpected content type: {type(last_content)}\")\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.extract_last_text","title":"extract_last_text  <code>classmethod</code>","text":"<pre><code>extract_last_text(\n    message: TResponseOutputItem,\n) -&gt; str | None\n</code></pre> <p>Extracts the last text content from a message, if any. Ignores refusals.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef extract_last_text(cls, message: TResponseOutputItem) -&gt; str | None:\n    \"\"\"Extracts the last text content from a message, if any. Ignores refusals.\"\"\"\n    if isinstance(message, ResponseOutputMessage):\n        last_content = message.content[-1]\n        if isinstance(last_content, ResponseOutputText):\n            return last_content.text\n\n    return None\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.input_to_new_input_list","title":"input_to_new_input_list  <code>classmethod</code>","text":"<pre><code>input_to_new_input_list(\n    input: str | list[TResponseInputItem],\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Converts a string or list of input items into a list of input items.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef input_to_new_input_list(\n    cls, input: str | list[TResponseInputItem]\n) -&gt; list[TResponseInputItem]:\n    \"\"\"Converts a string or list of input items into a list of input items.\"\"\"\n    if isinstance(input, str):\n        return [\n            {\n                \"content\": input,\n                \"role\": \"user\",\n            }\n        ]\n    return copy.deepcopy(input)\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.text_message_outputs","title":"text_message_outputs  <code>classmethod</code>","text":"<pre><code>text_message_outputs(items: list[RunItem]) -&gt; str\n</code></pre> <p>Concatenates all the text content from a list of message output items.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef text_message_outputs(cls, items: list[RunItem]) -&gt; str:\n    \"\"\"Concatenates all the text content from a list of message output items.\"\"\"\n    text = \"\"\n    for item in items:\n        if isinstance(item, MessageOutputItem):\n            text += cls.text_message_output(item)\n    return text\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.text_message_output","title":"text_message_output  <code>classmethod</code>","text":"<pre><code>text_message_output(message: MessageOutputItem) -&gt; str\n</code></pre> <p>Extracts all the text content from a single message output item.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef text_message_output(cls, message: MessageOutputItem) -&gt; str:\n    \"\"\"Extracts all the text content from a single message output item.\"\"\"\n    text = \"\"\n    for item in message.raw_item.content:\n        if isinstance(item, ResponseOutputText):\n            text += item.text\n    return text\n</code></pre>"},{"location":"ref/items/#agents.items.ItemHelpers.tool_call_output_item","title":"tool_call_output_item  <code>classmethod</code>","text":"<pre><code>tool_call_output_item(\n    tool_call: ResponseFunctionToolCall, output: str\n) -&gt; FunctionCallOutput\n</code></pre> <p>Creates a tool call output item from a tool call and its output.</p> Source code in <code>src/agents/items.py</code> <pre><code>@classmethod\ndef tool_call_output_item(\n    cls, tool_call: ResponseFunctionToolCall, output: str\n) -&gt; FunctionCallOutput:\n    \"\"\"Creates a tool call output item from a tool call and its output.\"\"\"\n    return {\n        \"call_id\": tool_call.call_id,\n        \"output\": output,\n        \"type\": \"function_call_output\",\n    }\n</code></pre>"},{"location":"ref/lifecycle/","title":"<code>Lifecycle</code>","text":""},{"location":"ref/lifecycle/#agents.lifecycle.RunHooks","title":"RunHooks  <code>module-attribute</code>","text":"<pre><code>RunHooks = RunHooksBase[TContext, Agent]\n</code></pre> <p>Run hooks when using <code>Agent</code>.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooks","title":"AgentHooks  <code>module-attribute</code>","text":"<pre><code>AgentHooks = AgentHooksBase[TContext, Agent]\n</code></pre> <p>Agent hooks for <code>Agent</code>s.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase","title":"RunHooksBase","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>A class that receives callbacks on various lifecycle events in an agent run. Subclass and override the methods you need.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_agent_start","title":"on_agent_start  <code>async</code>","text":"<pre><code>on_agent_start(\n    context: RunContextWrapper[TContext], agent: TAgent\n) -&gt; None\n</code></pre> <p>Called before the agent is invoked. Called each time the current agent changes.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_agent_end","title":"on_agent_end  <code>async</code>","text":"<pre><code>on_agent_end(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    output: Any,\n) -&gt; None\n</code></pre> <p>Called when the agent produces a final output.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_handoff","title":"on_handoff  <code>async</code>","text":"<pre><code>on_handoff(\n    context: RunContextWrapper[TContext],\n    from_agent: TAgent,\n    to_agent: TAgent,\n) -&gt; None\n</code></pre> <p>Called when a handoff occurs.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_tool_start","title":"on_tool_start  <code>async</code>","text":"<pre><code>on_tool_start(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n) -&gt; None\n</code></pre> <p>Called before a tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.RunHooksBase.on_tool_end","title":"on_tool_end  <code>async</code>","text":"<pre><code>on_tool_end(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n    result: str,\n) -&gt; None\n</code></pre> <p>Called after a tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase","title":"AgentHooksBase","text":"<p>               Bases: <code>Generic[TContext, TAgent]</code></p> <p>A class that receives callbacks on various lifecycle events for a specific agent. You can set this on <code>agent.hooks</code> to receive events for that specific agent.</p> <p>Subclass and override the methods you need.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start(\n    context: RunContextWrapper[TContext], agent: TAgent\n) -&gt; None\n</code></pre> <p>Called before the agent is invoked. Called each time the running agent is changed to this agent.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_end","title":"on_end  <code>async</code>","text":"<pre><code>on_end(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    output: Any,\n) -&gt; None\n</code></pre> <p>Called when the agent produces a final output.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_handoff","title":"on_handoff  <code>async</code>","text":"<pre><code>on_handoff(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    source: TAgent,\n) -&gt; None\n</code></pre> <p>Called when the agent is being handed off to. The <code>source</code> is the agent that is handing off to this agent.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_tool_start","title":"on_tool_start  <code>async</code>","text":"<pre><code>on_tool_start(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n) -&gt; None\n</code></pre> <p>Called before a tool is invoked.</p>"},{"location":"ref/lifecycle/#agents.lifecycle.AgentHooksBase.on_tool_end","title":"on_tool_end  <code>async</code>","text":"<pre><code>on_tool_end(\n    context: RunContextWrapper[TContext],\n    agent: TAgent,\n    tool: Tool,\n    result: str,\n) -&gt; None\n</code></pre> <p>Called after a tool is invoked.</p>"},{"location":"ref/memory/","title":"Memory","text":""},{"location":"ref/memory/#agents.memory.Session","title":"Session","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for session implementations.</p> <p>Session stores conversation history for a specific session, allowing agents to maintain context without requiring explicit manual memory management.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>@runtime_checkable\nclass Session(Protocol):\n    \"\"\"Protocol for session implementations.\n\n    Session stores conversation history for a specific session, allowing\n    agents to maintain context without requiring explicit manual memory management.\n    \"\"\"\n\n    session_id: str\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, retrieves all items.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n        ...\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        ...\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n        ...\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n        ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.Session.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n    ...\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession","title":"SQLiteSession","text":"<p>               Bases: <code>SessionABC</code></p> <p>SQLite-based implementation of session storage.</p> <p>This implementation stores conversation history in a SQLite database. By default, uses an in-memory database that is lost when the process ends. For persistent storage, provide a file path.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>class SQLiteSession(SessionABC):\n    \"\"\"SQLite-based implementation of session storage.\n\n    This implementation stores conversation history in a SQLite database.\n    By default, uses an in-memory database that is lost when the process ends.\n    For persistent storage, provide a file path.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        db_path: str | Path = \":memory:\",\n        sessions_table: str = \"agent_sessions\",\n        messages_table: str = \"agent_messages\",\n    ):\n        \"\"\"Initialize the SQLite session.\n\n        Args:\n            session_id: Unique identifier for the conversation session\n            db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n            sessions_table: Name of the table to store session metadata. Defaults to\n                'agent_sessions'\n            messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n        \"\"\"\n        self.session_id = session_id\n        self.db_path = db_path\n        self.sessions_table = sessions_table\n        self.messages_table = messages_table\n        self._local = threading.local()\n        self._lock = threading.Lock()\n\n        # For in-memory databases, we need a shared connection to avoid thread isolation\n        # For file databases, we use thread-local connections for better concurrency\n        self._is_memory_db = str(db_path) == \":memory:\"\n        if self._is_memory_db:\n            self._shared_connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n            self._shared_connection.execute(\"PRAGMA journal_mode=WAL\")\n            self._init_db_for_connection(self._shared_connection)\n        else:\n            # For file databases, initialize the schema once since it persists\n            init_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n            init_conn.execute(\"PRAGMA journal_mode=WAL\")\n            self._init_db_for_connection(init_conn)\n            init_conn.close()\n\n    def _get_connection(self) -&gt; sqlite3.Connection:\n        \"\"\"Get a database connection.\"\"\"\n        if self._is_memory_db:\n            # Use shared connection for in-memory database to avoid thread isolation\n            return self._shared_connection\n        else:\n            # Use thread-local connections for file databases\n            if not hasattr(self._local, \"connection\"):\n                self._local.connection = sqlite3.connect(\n                    str(self.db_path),\n                    check_same_thread=False,\n                )\n                self._local.connection.execute(\"PRAGMA journal_mode=WAL\")\n            assert isinstance(self._local.connection, sqlite3.Connection), (\n                f\"Expected sqlite3.Connection, got {type(self._local.connection)}\"\n            )\n            return self._local.connection\n\n    def _init_db_for_connection(self, conn: sqlite3.Connection) -&gt; None:\n        \"\"\"Initialize the database schema for a specific connection.\"\"\"\n        conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.sessions_table} (\n                session_id TEXT PRIMARY KEY,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\"\n        )\n\n        conn.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.messages_table} (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                message_data TEXT NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES {self.sessions_table} (session_id)\n                    ON DELETE CASCADE\n            )\n        \"\"\"\n        )\n\n        conn.execute(\n            f\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_{self.messages_table}_session_id\n            ON {self.messages_table} (session_id, created_at)\n        \"\"\"\n        )\n\n        conn.commit()\n\n    async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n        \"\"\"Retrieve the conversation history for this session.\n\n        Args:\n            limit: Maximum number of items to retrieve. If None, retrieves all items.\n                   When specified, returns the latest N items in chronological order.\n\n        Returns:\n            List of input items representing the conversation history\n        \"\"\"\n\n        def _get_items_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                if limit is None:\n                    # Fetch all items in chronological order\n                    cursor = conn.execute(\n                        f\"\"\"\n                        SELECT message_data FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY created_at ASC\n                    \"\"\",\n                        (self.session_id,),\n                    )\n                else:\n                    # Fetch the latest N items in chronological order\n                    cursor = conn.execute(\n                        f\"\"\"\n                        SELECT message_data FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY created_at DESC\n                        LIMIT ?\n                        \"\"\",\n                        (self.session_id, limit),\n                    )\n\n                rows = cursor.fetchall()\n\n                # Reverse to get chronological order when using DESC\n                if limit is not None:\n                    rows = list(reversed(rows))\n\n                items = []\n                for (message_data,) in rows:\n                    try:\n                        item = json.loads(message_data)\n                        items.append(item)\n                    except json.JSONDecodeError:\n                        # Skip invalid JSON entries\n                        continue\n\n                return items\n\n        return await asyncio.to_thread(_get_items_sync)\n\n    async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n        \"\"\"Add new items to the conversation history.\n\n        Args:\n            items: List of input items to add to the history\n        \"\"\"\n        if not items:\n            return\n\n        def _add_items_sync():\n            conn = self._get_connection()\n\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Ensure session exists\n                conn.execute(\n                    f\"\"\"\n                    INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                # Add items\n                message_data = [(self.session_id, json.dumps(item)) for item in items]\n                conn.executemany(\n                    f\"\"\"\n                    INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n                \"\"\",\n                    message_data,\n                )\n\n                # Update session timestamp\n                conn.execute(\n                    f\"\"\"\n                    UPDATE {self.sessions_table}\n                    SET updated_at = CURRENT_TIMESTAMP\n                    WHERE session_id = ?\n                \"\"\",\n                    (self.session_id,),\n                )\n\n                conn.commit()\n\n        await asyncio.to_thread(_add_items_sync)\n\n    async def pop_item(self) -&gt; TResponseInputItem | None:\n        \"\"\"Remove and return the most recent item from the session.\n\n        Returns:\n            The most recent item if it exists, None if the session is empty\n        \"\"\"\n\n        def _pop_item_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                # Use DELETE with RETURNING to atomically delete and return the most recent item\n                cursor = conn.execute(\n                    f\"\"\"\n                    DELETE FROM {self.messages_table}\n                    WHERE id = (\n                        SELECT id FROM {self.messages_table}\n                        WHERE session_id = ?\n                        ORDER BY created_at DESC\n                        LIMIT 1\n                    )\n                    RETURNING message_data\n                    \"\"\",\n                    (self.session_id,),\n                )\n\n                result = cursor.fetchone()\n                conn.commit()\n\n                if result:\n                    message_data = result[0]\n                    try:\n                        item = json.loads(message_data)\n                        return item\n                    except json.JSONDecodeError:\n                        # Return None for corrupted JSON entries (already deleted)\n                        return None\n\n                return None\n\n        return await asyncio.to_thread(_pop_item_sync)\n\n    async def clear_session(self) -&gt; None:\n        \"\"\"Clear all items for this session.\"\"\"\n\n        def _clear_session_sync():\n            conn = self._get_connection()\n            with self._lock if self._is_memory_db else threading.Lock():\n                conn.execute(\n                    f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                    (self.session_id,),\n                )\n                conn.execute(\n                    f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                    (self.session_id,),\n                )\n                conn.commit()\n\n        await asyncio.to_thread(_clear_session_sync)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the database connection.\"\"\"\n        if self._is_memory_db:\n            if hasattr(self, \"_shared_connection\"):\n                self._shared_connection.close()\n        else:\n            if hasattr(self._local, \"connection\"):\n                self._local.connection.close()\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n)\n</code></pre> <p>Initialize the SQLite session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Unique identifier for the conversation session</p> required <code>db_path</code> <code>str | Path</code> <p>Path to the SQLite database file. Defaults to ':memory:' (in-memory database)</p> <code>':memory:'</code> <code>sessions_table</code> <code>str</code> <p>Name of the table to store session metadata. Defaults to 'agent_sessions'</p> <code>'agent_sessions'</code> <code>messages_table</code> <code>str</code> <p>Name of the table to store message data. Defaults to 'agent_messages'</p> <code>'agent_messages'</code> Source code in <code>src/agents/memory/session.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    db_path: str | Path = \":memory:\",\n    sessions_table: str = \"agent_sessions\",\n    messages_table: str = \"agent_messages\",\n):\n    \"\"\"Initialize the SQLite session.\n\n    Args:\n        session_id: Unique identifier for the conversation session\n        db_path: Path to the SQLite database file. Defaults to ':memory:' (in-memory database)\n        sessions_table: Name of the table to store session metadata. Defaults to\n            'agent_sessions'\n        messages_table: Name of the table to store message data. Defaults to 'agent_messages'\n    \"\"\"\n    self.session_id = session_id\n    self.db_path = db_path\n    self.sessions_table = sessions_table\n    self.messages_table = messages_table\n    self._local = threading.local()\n    self._lock = threading.Lock()\n\n    # For in-memory databases, we need a shared connection to avoid thread isolation\n    # For file databases, we use thread-local connections for better concurrency\n    self._is_memory_db = str(db_path) == \":memory:\"\n    if self._is_memory_db:\n        self._shared_connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n        self._shared_connection.execute(\"PRAGMA journal_mode=WAL\")\n        self._init_db_for_connection(self._shared_connection)\n    else:\n        # For file databases, initialize the schema once since it persists\n        init_conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n        init_conn.execute(\"PRAGMA journal_mode=WAL\")\n        self._init_db_for_connection(init_conn)\n        init_conn.close()\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.get_items","title":"get_items  <code>async</code>","text":"<pre><code>get_items(\n    limit: int | None = None,\n) -&gt; list[TResponseInputItem]\n</code></pre> <p>Retrieve the conversation history for this session.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Maximum number of items to retrieve. If None, retrieves all items.    When specified, returns the latest N items in chronological order.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TResponseInputItem]</code> <p>List of input items representing the conversation history</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def get_items(self, limit: int | None = None) -&gt; list[TResponseInputItem]:\n    \"\"\"Retrieve the conversation history for this session.\n\n    Args:\n        limit: Maximum number of items to retrieve. If None, retrieves all items.\n               When specified, returns the latest N items in chronological order.\n\n    Returns:\n        List of input items representing the conversation history\n    \"\"\"\n\n    def _get_items_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            if limit is None:\n                # Fetch all items in chronological order\n                cursor = conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY created_at ASC\n                \"\"\",\n                    (self.session_id,),\n                )\n            else:\n                # Fetch the latest N items in chronological order\n                cursor = conn.execute(\n                    f\"\"\"\n                    SELECT message_data FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY created_at DESC\n                    LIMIT ?\n                    \"\"\",\n                    (self.session_id, limit),\n                )\n\n            rows = cursor.fetchall()\n\n            # Reverse to get chronological order when using DESC\n            if limit is not None:\n                rows = list(reversed(rows))\n\n            items = []\n            for (message_data,) in rows:\n                try:\n                    item = json.loads(message_data)\n                    items.append(item)\n                except json.JSONDecodeError:\n                    # Skip invalid JSON entries\n                    continue\n\n            return items\n\n    return await asyncio.to_thread(_get_items_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.add_items","title":"add_items  <code>async</code>","text":"<pre><code>add_items(items: list[TResponseInputItem]) -&gt; None\n</code></pre> <p>Add new items to the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[TResponseInputItem]</code> <p>List of input items to add to the history</p> required Source code in <code>src/agents/memory/session.py</code> <pre><code>async def add_items(self, items: list[TResponseInputItem]) -&gt; None:\n    \"\"\"Add new items to the conversation history.\n\n    Args:\n        items: List of input items to add to the history\n    \"\"\"\n    if not items:\n        return\n\n    def _add_items_sync():\n        conn = self._get_connection()\n\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Ensure session exists\n            conn.execute(\n                f\"\"\"\n                INSERT OR IGNORE INTO {self.sessions_table} (session_id) VALUES (?)\n            \"\"\",\n                (self.session_id,),\n            )\n\n            # Add items\n            message_data = [(self.session_id, json.dumps(item)) for item in items]\n            conn.executemany(\n                f\"\"\"\n                INSERT INTO {self.messages_table} (session_id, message_data) VALUES (?, ?)\n            \"\"\",\n                message_data,\n            )\n\n            # Update session timestamp\n            conn.execute(\n                f\"\"\"\n                UPDATE {self.sessions_table}\n                SET updated_at = CURRENT_TIMESTAMP\n                WHERE session_id = ?\n            \"\"\",\n                (self.session_id,),\n            )\n\n            conn.commit()\n\n    await asyncio.to_thread(_add_items_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.pop_item","title":"pop_item  <code>async</code>","text":"<pre><code>pop_item() -&gt; TResponseInputItem | None\n</code></pre> <p>Remove and return the most recent item from the session.</p> <p>Returns:</p> Type Description <code>TResponseInputItem | None</code> <p>The most recent item if it exists, None if the session is empty</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def pop_item(self) -&gt; TResponseInputItem | None:\n    \"\"\"Remove and return the most recent item from the session.\n\n    Returns:\n        The most recent item if it exists, None if the session is empty\n    \"\"\"\n\n    def _pop_item_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            # Use DELETE with RETURNING to atomically delete and return the most recent item\n            cursor = conn.execute(\n                f\"\"\"\n                DELETE FROM {self.messages_table}\n                WHERE id = (\n                    SELECT id FROM {self.messages_table}\n                    WHERE session_id = ?\n                    ORDER BY created_at DESC\n                    LIMIT 1\n                )\n                RETURNING message_data\n                \"\"\",\n                (self.session_id,),\n            )\n\n            result = cursor.fetchone()\n            conn.commit()\n\n            if result:\n                message_data = result[0]\n                try:\n                    item = json.loads(message_data)\n                    return item\n                except json.JSONDecodeError:\n                    # Return None for corrupted JSON entries (already deleted)\n                    return None\n\n            return None\n\n    return await asyncio.to_thread(_pop_item_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.clear_session","title":"clear_session  <code>async</code>","text":"<pre><code>clear_session() -&gt; None\n</code></pre> <p>Clear all items for this session.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>async def clear_session(self) -&gt; None:\n    \"\"\"Clear all items for this session.\"\"\"\n\n    def _clear_session_sync():\n        conn = self._get_connection()\n        with self._lock if self._is_memory_db else threading.Lock():\n            conn.execute(\n                f\"DELETE FROM {self.messages_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.execute(\n                f\"DELETE FROM {self.sessions_table} WHERE session_id = ?\",\n                (self.session_id,),\n            )\n            conn.commit()\n\n    await asyncio.to_thread(_clear_session_sync)\n</code></pre>"},{"location":"ref/memory/#agents.memory.SQLiteSession.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p> Source code in <code>src/agents/memory/session.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._is_memory_db:\n        if hasattr(self, \"_shared_connection\"):\n            self._shared_connection.close()\n    else:\n        if hasattr(self._local, \"connection\"):\n            self._local.connection.close()\n</code></pre>"},{"location":"ref/model_settings/","title":"<code>Model settings</code>","text":""},{"location":"ref/model_settings/#agents.model_settings.ModelSettings","title":"ModelSettings","text":"<p>Settings to use when calling an LLM.</p> <p>This class holds optional model configuration parameters (e.g. temperature, top_p, penalties, truncation, etc.).</p> <p>Not all models/providers support all of these parameters, so please check the API documentation for the specific model and provider you are using.</p> Source code in <code>src/agents/model_settings.py</code> <pre><code>@dataclass\nclass ModelSettings:\n    \"\"\"Settings to use when calling an LLM.\n\n    This class holds optional model configuration parameters (e.g. temperature,\n    top_p, penalties, truncation, etc.).\n\n    Not all models/providers support all of these parameters, so please check the API documentation\n    for the specific model and provider you are using.\n    \"\"\"\n\n    temperature: float | None = None\n    \"\"\"The temperature to use when calling the model.\"\"\"\n\n    top_p: float | None = None\n    \"\"\"The top_p to use when calling the model.\"\"\"\n\n    frequency_penalty: float | None = None\n    \"\"\"The frequency penalty to use when calling the model.\"\"\"\n\n    presence_penalty: float | None = None\n    \"\"\"The presence penalty to use when calling the model.\"\"\"\n\n    tool_choice: ToolChoice | None = None\n    \"\"\"The tool choice to use when calling the model.\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Controls whether the model can make multiple parallel tool calls in a single turn.\n    If not provided (i.e., set to None), this behavior defers to the underlying\n    model provider's default. For most current providers (e.g., OpenAI), this typically\n    means parallel tool calls are enabled (True).\n    Set to True to explicitly enable parallel tool calls, or False to restrict the\n    model to at most one tool call per turn.\n    \"\"\"\n\n    truncation: Literal[\"auto\", \"disabled\"] | None = None\n    \"\"\"The truncation strategy to use when calling the model.\"\"\"\n\n    max_tokens: int | None = None\n    \"\"\"The maximum number of output tokens to generate.\"\"\"\n\n    reasoning: Reasoning | None = None\n    \"\"\"Configuration options for\n    [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n    \"\"\"\n\n    metadata: dict[str, str] | None = None\n    \"\"\"Metadata to include with the model response call.\"\"\"\n\n    store: bool | None = None\n    \"\"\"Whether to store the generated model response for later retrieval.\n    Defaults to True if not provided.\"\"\"\n\n    include_usage: bool | None = None\n    \"\"\"Whether to include usage chunk.\n    Defaults to True if not provided.\"\"\"\n\n    response_include: list[ResponseIncludable] | None = None\n    \"\"\"Additional output data to include in the model response.\n    [include parameter](https://platform.openai.com/docs/api-reference/responses/create#responses-create-include)\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top tokens to return logprobs for. Setting this will\n    automatically include ``\"message.output_text.logprobs\"`` in the response.\"\"\"\n\n    extra_query: Query | None = None\n    \"\"\"Additional query fields to provide with the request.\n    Defaults to None if not provided.\"\"\"\n\n    extra_body: Body | None = None\n    \"\"\"Additional body fields to provide with the request.\n    Defaults to None if not provided.\"\"\"\n\n    extra_headers: Headers | None = None\n    \"\"\"Additional headers to provide with the request.\n    Defaults to None if not provided.\"\"\"\n\n    extra_args: dict[str, Any] | None = None\n    \"\"\"Arbitrary keyword arguments to pass to the model API call.\n    These will be passed directly to the underlying model provider's API.\n    Use with caution as not all models support all parameters.\"\"\"\n\n    def resolve(self, override: ModelSettings | None) -&gt; ModelSettings:\n        \"\"\"Produce a new ModelSettings by overlaying any non-None values from the\n        override on top of this instance.\"\"\"\n        if override is None:\n            return self\n\n        changes = {\n            field.name: getattr(override, field.name)\n            for field in fields(self)\n            if getattr(override, field.name) is not None\n        }\n\n        # Handle extra_args merging specially - merge dictionaries instead of replacing\n        if self.extra_args is not None or override.extra_args is not None:\n            merged_args = {}\n            if self.extra_args:\n                merged_args.update(self.extra_args)\n            if override.extra_args:\n                merged_args.update(override.extra_args)\n            changes[\"extra_args\"] = merged_args if merged_args else None\n\n        return replace(self, **changes)\n\n    def to_json_dict(self) -&gt; dict[str, Any]:\n        dataclass_dict = dataclasses.asdict(self)\n\n        json_dict: dict[str, Any] = {}\n\n        for field_name, value in dataclass_dict.items():\n            if isinstance(value, BaseModel):\n                json_dict[field_name] = value.model_dump(mode=\"json\")\n            else:\n                json_dict[field_name] = value\n\n        return json_dict\n</code></pre>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float | None = None\n</code></pre> <p>The temperature to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p: float | None = None\n</code></pre> <p>The top_p to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.frequency_penalty","title":"frequency_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: float | None = None\n</code></pre> <p>The frequency penalty to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.presence_penalty","title":"presence_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>presence_penalty: float | None = None\n</code></pre> <p>The presence penalty to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.tool_choice","title":"tool_choice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_choice: ToolChoice | None = None\n</code></pre> <p>The tool choice to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.parallel_tool_calls","title":"parallel_tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parallel_tool_calls: bool | None = None\n</code></pre> <p>Controls whether the model can make multiple parallel tool calls in a single turn. If not provided (i.e., set to None), this behavior defers to the underlying model provider's default. For most current providers (e.g., OpenAI), this typically means parallel tool calls are enabled (True). Set to True to explicitly enable parallel tool calls, or False to restrict the model to at most one tool call per turn.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.truncation","title":"truncation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>truncation: Literal['auto', 'disabled'] | None = None\n</code></pre> <p>The truncation strategy to use when calling the model.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int | None = None\n</code></pre> <p>The maximum number of output tokens to generate.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.reasoning","title":"reasoning  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reasoning: Reasoning | None = None\n</code></pre> <p>Configuration options for reasoning models.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: dict[str, str] | None = None\n</code></pre> <p>Metadata to include with the model response call.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.store","title":"store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>store: bool | None = None\n</code></pre> <p>Whether to store the generated model response for later retrieval. Defaults to True if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.include_usage","title":"include_usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_usage: bool | None = None\n</code></pre> <p>Whether to include usage chunk. Defaults to True if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.response_include","title":"response_include  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>response_include: list[ResponseIncludable] | None = None\n</code></pre> <p>Additional output data to include in the model response. include parameter</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.top_logprobs","title":"top_logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_logprobs: int | None = None\n</code></pre> <p>Number of top tokens to return logprobs for. Setting this will automatically include <code>\"message.output_text.logprobs\"</code> in the response.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_query","title":"extra_query  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_query: Query | None = None\n</code></pre> <p>Additional query fields to provide with the request. Defaults to None if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_body","title":"extra_body  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_body: Body | None = None\n</code></pre> <p>Additional body fields to provide with the request. Defaults to None if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_headers","title":"extra_headers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_headers: Headers | None = None\n</code></pre> <p>Additional headers to provide with the request. Defaults to None if not provided.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.extra_args","title":"extra_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_args: dict[str, Any] | None = None\n</code></pre> <p>Arbitrary keyword arguments to pass to the model API call. These will be passed directly to the underlying model provider's API. Use with caution as not all models support all parameters.</p>"},{"location":"ref/model_settings/#agents.model_settings.ModelSettings.resolve","title":"resolve","text":"<pre><code>resolve(override: ModelSettings | None) -&gt; ModelSettings\n</code></pre> <p>Produce a new ModelSettings by overlaying any non-None values from the override on top of this instance.</p> Source code in <code>src/agents/model_settings.py</code> <pre><code>def resolve(self, override: ModelSettings | None) -&gt; ModelSettings:\n    \"\"\"Produce a new ModelSettings by overlaying any non-None values from the\n    override on top of this instance.\"\"\"\n    if override is None:\n        return self\n\n    changes = {\n        field.name: getattr(override, field.name)\n        for field in fields(self)\n        if getattr(override, field.name) is not None\n    }\n\n    # Handle extra_args merging specially - merge dictionaries instead of replacing\n    if self.extra_args is not None or override.extra_args is not None:\n        merged_args = {}\n        if self.extra_args:\n            merged_args.update(self.extra_args)\n        if override.extra_args:\n            merged_args.update(override.extra_args)\n        changes[\"extra_args\"] = merged_args if merged_args else None\n\n    return replace(self, **changes)\n</code></pre>"},{"location":"ref/repl/","title":"<code>repl</code>","text":""},{"location":"ref/repl/#agents.repl.run_demo_loop","title":"run_demo_loop  <code>async</code>","text":"<pre><code>run_demo_loop(\n    agent: Agent[Any], *, stream: bool = True\n) -&gt; None\n</code></pre> <p>Run a simple REPL loop with the given agent.</p> <p>This utility allows quick manual testing and debugging of an agent from the command line. Conversation state is preserved across turns. Enter <code>exit</code> or <code>quit</code> to stop the loop.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[Any]</code> <p>The starting agent to run.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the agent output.</p> <code>True</code> Source code in <code>src/agents/repl.py</code> <pre><code>async def run_demo_loop(agent: Agent[Any], *, stream: bool = True) -&gt; None:\n    \"\"\"Run a simple REPL loop with the given agent.\n\n    This utility allows quick manual testing and debugging of an agent from the\n    command line. Conversation state is preserved across turns. Enter ``exit``\n    or ``quit`` to stop the loop.\n\n    Args:\n        agent: The starting agent to run.\n        stream: Whether to stream the agent output.\n    \"\"\"\n\n    current_agent = agent\n    input_items: list[TResponseInputItem] = []\n    while True:\n        try:\n            user_input = input(\" &gt; \")\n        except (EOFError, KeyboardInterrupt):\n            print()\n            break\n        if user_input.strip().lower() in {\"exit\", \"quit\"}:\n            break\n        if not user_input:\n            continue\n\n        input_items.append({\"role\": \"user\", \"content\": user_input})\n\n        result: RunResultBase\n        if stream:\n            result = Runner.run_streamed(current_agent, input=input_items)\n            async for event in result.stream_events():\n                if isinstance(event, RawResponsesStreamEvent):\n                    if isinstance(event.data, ResponseTextDeltaEvent):\n                        print(event.data.delta, end=\"\", flush=True)\n                elif isinstance(event, RunItemStreamEvent):\n                    if event.item.type == \"tool_call_item\":\n                        print(\"\\n[tool called]\", flush=True)\n                    elif event.item.type == \"tool_call_output_item\":\n                        print(f\"\\n[tool output: {event.item.output}]\", flush=True)\n                elif isinstance(event, AgentUpdatedStreamEvent):\n                    print(f\"\\n[Agent updated: {event.new_agent.name}]\", flush=True)\n            print()\n        else:\n            result = await Runner.run(current_agent, input_items)\n            if result.final_output is not None:\n                print(result.final_output)\n\n        current_agent = result.last_agent\n        input_items = result.to_input_list()\n</code></pre>"},{"location":"ref/result/","title":"<code>Results</code>","text":""},{"location":"ref/result/#agents.result.RunResultBase","title":"RunResultBase  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/agents/result.py</code> <pre><code>@dataclass\nclass RunResultBase(abc.ABC):\n    input: str | list[TResponseInputItem]\n    \"\"\"The original input items i.e. the items before run() was called. This may be a mutated\n    version of the input, if there are handoff input filters that mutate the input.\n    \"\"\"\n\n    new_items: list[RunItem]\n    \"\"\"The new items generated during the agent run. These include things like new messages, tool\n    calls and their outputs, etc.\n    \"\"\"\n\n    raw_responses: list[ModelResponse]\n    \"\"\"The raw LLM responses generated by the model during the agent run.\"\"\"\n\n    final_output: Any\n    \"\"\"The output of the last agent.\"\"\"\n\n    input_guardrail_results: list[InputGuardrailResult]\n    \"\"\"Guardrail results for the input messages.\"\"\"\n\n    output_guardrail_results: list[OutputGuardrailResult]\n    \"\"\"Guardrail results for the final output of the agent.\"\"\"\n\n    context_wrapper: RunContextWrapper[Any]\n    \"\"\"The context wrapper for the agent run.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def last_agent(self) -&gt; Agent[Any]:\n        \"\"\"The last agent that was run.\"\"\"\n\n    def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n        \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n        is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n        TypeError if the final output is not of the given type.\n\n        Args:\n            cls: The type to cast the final output to.\n            raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n                the given type.\n\n        Returns:\n            The final output casted to the given type.\n        \"\"\"\n        if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n            raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n        return cast(T, self.final_output)\n\n    def to_input_list(self) -&gt; list[TResponseInputItem]:\n        \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n        original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n        new_items = [item.to_input_item() for item in self.new_items]\n\n        return original_items + new_items\n\n    @property\n    def last_response_id(self) -&gt; str | None:\n        \"\"\"Convenience method to get the response ID of the last model response.\"\"\"\n        if not self.raw_responses:\n            return None\n\n        return self.raw_responses[-1].response_id\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultBase.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str | list[TResponseInputItem]\n</code></pre> <p>The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input.</p>"},{"location":"ref/result/#agents.result.RunResultBase.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: list[RunItem]\n</code></pre> <p>The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc.</p>"},{"location":"ref/result/#agents.result.RunResultBase.raw_responses","title":"raw_responses  <code>instance-attribute</code>","text":"<pre><code>raw_responses: list[ModelResponse]\n</code></pre> <p>The raw LLM responses generated by the model during the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.final_output","title":"final_output  <code>instance-attribute</code>","text":"<pre><code>final_output: Any\n</code></pre> <p>The output of the last agent.</p>"},{"location":"ref/result/#agents.result.RunResultBase.input_guardrail_results","title":"input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>input_guardrail_results: list[InputGuardrailResult]\n</code></pre> <p>Guardrail results for the input messages.</p>"},{"location":"ref/result/#agents.result.RunResultBase.output_guardrail_results","title":"output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>Guardrail results for the final output of the agent.</p>"},{"location":"ref/result/#agents.result.RunResultBase.context_wrapper","title":"context_wrapper  <code>instance-attribute</code>","text":"<pre><code>context_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The context wrapper for the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.last_agent","title":"last_agent  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>last_agent: Agent[Any]\n</code></pre> <p>The last agent that was run.</p>"},{"location":"ref/result/#agents.result.RunResultBase.last_response_id","title":"last_response_id  <code>property</code>","text":"<pre><code>last_response_id: str | None\n</code></pre> <p>Convenience method to get the response ID of the last model response.</p>"},{"location":"ref/result/#agents.result.RunResultBase.final_output_as","title":"final_output_as","text":"<pre><code>final_output_as(\n    cls: type[T], raise_if_incorrect_type: bool = False\n) -&gt; T\n</code></pre> <p>A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set <code>raise_if_incorrect_type</code> to True, we'll raise a TypeError if the final output is not of the given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The type to cast the final output to.</p> required <code>raise_if_incorrect_type</code> <code>bool</code> <p>If True, we'll raise a TypeError if the final output is not of the given type.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>The final output casted to the given type.</p> Source code in <code>src/agents/result.py</code> <pre><code>def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n    \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n    is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n    TypeError if the final output is not of the given type.\n\n    Args:\n        cls: The type to cast the final output to.\n        raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n            the given type.\n\n    Returns:\n        The final output casted to the given type.\n    \"\"\"\n    if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n        raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n    return cast(T, self.final_output)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultBase.to_input_list","title":"to_input_list","text":"<pre><code>to_input_list() -&gt; list[TResponseInputItem]\n</code></pre> <p>Creates a new input list, merging the original input with all the new items generated.</p> Source code in <code>src/agents/result.py</code> <pre><code>def to_input_list(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n    original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n    new_items = [item.to_input_item() for item in self.new_items]\n\n    return original_items + new_items\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult","title":"RunResult  <code>dataclass</code>","text":"<p>               Bases: <code>RunResultBase</code></p> Source code in <code>src/agents/result.py</code> <pre><code>@dataclass\nclass RunResult(RunResultBase):\n    _last_agent: Agent[Any]\n\n    @property\n    def last_agent(self) -&gt; Agent[Any]:\n        \"\"\"The last agent that was run.\"\"\"\n        return self._last_agent\n\n    def __str__(self) -&gt; str:\n        return pretty_print_result(self)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult.last_agent","title":"last_agent  <code>property</code>","text":"<pre><code>last_agent: Agent[Any]\n</code></pre> <p>The last agent that was run.</p>"},{"location":"ref/result/#agents.result.RunResult.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str | list[TResponseInputItem]\n</code></pre> <p>The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input.</p>"},{"location":"ref/result/#agents.result.RunResult.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: list[RunItem]\n</code></pre> <p>The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc.</p>"},{"location":"ref/result/#agents.result.RunResult.raw_responses","title":"raw_responses  <code>instance-attribute</code>","text":"<pre><code>raw_responses: list[ModelResponse]\n</code></pre> <p>The raw LLM responses generated by the model during the agent run.</p>"},{"location":"ref/result/#agents.result.RunResult.final_output","title":"final_output  <code>instance-attribute</code>","text":"<pre><code>final_output: Any\n</code></pre> <p>The output of the last agent.</p>"},{"location":"ref/result/#agents.result.RunResult.input_guardrail_results","title":"input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>input_guardrail_results: list[InputGuardrailResult]\n</code></pre> <p>Guardrail results for the input messages.</p>"},{"location":"ref/result/#agents.result.RunResult.output_guardrail_results","title":"output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>Guardrail results for the final output of the agent.</p>"},{"location":"ref/result/#agents.result.RunResult.context_wrapper","title":"context_wrapper  <code>instance-attribute</code>","text":"<pre><code>context_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The context wrapper for the agent run.</p>"},{"location":"ref/result/#agents.result.RunResult.last_response_id","title":"last_response_id  <code>property</code>","text":"<pre><code>last_response_id: str | None\n</code></pre> <p>Convenience method to get the response ID of the last model response.</p>"},{"location":"ref/result/#agents.result.RunResult.final_output_as","title":"final_output_as","text":"<pre><code>final_output_as(\n    cls: type[T], raise_if_incorrect_type: bool = False\n) -&gt; T\n</code></pre> <p>A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set <code>raise_if_incorrect_type</code> to True, we'll raise a TypeError if the final output is not of the given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The type to cast the final output to.</p> required <code>raise_if_incorrect_type</code> <code>bool</code> <p>If True, we'll raise a TypeError if the final output is not of the given type.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>The final output casted to the given type.</p> Source code in <code>src/agents/result.py</code> <pre><code>def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n    \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n    is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n    TypeError if the final output is not of the given type.\n\n    Args:\n        cls: The type to cast the final output to.\n        raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n            the given type.\n\n    Returns:\n        The final output casted to the given type.\n    \"\"\"\n    if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n        raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n    return cast(T, self.final_output)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResult.to_input_list","title":"to_input_list","text":"<pre><code>to_input_list() -&gt; list[TResponseInputItem]\n</code></pre> <p>Creates a new input list, merging the original input with all the new items generated.</p> Source code in <code>src/agents/result.py</code> <pre><code>def to_input_list(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n    original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n    new_items = [item.to_input_item() for item in self.new_items]\n\n    return original_items + new_items\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming","title":"RunResultStreaming  <code>dataclass</code>","text":"<p>               Bases: <code>RunResultBase</code></p> <p>The result of an agent run in streaming mode. You can use the <code>stream_events</code> method to receive semantic events as they are generated.</p> <p>The streaming method will raise: - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit. - A GuardrailTripwireTriggered exception if a guardrail is tripped.</p> Source code in <code>src/agents/result.py</code> <pre><code>@dataclass\nclass RunResultStreaming(RunResultBase):\n    \"\"\"The result of an agent run in streaming mode. You can use the `stream_events` method to\n    receive semantic events as they are generated.\n\n    The streaming method will raise:\n    - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n    - A GuardrailTripwireTriggered exception if a guardrail is tripped.\n    \"\"\"\n\n    current_agent: Agent[Any]\n    \"\"\"The current agent that is running.\"\"\"\n\n    current_turn: int\n    \"\"\"The current turn number.\"\"\"\n\n    max_turns: int\n    \"\"\"The maximum number of turns the agent can run for.\"\"\"\n\n    final_output: Any\n    \"\"\"The final output of the agent. This is None until the agent has finished running.\"\"\"\n\n    _current_agent_output_schema: AgentOutputSchemaBase | None = field(repr=False)\n\n    trace: Trace | None = field(repr=False)\n\n    is_complete: bool = False\n    \"\"\"Whether the agent has finished running.\"\"\"\n\n    # Queues that the background run_loop writes to\n    _event_queue: asyncio.Queue[StreamEvent | QueueCompleteSentinel] = field(\n        default_factory=asyncio.Queue, repr=False\n    )\n    _input_guardrail_queue: asyncio.Queue[InputGuardrailResult] = field(\n        default_factory=asyncio.Queue, repr=False\n    )\n\n    # Store the asyncio tasks that we're waiting on\n    _run_impl_task: asyncio.Task[Any] | None = field(default=None, repr=False)\n    _input_guardrails_task: asyncio.Task[Any] | None = field(default=None, repr=False)\n    _output_guardrails_task: asyncio.Task[Any] | None = field(default=None, repr=False)\n    _stored_exception: Exception | None = field(default=None, repr=False)\n\n    @property\n    def last_agent(self) -&gt; Agent[Any]:\n        \"\"\"The last agent that was run. Updates as the agent run progresses, so the true last agent\n        is only available after the agent run is complete.\n        \"\"\"\n        return self.current_agent\n\n    def cancel(self) -&gt; None:\n        \"\"\"Cancels the streaming run, stopping all background tasks and marking the run as\n        complete.\"\"\"\n        self._cleanup_tasks()  # Cancel all running tasks\n        self.is_complete = True  # Mark the run as complete to stop event streaming\n\n        # Optionally, clear the event queue to prevent processing stale events\n        while not self._event_queue.empty():\n            self._event_queue.get_nowait()\n        while not self._input_guardrail_queue.empty():\n            self._input_guardrail_queue.get_nowait()\n\n    async def stream_events(self) -&gt; AsyncIterator[StreamEvent]:\n        \"\"\"Stream deltas for new items as they are generated. We're using the types from the\n        OpenAI Responses API, so these are semantic events: each event has a `type` field that\n        describes the type of the event, along with the data for that event.\n\n        This will raise:\n        - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n        - A GuardrailTripwireTriggered exception if a guardrail is tripped.\n        \"\"\"\n        while True:\n            self._check_errors()\n            if self._stored_exception:\n                logger.debug(\"Breaking due to stored exception\")\n                self.is_complete = True\n                break\n\n            if self.is_complete and self._event_queue.empty():\n                break\n\n            try:\n                item = await self._event_queue.get()\n            except asyncio.CancelledError:\n                break\n\n            if isinstance(item, QueueCompleteSentinel):\n                self._event_queue.task_done()\n                # Check for errors, in case the queue was completed due to an exception\n                self._check_errors()\n                break\n\n            yield item\n            self._event_queue.task_done()\n\n        self._cleanup_tasks()\n\n        if self._stored_exception:\n            raise self._stored_exception\n\n    def _create_error_details(self) -&gt; RunErrorDetails:\n        \"\"\"Return a `RunErrorDetails` object considering the current attributes of the class.\"\"\"\n        return RunErrorDetails(\n            input=self.input,\n            new_items=self.new_items,\n            raw_responses=self.raw_responses,\n            last_agent=self.current_agent,\n            context_wrapper=self.context_wrapper,\n            input_guardrail_results=self.input_guardrail_results,\n            output_guardrail_results=self.output_guardrail_results,\n        )\n\n    def _check_errors(self):\n        if self.current_turn &gt; self.max_turns:\n            max_turns_exc = MaxTurnsExceeded(f\"Max turns ({self.max_turns}) exceeded\")\n            max_turns_exc.run_data = self._create_error_details()\n            self._stored_exception = max_turns_exc\n\n        # Fetch all the completed guardrail results from the queue and raise if needed\n        while not self._input_guardrail_queue.empty():\n            guardrail_result = self._input_guardrail_queue.get_nowait()\n            if guardrail_result.output.tripwire_triggered:\n                tripwire_exc = InputGuardrailTripwireTriggered(guardrail_result)\n                tripwire_exc.run_data = self._create_error_details()\n                self._stored_exception = tripwire_exc\n\n        # Check the tasks for any exceptions\n        if self._run_impl_task and self._run_impl_task.done():\n            run_impl_exc = self._run_impl_task.exception()\n            if run_impl_exc and isinstance(run_impl_exc, Exception):\n                if isinstance(run_impl_exc, AgentsException) and run_impl_exc.run_data is None:\n                    run_impl_exc.run_data = self._create_error_details()\n                self._stored_exception = run_impl_exc\n\n        if self._input_guardrails_task and self._input_guardrails_task.done():\n            in_guard_exc = self._input_guardrails_task.exception()\n            if in_guard_exc and isinstance(in_guard_exc, Exception):\n                if isinstance(in_guard_exc, AgentsException) and in_guard_exc.run_data is None:\n                    in_guard_exc.run_data = self._create_error_details()\n                self._stored_exception = in_guard_exc\n\n        if self._output_guardrails_task and self._output_guardrails_task.done():\n            out_guard_exc = self._output_guardrails_task.exception()\n            if out_guard_exc and isinstance(out_guard_exc, Exception):\n                if isinstance(out_guard_exc, AgentsException) and out_guard_exc.run_data is None:\n                    out_guard_exc.run_data = self._create_error_details()\n                self._stored_exception = out_guard_exc\n\n    def _cleanup_tasks(self):\n        if self._run_impl_task and not self._run_impl_task.done():\n            self._run_impl_task.cancel()\n\n        if self._input_guardrails_task and not self._input_guardrails_task.done():\n            self._input_guardrails_task.cancel()\n\n        if self._output_guardrails_task and not self._output_guardrails_task.done():\n            self._output_guardrails_task.cancel()\n\n    def __str__(self) -&gt; str:\n        return pretty_print_run_result_streaming(self)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.current_agent","title":"current_agent  <code>instance-attribute</code>","text":"<pre><code>current_agent: Agent[Any]\n</code></pre> <p>The current agent that is running.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.current_turn","title":"current_turn  <code>instance-attribute</code>","text":"<pre><code>current_turn: int\n</code></pre> <p>The current turn number.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.max_turns","title":"max_turns  <code>instance-attribute</code>","text":"<pre><code>max_turns: int\n</code></pre> <p>The maximum number of turns the agent can run for.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.final_output","title":"final_output  <code>instance-attribute</code>","text":"<pre><code>final_output: Any\n</code></pre> <p>The final output of the agent. This is None until the agent has finished running.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.is_complete","title":"is_complete  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_complete: bool = False\n</code></pre> <p>Whether the agent has finished running.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.last_agent","title":"last_agent  <code>property</code>","text":"<pre><code>last_agent: Agent[Any]\n</code></pre> <p>The last agent that was run. Updates as the agent run progresses, so the true last agent is only available after the agent run is complete.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.input","title":"input  <code>instance-attribute</code>","text":"<pre><code>input: str | list[TResponseInputItem]\n</code></pre> <p>The original input items i.e. the items before run() was called. This may be a mutated version of the input, if there are handoff input filters that mutate the input.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.new_items","title":"new_items  <code>instance-attribute</code>","text":"<pre><code>new_items: list[RunItem]\n</code></pre> <p>The new items generated during the agent run. These include things like new messages, tool calls and their outputs, etc.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.raw_responses","title":"raw_responses  <code>instance-attribute</code>","text":"<pre><code>raw_responses: list[ModelResponse]\n</code></pre> <p>The raw LLM responses generated by the model during the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.input_guardrail_results","title":"input_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>input_guardrail_results: list[InputGuardrailResult]\n</code></pre> <p>Guardrail results for the input messages.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.output_guardrail_results","title":"output_guardrail_results  <code>instance-attribute</code>","text":"<pre><code>output_guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>Guardrail results for the final output of the agent.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.context_wrapper","title":"context_wrapper  <code>instance-attribute</code>","text":"<pre><code>context_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The context wrapper for the agent run.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.last_response_id","title":"last_response_id  <code>property</code>","text":"<pre><code>last_response_id: str | None\n</code></pre> <p>Convenience method to get the response ID of the last model response.</p>"},{"location":"ref/result/#agents.result.RunResultStreaming.cancel","title":"cancel","text":"<pre><code>cancel() -&gt; None\n</code></pre> <p>Cancels the streaming run, stopping all background tasks and marking the run as complete.</p> Source code in <code>src/agents/result.py</code> <pre><code>def cancel(self) -&gt; None:\n    \"\"\"Cancels the streaming run, stopping all background tasks and marking the run as\n    complete.\"\"\"\n    self._cleanup_tasks()  # Cancel all running tasks\n    self.is_complete = True  # Mark the run as complete to stop event streaming\n\n    # Optionally, clear the event queue to prevent processing stale events\n    while not self._event_queue.empty():\n        self._event_queue.get_nowait()\n    while not self._input_guardrail_queue.empty():\n        self._input_guardrail_queue.get_nowait()\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.stream_events","title":"stream_events  <code>async</code>","text":"<pre><code>stream_events() -&gt; AsyncIterator[StreamEvent]\n</code></pre> <p>Stream deltas for new items as they are generated. We're using the types from the OpenAI Responses API, so these are semantic events: each event has a <code>type</code> field that describes the type of the event, along with the data for that event.</p> <p>This will raise: - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit. - A GuardrailTripwireTriggered exception if a guardrail is tripped.</p> Source code in <code>src/agents/result.py</code> <pre><code>async def stream_events(self) -&gt; AsyncIterator[StreamEvent]:\n    \"\"\"Stream deltas for new items as they are generated. We're using the types from the\n    OpenAI Responses API, so these are semantic events: each event has a `type` field that\n    describes the type of the event, along with the data for that event.\n\n    This will raise:\n    - A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n    - A GuardrailTripwireTriggered exception if a guardrail is tripped.\n    \"\"\"\n    while True:\n        self._check_errors()\n        if self._stored_exception:\n            logger.debug(\"Breaking due to stored exception\")\n            self.is_complete = True\n            break\n\n        if self.is_complete and self._event_queue.empty():\n            break\n\n        try:\n            item = await self._event_queue.get()\n        except asyncio.CancelledError:\n            break\n\n        if isinstance(item, QueueCompleteSentinel):\n            self._event_queue.task_done()\n            # Check for errors, in case the queue was completed due to an exception\n            self._check_errors()\n            break\n\n        yield item\n        self._event_queue.task_done()\n\n    self._cleanup_tasks()\n\n    if self._stored_exception:\n        raise self._stored_exception\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.final_output_as","title":"final_output_as","text":"<pre><code>final_output_as(\n    cls: type[T], raise_if_incorrect_type: bool = False\n) -&gt; T\n</code></pre> <p>A convenience method to cast the final output to a specific type. By default, the cast is only for the typechecker. If you set <code>raise_if_incorrect_type</code> to True, we'll raise a TypeError if the final output is not of the given type.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[T]</code> <p>The type to cast the final output to.</p> required <code>raise_if_incorrect_type</code> <code>bool</code> <p>If True, we'll raise a TypeError if the final output is not of the given type.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>The final output casted to the given type.</p> Source code in <code>src/agents/result.py</code> <pre><code>def final_output_as(self, cls: type[T], raise_if_incorrect_type: bool = False) -&gt; T:\n    \"\"\"A convenience method to cast the final output to a specific type. By default, the cast\n    is only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\n    TypeError if the final output is not of the given type.\n\n    Args:\n        cls: The type to cast the final output to.\n        raise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\n            the given type.\n\n    Returns:\n        The final output casted to the given type.\n    \"\"\"\n    if raise_if_incorrect_type and not isinstance(self.final_output, cls):\n        raise TypeError(f\"Final output is not of type {cls.__name__}\")\n\n    return cast(T, self.final_output)\n</code></pre>"},{"location":"ref/result/#agents.result.RunResultStreaming.to_input_list","title":"to_input_list","text":"<pre><code>to_input_list() -&gt; list[TResponseInputItem]\n</code></pre> <p>Creates a new input list, merging the original input with all the new items generated.</p> Source code in <code>src/agents/result.py</code> <pre><code>def to_input_list(self) -&gt; list[TResponseInputItem]:\n    \"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\n    original_items: list[TResponseInputItem] = ItemHelpers.input_to_new_input_list(self.input)\n    new_items = [item.to_input_item() for item in self.new_items]\n\n    return original_items + new_items\n</code></pre>"},{"location":"ref/run/","title":"<code>Runner</code>","text":""},{"location":"ref/run/#agents.run.Runner","title":"Runner","text":"Source code in <code>src/agents/run.py</code> <pre><code>class Runner:\n    @classmethod\n    async def run(\n        cls,\n        starting_agent: Agent[TContext],\n        input: str | list[TResponseInputItem],\n        *,\n        context: TContext | None = None,\n        max_turns: int = DEFAULT_MAX_TURNS,\n        hooks: RunHooks[TContext] | None = None,\n        run_config: RunConfig | None = None,\n        previous_response_id: str | None = None,\n        session: Session | None = None,\n    ) -&gt; RunResult:\n        \"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\n        output is generated. The loop runs like so:\n        1. The agent is invoked with the given input.\n        2. If there is a final output (i.e. the agent produces something of type\n            `agent.output_type`, the loop terminates.\n        3. If there's a handoff, we run the loop again, with the new agent.\n        4. Else, we run tool calls (if any), and re-run the loop.\n        In two cases, the agent may raise an exception:\n        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\n        Note that only the first agent's input guardrails are run.\n        Args:\n            starting_agent: The starting agent to run.\n            input: The initial input to the agent. You can pass a single string for a user message,\n                or a list of input items.\n            context: The context to run the agent with.\n            max_turns: The maximum number of turns to run the agent for. A turn is defined as one\n                AI invocation (including any tool calls that might occur).\n            hooks: An object that receives callbacks on various lifecycle events.\n            run_config: Global settings for the entire agent run.\n            previous_response_id: The ID of the previous response, if using OpenAI models via the\n                Responses API, this allows you to skip passing in input from the previous turn.\n        Returns:\n            A run result containing all the inputs, guardrail results and the output of the last\n            agent. Agents may perform handoffs, so we don't know the specific type of the output.\n        \"\"\"\n        runner = DEFAULT_AGENT_RUNNER\n        return await runner.run(\n            starting_agent,\n            input,\n            context=context,\n            max_turns=max_turns,\n            hooks=hooks,\n            run_config=run_config,\n            previous_response_id=previous_response_id,\n            session=session,\n        )\n\n    @classmethod\n    def run_sync(\n        cls,\n        starting_agent: Agent[TContext],\n        input: str | list[TResponseInputItem],\n        *,\n        context: TContext | None = None,\n        max_turns: int = DEFAULT_MAX_TURNS,\n        hooks: RunHooks[TContext] | None = None,\n        run_config: RunConfig | None = None,\n        previous_response_id: str | None = None,\n        session: Session | None = None,\n    ) -&gt; RunResult:\n        \"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the\n        `run` method, so it will not work if there's already an event loop (e.g. inside an async\n        function, or in a Jupyter notebook or async context like FastAPI). For those cases, use\n        the `run` method instead.\n        The agent will run in a loop until a final output is generated. The loop runs like so:\n        1. The agent is invoked with the given input.\n        2. If there is a final output (i.e. the agent produces something of type\n            `agent.output_type`, the loop terminates.\n        3. If there's a handoff, we run the loop again, with the new agent.\n        4. Else, we run tool calls (if any), and re-run the loop.\n        In two cases, the agent may raise an exception:\n        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\n        Note that only the first agent's input guardrails are run.\n        Args:\n            starting_agent: The starting agent to run.\n            input: The initial input to the agent. You can pass a single string for a user message,\n                or a list of input items.\n            context: The context to run the agent with.\n            max_turns: The maximum number of turns to run the agent for. A turn is defined as one\n                AI invocation (including any tool calls that might occur).\n            hooks: An object that receives callbacks on various lifecycle events.\n            run_config: Global settings for the entire agent run.\n            previous_response_id: The ID of the previous response, if using OpenAI models via the\n                Responses API, this allows you to skip passing in input from the previous turn.\n        Returns:\n            A run result containing all the inputs, guardrail results and the output of the last\n            agent. Agents may perform handoffs, so we don't know the specific type of the output.\n        \"\"\"\n        runner = DEFAULT_AGENT_RUNNER\n        return runner.run_sync(\n            starting_agent,\n            input,\n            context=context,\n            max_turns=max_turns,\n            hooks=hooks,\n            run_config=run_config,\n            previous_response_id=previous_response_id,\n            session=session,\n        )\n\n    @classmethod\n    def run_streamed(\n        cls,\n        starting_agent: Agent[TContext],\n        input: str | list[TResponseInputItem],\n        context: TContext | None = None,\n        max_turns: int = DEFAULT_MAX_TURNS,\n        hooks: RunHooks[TContext] | None = None,\n        run_config: RunConfig | None = None,\n        previous_response_id: str | None = None,\n        session: Session | None = None,\n    ) -&gt; RunResultStreaming:\n        \"\"\"Run a workflow starting at the given agent in streaming mode. The returned result object\n        contains a method you can use to stream semantic events as they are generated.\n        The agent will run in a loop until a final output is generated. The loop runs like so:\n        1. The agent is invoked with the given input.\n        2. If there is a final output (i.e. the agent produces something of type\n            `agent.output_type`, the loop terminates.\n        3. If there's a handoff, we run the loop again, with the new agent.\n        4. Else, we run tool calls (if any), and re-run the loop.\n        In two cases, the agent may raise an exception:\n        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\n        Note that only the first agent's input guardrails are run.\n        Args:\n            starting_agent: The starting agent to run.\n            input: The initial input to the agent. You can pass a single string for a user message,\n                or a list of input items.\n            context: The context to run the agent with.\n            max_turns: The maximum number of turns to run the agent for. A turn is defined as one\n                AI invocation (including any tool calls that might occur).\n            hooks: An object that receives callbacks on various lifecycle events.\n            run_config: Global settings for the entire agent run.\n            previous_response_id: The ID of the previous response, if using OpenAI models via the\n                Responses API, this allows you to skip passing in input from the previous turn.\n        Returns:\n            A result object that contains data about the run, as well as a method to stream events.\n        \"\"\"\n        runner = DEFAULT_AGENT_RUNNER\n        return runner.run_streamed(\n            starting_agent,\n            input,\n            context=context,\n            max_turns=max_turns,\n            hooks=hooks,\n            run_config=run_config,\n            previous_response_id=previous_response_id,\n            session=session,\n        )\n</code></pre>"},{"location":"ref/run/#agents.run.Runner.run","title":"run  <code>async</code> <code>classmethod</code>","text":"<pre><code>run(\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult\n</code></pre> <p>Run a workflow starting at the given agent. The agent will run in a loop until a final output is generated. The loop runs like so: 1. The agent is invoked with the given input. 2. If there is a final output (i.e. the agent produces something of type     <code>agent.output_type</code>, the loop terminates. 3. If there's a handoff, we run the loop again, with the new agent. 4. Else, we run tool calls (if any), and re-run the loop. In two cases, the agent may raise an exception: 1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised. 2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised. Note that only the first agent's input guardrails are run. Args:     starting_agent: The starting agent to run.     input: The initial input to the agent. You can pass a single string for a user message,         or a list of input items.     context: The context to run the agent with.     max_turns: The maximum number of turns to run the agent for. A turn is defined as one         AI invocation (including any tool calls that might occur).     hooks: An object that receives callbacks on various lifecycle events.     run_config: Global settings for the entire agent run.     previous_response_id: The ID of the previous response, if using OpenAI models via the         Responses API, this allows you to skip passing in input from the previous turn. Returns:     A run result containing all the inputs, guardrail results and the output of the last     agent. Agents may perform handoffs, so we don't know the specific type of the output.</p> Source code in <code>src/agents/run.py</code> <pre><code>@classmethod\nasync def run(\n    cls,\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult:\n    \"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\n    output is generated. The loop runs like so:\n    1. The agent is invoked with the given input.\n    2. If there is a final output (i.e. the agent produces something of type\n        `agent.output_type`, the loop terminates.\n    3. If there's a handoff, we run the loop again, with the new agent.\n    4. Else, we run tool calls (if any), and re-run the loop.\n    In two cases, the agent may raise an exception:\n    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\n    Note that only the first agent's input guardrails are run.\n    Args:\n        starting_agent: The starting agent to run.\n        input: The initial input to the agent. You can pass a single string for a user message,\n            or a list of input items.\n        context: The context to run the agent with.\n        max_turns: The maximum number of turns to run the agent for. A turn is defined as one\n            AI invocation (including any tool calls that might occur).\n        hooks: An object that receives callbacks on various lifecycle events.\n        run_config: Global settings for the entire agent run.\n        previous_response_id: The ID of the previous response, if using OpenAI models via the\n            Responses API, this allows you to skip passing in input from the previous turn.\n    Returns:\n        A run result containing all the inputs, guardrail results and the output of the last\n        agent. Agents may perform handoffs, so we don't know the specific type of the output.\n    \"\"\"\n    runner = DEFAULT_AGENT_RUNNER\n    return await runner.run(\n        starting_agent,\n        input,\n        context=context,\n        max_turns=max_turns,\n        hooks=hooks,\n        run_config=run_config,\n        previous_response_id=previous_response_id,\n        session=session,\n    )\n</code></pre>"},{"location":"ref/run/#agents.run.Runner.run_sync","title":"run_sync  <code>classmethod</code>","text":"<pre><code>run_sync(\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult\n</code></pre> <p>Run a workflow synchronously, starting at the given agent. Note that this just wraps the <code>run</code> method, so it will not work if there's already an event loop (e.g. inside an async function, or in a Jupyter notebook or async context like FastAPI). For those cases, use the <code>run</code> method instead. The agent will run in a loop until a final output is generated. The loop runs like so: 1. The agent is invoked with the given input. 2. If there is a final output (i.e. the agent produces something of type     <code>agent.output_type</code>, the loop terminates. 3. If there's a handoff, we run the loop again, with the new agent. 4. Else, we run tool calls (if any), and re-run the loop. In two cases, the agent may raise an exception: 1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised. 2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised. Note that only the first agent's input guardrails are run. Args:     starting_agent: The starting agent to run.     input: The initial input to the agent. You can pass a single string for a user message,         or a list of input items.     context: The context to run the agent with.     max_turns: The maximum number of turns to run the agent for. A turn is defined as one         AI invocation (including any tool calls that might occur).     hooks: An object that receives callbacks on various lifecycle events.     run_config: Global settings for the entire agent run.     previous_response_id: The ID of the previous response, if using OpenAI models via the         Responses API, this allows you to skip passing in input from the previous turn. Returns:     A run result containing all the inputs, guardrail results and the output of the last     agent. Agents may perform handoffs, so we don't know the specific type of the output.</p> Source code in <code>src/agents/run.py</code> <pre><code>@classmethod\ndef run_sync(\n    cls,\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResult:\n    \"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the\n    `run` method, so it will not work if there's already an event loop (e.g. inside an async\n    function, or in a Jupyter notebook or async context like FastAPI). For those cases, use\n    the `run` method instead.\n    The agent will run in a loop until a final output is generated. The loop runs like so:\n    1. The agent is invoked with the given input.\n    2. If there is a final output (i.e. the agent produces something of type\n        `agent.output_type`, the loop terminates.\n    3. If there's a handoff, we run the loop again, with the new agent.\n    4. Else, we run tool calls (if any), and re-run the loop.\n    In two cases, the agent may raise an exception:\n    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\n    Note that only the first agent's input guardrails are run.\n    Args:\n        starting_agent: The starting agent to run.\n        input: The initial input to the agent. You can pass a single string for a user message,\n            or a list of input items.\n        context: The context to run the agent with.\n        max_turns: The maximum number of turns to run the agent for. A turn is defined as one\n            AI invocation (including any tool calls that might occur).\n        hooks: An object that receives callbacks on various lifecycle events.\n        run_config: Global settings for the entire agent run.\n        previous_response_id: The ID of the previous response, if using OpenAI models via the\n            Responses API, this allows you to skip passing in input from the previous turn.\n    Returns:\n        A run result containing all the inputs, guardrail results and the output of the last\n        agent. Agents may perform handoffs, so we don't know the specific type of the output.\n    \"\"\"\n    runner = DEFAULT_AGENT_RUNNER\n    return runner.run_sync(\n        starting_agent,\n        input,\n        context=context,\n        max_turns=max_turns,\n        hooks=hooks,\n        run_config=run_config,\n        previous_response_id=previous_response_id,\n        session=session,\n    )\n</code></pre>"},{"location":"ref/run/#agents.run.Runner.run_streamed","title":"run_streamed  <code>classmethod</code>","text":"<pre><code>run_streamed(\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResultStreaming\n</code></pre> <p>Run a workflow starting at the given agent in streaming mode. The returned result object contains a method you can use to stream semantic events as they are generated. The agent will run in a loop until a final output is generated. The loop runs like so: 1. The agent is invoked with the given input. 2. If there is a final output (i.e. the agent produces something of type     <code>agent.output_type</code>, the loop terminates. 3. If there's a handoff, we run the loop again, with the new agent. 4. Else, we run tool calls (if any), and re-run the loop. In two cases, the agent may raise an exception: 1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised. 2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised. Note that only the first agent's input guardrails are run. Args:     starting_agent: The starting agent to run.     input: The initial input to the agent. You can pass a single string for a user message,         or a list of input items.     context: The context to run the agent with.     max_turns: The maximum number of turns to run the agent for. A turn is defined as one         AI invocation (including any tool calls that might occur).     hooks: An object that receives callbacks on various lifecycle events.     run_config: Global settings for the entire agent run.     previous_response_id: The ID of the previous response, if using OpenAI models via the         Responses API, this allows you to skip passing in input from the previous turn. Returns:     A result object that contains data about the run, as well as a method to stream events.</p> Source code in <code>src/agents/run.py</code> <pre><code>@classmethod\ndef run_streamed(\n    cls,\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n    session: Session | None = None,\n) -&gt; RunResultStreaming:\n    \"\"\"Run a workflow starting at the given agent in streaming mode. The returned result object\n    contains a method you can use to stream semantic events as they are generated.\n    The agent will run in a loop until a final output is generated. The loop runs like so:\n    1. The agent is invoked with the given input.\n    2. If there is a final output (i.e. the agent produces something of type\n        `agent.output_type`, the loop terminates.\n    3. If there's a handoff, we run the loop again, with the new agent.\n    4. Else, we run tool calls (if any), and re-run the loop.\n    In two cases, the agent may raise an exception:\n    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\n    Note that only the first agent's input guardrails are run.\n    Args:\n        starting_agent: The starting agent to run.\n        input: The initial input to the agent. You can pass a single string for a user message,\n            or a list of input items.\n        context: The context to run the agent with.\n        max_turns: The maximum number of turns to run the agent for. A turn is defined as one\n            AI invocation (including any tool calls that might occur).\n        hooks: An object that receives callbacks on various lifecycle events.\n        run_config: Global settings for the entire agent run.\n        previous_response_id: The ID of the previous response, if using OpenAI models via the\n            Responses API, this allows you to skip passing in input from the previous turn.\n    Returns:\n        A result object that contains data about the run, as well as a method to stream events.\n    \"\"\"\n    runner = DEFAULT_AGENT_RUNNER\n    return runner.run_streamed(\n        starting_agent,\n        input,\n        context=context,\n        max_turns=max_turns,\n        hooks=hooks,\n        run_config=run_config,\n        previous_response_id=previous_response_id,\n        session=session,\n    )\n</code></pre>"},{"location":"ref/run/#agents.run.RunConfig","title":"RunConfig  <code>dataclass</code>","text":"<p>Configures settings for the entire agent run.</p> Source code in <code>src/agents/run.py</code> <pre><code>@dataclass\nclass RunConfig:\n    \"\"\"Configures settings for the entire agent run.\"\"\"\n\n    model: str | Model | None = None\n    \"\"\"The model to use for the entire agent run. If set, will override the model set on every\n    agent. The model_provider passed in below must be able to resolve this model name.\n    \"\"\"\n\n    model_provider: ModelProvider = field(default_factory=MultiProvider)\n    \"\"\"The model provider to use when looking up string model names. Defaults to OpenAI.\"\"\"\n\n    model_settings: ModelSettings | None = None\n    \"\"\"Configure global model settings. Any non-null values will override the agent-specific model\n    settings.\n    \"\"\"\n\n    handoff_input_filter: HandoffInputFilter | None = None\n    \"\"\"A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that\n    will take precedence. The input filter allows you to edit the inputs that are sent to the new\n    agent. See the documentation in `Handoff.input_filter` for more details.\n    \"\"\"\n\n    input_guardrails: list[InputGuardrail[Any]] | None = None\n    \"\"\"A list of input guardrails to run on the initial run input.\"\"\"\n\n    output_guardrails: list[OutputGuardrail[Any]] | None = None\n    \"\"\"A list of output guardrails to run on the final output of the run.\"\"\"\n\n    tracing_disabled: bool = False\n    \"\"\"Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.\n    \"\"\"\n\n    trace_include_sensitive_data: bool = True\n    \"\"\"Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or\n    LLM generations) in traces. If False, we'll still create spans for these events, but the\n    sensitive data will not be included.\n    \"\"\"\n\n    workflow_name: str = \"Agent workflow\"\n    \"\"\"The name of the run, used for tracing. Should be a logical name for the run, like\n    \"Code generation workflow\" or \"Customer support agent\".\n    \"\"\"\n\n    trace_id: str | None = None\n    \"\"\"A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\"\"\"\n\n    group_id: str | None = None\n    \"\"\"\n    A grouping identifier to use for tracing, to link multiple traces from the same conversation\n    or process. For example, you might use a chat thread ID.\n    \"\"\"\n\n    trace_metadata: dict[str, Any] | None = None\n    \"\"\"\n    An optional dictionary of additional metadata to include with the trace.\n    \"\"\"\n</code></pre>"},{"location":"ref/run/#agents.run.RunConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str | Model | None = None\n</code></pre> <p>The model to use for the entire agent run. If set, will override the model set on every agent. The model_provider passed in below must be able to resolve this model name.</p>"},{"location":"ref/run/#agents.run.RunConfig.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: ModelProvider = field(\n    default_factory=MultiProvider\n)\n</code></pre> <p>The model provider to use when looking up string model names. Defaults to OpenAI.</p>"},{"location":"ref/run/#agents.run.RunConfig.model_settings","title":"model_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_settings: ModelSettings | None = None\n</code></pre> <p>Configure global model settings. Any non-null values will override the agent-specific model settings.</p>"},{"location":"ref/run/#agents.run.RunConfig.handoff_input_filter","title":"handoff_input_filter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_input_filter: HandoffInputFilter | None = None\n</code></pre> <p>A global input filter to apply to all handoffs. If <code>Handoff.input_filter</code> is set, then that will take precedence. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in <code>Handoff.input_filter</code> for more details.</p>"},{"location":"ref/run/#agents.run.RunConfig.input_guardrails","title":"input_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_guardrails: list[InputGuardrail[Any]] | None = None\n</code></pre> <p>A list of input guardrails to run on the initial run input.</p>"},{"location":"ref/run/#agents.run.RunConfig.output_guardrails","title":"output_guardrails  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_guardrails: list[OutputGuardrail[Any]] | None = None\n</code></pre> <p>A list of output guardrails to run on the final output of the run.</p>"},{"location":"ref/run/#agents.run.RunConfig.tracing_disabled","title":"tracing_disabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: bool = False\n</code></pre> <p>Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.</p>"},{"location":"ref/run/#agents.run.RunConfig.trace_include_sensitive_data","title":"trace_include_sensitive_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_data: bool = True\n</code></pre> <p>Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or LLM generations) in traces. If False, we'll still create spans for these events, but the sensitive data will not be included.</p>"},{"location":"ref/run/#agents.run.RunConfig.workflow_name","title":"workflow_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workflow_name: str = 'Agent workflow'\n</code></pre> <p>The name of the run, used for tracing. Should be a logical name for the run, like \"Code generation workflow\" or \"Customer support agent\".</p>"},{"location":"ref/run/#agents.run.RunConfig.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: str | None = None\n</code></pre> <p>A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.</p>"},{"location":"ref/run/#agents.run.RunConfig.group_id","title":"group_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_id: str | None = None\n</code></pre> <p>A grouping identifier to use for tracing, to link multiple traces from the same conversation or process. For example, you might use a chat thread ID.</p>"},{"location":"ref/run/#agents.run.RunConfig.trace_metadata","title":"trace_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_metadata: dict[str, Any] | None = None\n</code></pre> <p>An optional dictionary of additional metadata to include with the trace.</p>"},{"location":"ref/run_context/","title":"<code>Run context</code>","text":""},{"location":"ref/run_context/#agents.run_context.RunContextWrapper","title":"RunContextWrapper  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[TContext]</code></p> <p>This wraps the context object that you passed to <code>Runner.run()</code>. It also contains information about the usage of the agent run so far.</p> <p>NOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code you implement, like tool functions, callbacks, hooks, etc.</p> Source code in <code>src/agents/run_context.py</code> <pre><code>@dataclass\nclass RunContextWrapper(Generic[TContext]):\n    \"\"\"This wraps the context object that you passed to `Runner.run()`. It also contains\n    information about the usage of the agent run so far.\n\n    NOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code\n    you implement, like tool functions, callbacks, hooks, etc.\n    \"\"\"\n\n    context: TContext\n    \"\"\"The context object (or None), passed by you to `Runner.run()`\"\"\"\n\n    usage: Usage = field(default_factory=Usage)\n    \"\"\"The usage of the agent run so far. For streamed responses, the usage will be stale until the\n    last chunk of the stream is processed.\n    \"\"\"\n</code></pre>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: TContext\n</code></pre> <p>The context object (or None), passed by you to <code>Runner.run()</code></p>"},{"location":"ref/run_context/#agents.run_context.RunContextWrapper.usage","title":"usage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>usage: Usage = field(default_factory=Usage)\n</code></pre> <p>The usage of the agent run so far. For streamed responses, the usage will be stale until the last chunk of the stream is processed.</p>"},{"location":"ref/stream_events/","title":"<code>Streaming events</code>","text":""},{"location":"ref/stream_events/#agents.stream_events.StreamEvent","title":"StreamEvent  <code>module-attribute</code>","text":"<pre><code>StreamEvent: TypeAlias = Union[\n    RawResponsesStreamEvent,\n    RunItemStreamEvent,\n    AgentUpdatedStreamEvent,\n]\n</code></pre> <p>A streaming event from an agent.</p>"},{"location":"ref/stream_events/#agents.stream_events.RawResponsesStreamEvent","title":"RawResponsesStreamEvent  <code>dataclass</code>","text":"<p>Streaming event from the LLM. These are 'raw' events, i.e. they are directly passed through from the LLM.</p> Source code in <code>src/agents/stream_events.py</code> <pre><code>@dataclass\nclass RawResponsesStreamEvent:\n    \"\"\"Streaming event from the LLM. These are 'raw' events, i.e. they are directly passed through\n    from the LLM.\n    \"\"\"\n\n    data: TResponseStreamEvent\n    \"\"\"The raw responses streaming event from the LLM.\"\"\"\n\n    type: Literal[\"raw_response_event\"] = \"raw_response_event\"\n    \"\"\"The type of the event.\"\"\"\n</code></pre>"},{"location":"ref/stream_events/#agents.stream_events.RawResponsesStreamEvent.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: TResponseStreamEvent\n</code></pre> <p>The raw responses streaming event from the LLM.</p>"},{"location":"ref/stream_events/#agents.stream_events.RawResponsesStreamEvent.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal['raw_response_event'] = 'raw_response_event'\n</code></pre> <p>The type of the event.</p>"},{"location":"ref/stream_events/#agents.stream_events.RunItemStreamEvent","title":"RunItemStreamEvent  <code>dataclass</code>","text":"<p>Streaming events that wrap a <code>RunItem</code>. As the agent processes the LLM response, it will generate these events for new messages, tool calls, tool outputs, handoffs, etc.</p> Source code in <code>src/agents/stream_events.py</code> <pre><code>@dataclass\nclass RunItemStreamEvent:\n    \"\"\"Streaming events that wrap a `RunItem`. As the agent processes the LLM response, it will\n    generate these events for new messages, tool calls, tool outputs, handoffs, etc.\n    \"\"\"\n\n    name: Literal[\n        \"message_output_created\",\n        \"handoff_requested\",\n        # This is misspelled, but we can't change it because that would be a breaking change\n        \"handoff_occured\",\n        \"tool_called\",\n        \"tool_output\",\n        \"reasoning_item_created\",\n        \"mcp_approval_requested\",\n        \"mcp_list_tools\",\n    ]\n    \"\"\"The name of the event.\"\"\"\n\n    item: RunItem\n    \"\"\"The item that was created.\"\"\"\n\n    type: Literal[\"run_item_stream_event\"] = \"run_item_stream_event\"\n</code></pre>"},{"location":"ref/stream_events/#agents.stream_events.RunItemStreamEvent.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Literal[\n    \"message_output_created\",\n    \"handoff_requested\",\n    \"handoff_occured\",\n    \"tool_called\",\n    \"tool_output\",\n    \"reasoning_item_created\",\n    \"mcp_approval_requested\",\n    \"mcp_list_tools\",\n]\n</code></pre> <p>The name of the event.</p>"},{"location":"ref/stream_events/#agents.stream_events.RunItemStreamEvent.item","title":"item  <code>instance-attribute</code>","text":"<pre><code>item: RunItem\n</code></pre> <p>The item that was created.</p>"},{"location":"ref/stream_events/#agents.stream_events.AgentUpdatedStreamEvent","title":"AgentUpdatedStreamEvent  <code>dataclass</code>","text":"<p>Event that notifies that there is a new agent running.</p> Source code in <code>src/agents/stream_events.py</code> <pre><code>@dataclass\nclass AgentUpdatedStreamEvent:\n    \"\"\"Event that notifies that there is a new agent running.\"\"\"\n\n    new_agent: Agent[Any]\n    \"\"\"The new agent.\"\"\"\n\n    type: Literal[\"agent_updated_stream_event\"] = \"agent_updated_stream_event\"\n</code></pre>"},{"location":"ref/stream_events/#agents.stream_events.AgentUpdatedStreamEvent.new_agent","title":"new_agent  <code>instance-attribute</code>","text":"<pre><code>new_agent: Agent[Any]\n</code></pre> <p>The new agent.</p>"},{"location":"ref/tool/","title":"<code>Tools</code>","text":""},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunction","title":"MCPToolApprovalFunction  <code>module-attribute</code>","text":"<pre><code>MCPToolApprovalFunction = Callable[\n    [MCPToolApprovalRequest],\n    MaybeAwaitable[MCPToolApprovalFunctionResult],\n]\n</code></pre> <p>A function that approves or rejects a tool call.</p>"},{"location":"ref/tool/#agents.tool.LocalShellExecutor","title":"LocalShellExecutor  <code>module-attribute</code>","text":"<pre><code>LocalShellExecutor = Callable[\n    [LocalShellCommandRequest], MaybeAwaitable[str]\n]\n</code></pre> <p>A function that executes a command on a shell.</p>"},{"location":"ref/tool/#agents.tool.Tool","title":"Tool  <code>module-attribute</code>","text":"<pre><code>Tool = Union[\n    FunctionTool,\n    FileSearchTool,\n    WebSearchTool,\n    ComputerTool,\n    HostedMCPTool,\n    LocalShellTool,\n    ImageGenerationTool,\n    CodeInterpreterTool,\n]\n</code></pre> <p>A tool that can be used in an agent.</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult","title":"FunctionToolResult  <code>dataclass</code>","text":"Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass FunctionToolResult:\n    tool: FunctionTool\n    \"\"\"The tool that was run.\"\"\"\n\n    output: Any\n    \"\"\"The output of the tool.\"\"\"\n\n    run_item: RunItem\n    \"\"\"The run item that was produced as a result of the tool call.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.tool","title":"tool  <code>instance-attribute</code>","text":"<pre><code>tool: FunctionTool\n</code></pre> <p>The tool that was run.</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output of the tool.</p>"},{"location":"ref/tool/#agents.tool.FunctionToolResult.run_item","title":"run_item  <code>instance-attribute</code>","text":"<pre><code>run_item: RunItem\n</code></pre> <p>The run item that was produced as a result of the tool call.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool","title":"FunctionTool  <code>dataclass</code>","text":"<p>A tool that wraps a function. In most cases, you should use  the <code>function_tool</code> helpers to create a FunctionTool, as they let you easily wrap a Python function.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass FunctionTool:\n    \"\"\"A tool that wraps a function. In most cases, you should use  the `function_tool` helpers to\n    create a FunctionTool, as they let you easily wrap a Python function.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the tool, as shown to the LLM. Generally the name of the function.\"\"\"\n\n    description: str\n    \"\"\"A description of the tool, as shown to the LLM.\"\"\"\n\n    params_json_schema: dict[str, Any]\n    \"\"\"The JSON schema for the tool's parameters.\"\"\"\n\n    on_invoke_tool: Callable[[ToolContext[Any], str], Awaitable[Any]]\n    \"\"\"A function that invokes the tool with the given context and parameters. The params passed\n    are:\n    1. The tool run context.\n    2. The arguments from the LLM, as a JSON string.\n\n    You must return a string representation of the tool output, or something we can call `str()` on.\n    In case of errors, you can either raise an Exception (which will cause the run to fail) or\n    return a string error message (which will be sent back to the LLM).\n    \"\"\"\n\n    strict_json_schema: bool = True\n    \"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\n    as it increases the likelihood of correct JSON input.\"\"\"\n\n    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]] = True\n    \"\"\"Whether the tool is enabled. Either a bool or a Callable that takes the run context and agent\n    and returns whether the tool is enabled. You can use this to dynamically enable/disable a tool\n    based on your context/state.\"\"\"\n\n    def __post_init__(self):\n        if self.strict_json_schema:\n            self.params_json_schema = ensure_strict_json_schema(self.params_json_schema)\n</code></pre>"},{"location":"ref/tool/#agents.tool.FunctionTool.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool, as shown to the LLM. Generally the name of the function.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str\n</code></pre> <p>A description of the tool, as shown to the LLM.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.params_json_schema","title":"params_json_schema  <code>instance-attribute</code>","text":"<pre><code>params_json_schema: dict[str, Any]\n</code></pre> <p>The JSON schema for the tool's parameters.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.on_invoke_tool","title":"on_invoke_tool  <code>instance-attribute</code>","text":"<pre><code>on_invoke_tool: Callable[\n    [ToolContext[Any], str], Awaitable[Any]\n]\n</code></pre> <p>A function that invokes the tool with the given context and parameters. The params passed are: 1. The tool run context. 2. The arguments from the LLM, as a JSON string.</p> <p>You must return a string representation of the tool output, or something we can call <code>str()</code> on. In case of errors, you can either raise an Exception (which will cause the run to fail) or return a string error message (which will be sent back to the LLM).</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.strict_json_schema","title":"strict_json_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strict_json_schema: bool = True\n</code></pre> <p>Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.</p>"},{"location":"ref/tool/#agents.tool.FunctionTool.is_enabled","title":"is_enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_enabled: (\n    bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ]\n) = True\n</code></pre> <p>Whether the tool is enabled. Either a bool or a Callable that takes the run context and agent and returns whether the tool is enabled. You can use this to dynamically enable/disable a tool based on your context/state.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool","title":"FileSearchTool  <code>dataclass</code>","text":"<p>A hosted tool that lets the LLM search through a vector store. Currently only supported with OpenAI models, using the Responses API.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass FileSearchTool:\n    \"\"\"A hosted tool that lets the LLM search through a vector store. Currently only supported with\n    OpenAI models, using the Responses API.\n    \"\"\"\n\n    vector_store_ids: list[str]\n    \"\"\"The IDs of the vector stores to search.\"\"\"\n\n    max_num_results: int | None = None\n    \"\"\"The maximum number of results to return.\"\"\"\n\n    include_search_results: bool = False\n    \"\"\"Whether to include the search results in the output produced by the LLM.\"\"\"\n\n    ranking_options: RankingOptions | None = None\n    \"\"\"Ranking options for search.\"\"\"\n\n    filters: Filters | None = None\n    \"\"\"A filter to apply based on file attributes.\"\"\"\n\n    @property\n    def name(self):\n        return \"file_search\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.FileSearchTool.vector_store_ids","title":"vector_store_ids  <code>instance-attribute</code>","text":"<pre><code>vector_store_ids: list[str]\n</code></pre> <p>The IDs of the vector stores to search.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.max_num_results","title":"max_num_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_num_results: int | None = None\n</code></pre> <p>The maximum number of results to return.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.include_search_results","title":"include_search_results  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_search_results: bool = False\n</code></pre> <p>Whether to include the search results in the output produced by the LLM.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.ranking_options","title":"ranking_options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ranking_options: RankingOptions | None = None\n</code></pre> <p>Ranking options for search.</p>"},{"location":"ref/tool/#agents.tool.FileSearchTool.filters","title":"filters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filters: Filters | None = None\n</code></pre> <p>A filter to apply based on file attributes.</p>"},{"location":"ref/tool/#agents.tool.WebSearchTool","title":"WebSearchTool  <code>dataclass</code>","text":"<p>A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models, using the Responses API.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass WebSearchTool:\n    \"\"\"A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models,\n    using the Responses API.\n    \"\"\"\n\n    user_location: UserLocation | None = None\n    \"\"\"Optional location for the search. Lets you customize results to be relevant to a location.\"\"\"\n\n    search_context_size: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n    \"\"\"The amount of context to use for the search.\"\"\"\n\n    @property\n    def name(self):\n        return \"web_search_preview\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.WebSearchTool.user_location","title":"user_location  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_location: UserLocation | None = None\n</code></pre> <p>Optional location for the search. Lets you customize results to be relevant to a location.</p>"},{"location":"ref/tool/#agents.tool.WebSearchTool.search_context_size","title":"search_context_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>search_context_size: Literal[\"low\", \"medium\", \"high\"] = (\n    \"medium\"\n)\n</code></pre> <p>The amount of context to use for the search.</p>"},{"location":"ref/tool/#agents.tool.ComputerTool","title":"ComputerTool  <code>dataclass</code>","text":"<p>A hosted tool that lets the LLM control a computer.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ComputerTool:\n    \"\"\"A hosted tool that lets the LLM control a computer.\"\"\"\n\n    computer: Computer | AsyncComputer\n    \"\"\"The computer implementation, which describes the environment and dimensions of the computer,\n    as well as implements the computer actions like click, screenshot, etc.\n    \"\"\"\n\n    on_safety_check: Callable[[ComputerToolSafetyCheckData], MaybeAwaitable[bool]] | None = None\n    \"\"\"Optional callback to acknowledge computer tool safety checks.\"\"\"\n\n    @property\n    def name(self):\n        return \"computer_use_preview\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerTool.computer","title":"computer  <code>instance-attribute</code>","text":"<pre><code>computer: Computer | AsyncComputer\n</code></pre> <p>The computer implementation, which describes the environment and dimensions of the computer, as well as implements the computer actions like click, screenshot, etc.</p>"},{"location":"ref/tool/#agents.tool.ComputerTool.on_safety_check","title":"on_safety_check  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_safety_check: (\n    Callable[\n        [ComputerToolSafetyCheckData], MaybeAwaitable[bool]\n    ]\n    | None\n) = None\n</code></pre> <p>Optional callback to acknowledge computer tool safety checks.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData","title":"ComputerToolSafetyCheckData  <code>dataclass</code>","text":"<p>Information about a computer tool safety check.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ComputerToolSafetyCheckData:\n    \"\"\"Information about a computer tool safety check.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    agent: Agent[Any]\n    \"\"\"The agent performing the computer action.\"\"\"\n\n    tool_call: ResponseComputerToolCall\n    \"\"\"The computer tool call.\"\"\"\n\n    safety_check: PendingSafetyCheck\n    \"\"\"The pending safety check to acknowledge.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.ctx_wrapper","title":"ctx_wrapper  <code>instance-attribute</code>","text":"<pre><code>ctx_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: Agent[Any]\n</code></pre> <p>The agent performing the computer action.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.tool_call","title":"tool_call  <code>instance-attribute</code>","text":"<pre><code>tool_call: ResponseComputerToolCall\n</code></pre> <p>The computer tool call.</p>"},{"location":"ref/tool/#agents.tool.ComputerToolSafetyCheckData.safety_check","title":"safety_check  <code>instance-attribute</code>","text":"<pre><code>safety_check: PendingSafetyCheck\n</code></pre> <p>The pending safety check to acknowledge.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalRequest","title":"MCPToolApprovalRequest  <code>dataclass</code>","text":"<p>A request to approve a tool call.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass MCPToolApprovalRequest:\n    \"\"\"A request to approve a tool call.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    data: McpApprovalRequest\n    \"\"\"The data from the MCP tool approval request.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalRequest.ctx_wrapper","title":"ctx_wrapper  <code>instance-attribute</code>","text":"<pre><code>ctx_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalRequest.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: McpApprovalRequest\n</code></pre> <p>The data from the MCP tool approval request.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunctionResult","title":"MCPToolApprovalFunctionResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>The result of an MCP tool approval function.</p> Source code in <code>src/agents/tool.py</code> <pre><code>class MCPToolApprovalFunctionResult(TypedDict):\n    \"\"\"The result of an MCP tool approval function.\"\"\"\n\n    approve: bool\n    \"\"\"Whether to approve the tool call.\"\"\"\n\n    reason: NotRequired[str]\n    \"\"\"An optional reason, if rejected.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunctionResult.approve","title":"approve  <code>instance-attribute</code>","text":"<pre><code>approve: bool\n</code></pre> <p>Whether to approve the tool call.</p>"},{"location":"ref/tool/#agents.tool.MCPToolApprovalFunctionResult.reason","title":"reason  <code>instance-attribute</code>","text":"<pre><code>reason: NotRequired[str]\n</code></pre> <p>An optional reason, if rejected.</p>"},{"location":"ref/tool/#agents.tool.HostedMCPTool","title":"HostedMCPTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to use a remote MCP server. The LLM will automatically list and call tools, without requiring a round trip back to your code. If you want to run MCP servers locally via stdio, in a VPC or other non-publicly-accessible environment, or you just prefer to run tool calls locally, then you can instead use the servers in <code>agents.mcp</code> and pass <code>Agent(mcp_servers=[...])</code> to the agent.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass HostedMCPTool:\n    \"\"\"A tool that allows the LLM to use a remote MCP server. The LLM will automatically list and\n    call tools, without requiring a round trip back to your code.\n    If you want to run MCP servers locally via stdio, in a VPC or other non-publicly-accessible\n    environment, or you just prefer to run tool calls locally, then you can instead use the servers\n    in `agents.mcp` and pass `Agent(mcp_servers=[...])` to the agent.\"\"\"\n\n    tool_config: Mcp\n    \"\"\"The MCP tool config, which includes the server URL and other settings.\"\"\"\n\n    on_approval_request: MCPToolApprovalFunction | None = None\n    \"\"\"An optional function that will be called if approval is requested for an MCP tool. If not\n    provided, you will need to manually add approvals/rejections to the input and call\n    `Runner.run(...)` again.\"\"\"\n\n    @property\n    def name(self):\n        return \"hosted_mcp\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.HostedMCPTool.tool_config","title":"tool_config  <code>instance-attribute</code>","text":"<pre><code>tool_config: Mcp\n</code></pre> <p>The MCP tool config, which includes the server URL and other settings.</p>"},{"location":"ref/tool/#agents.tool.HostedMCPTool.on_approval_request","title":"on_approval_request  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_approval_request: MCPToolApprovalFunction | None = None\n</code></pre> <p>An optional function that will be called if approval is requested for an MCP tool. If not provided, you will need to manually add approvals/rejections to the input and call <code>Runner.run(...)</code> again.</p>"},{"location":"ref/tool/#agents.tool.CodeInterpreterTool","title":"CodeInterpreterTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to execute code in a sandboxed environment.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass CodeInterpreterTool:\n    \"\"\"A tool that allows the LLM to execute code in a sandboxed environment.\"\"\"\n\n    tool_config: CodeInterpreter\n    \"\"\"The tool config, which includes the container and other settings.\"\"\"\n\n    @property\n    def name(self):\n        return \"code_interpreter\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.CodeInterpreterTool.tool_config","title":"tool_config  <code>instance-attribute</code>","text":"<pre><code>tool_config: CodeInterpreter\n</code></pre> <p>The tool config, which includes the container and other settings.</p>"},{"location":"ref/tool/#agents.tool.ImageGenerationTool","title":"ImageGenerationTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to generate images.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass ImageGenerationTool:\n    \"\"\"A tool that allows the LLM to generate images.\"\"\"\n\n    tool_config: ImageGeneration\n    \"\"\"The tool config, which image generation settings.\"\"\"\n\n    @property\n    def name(self):\n        return \"image_generation\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.ImageGenerationTool.tool_config","title":"tool_config  <code>instance-attribute</code>","text":"<pre><code>tool_config: ImageGeneration\n</code></pre> <p>The tool config, which image generation settings.</p>"},{"location":"ref/tool/#agents.tool.LocalShellCommandRequest","title":"LocalShellCommandRequest  <code>dataclass</code>","text":"<p>A request to execute a command on a shell.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass LocalShellCommandRequest:\n    \"\"\"A request to execute a command on a shell.\"\"\"\n\n    ctx_wrapper: RunContextWrapper[Any]\n    \"\"\"The run context.\"\"\"\n\n    data: LocalShellCall\n    \"\"\"The data from the local shell tool call.\"\"\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.LocalShellCommandRequest.ctx_wrapper","title":"ctx_wrapper  <code>instance-attribute</code>","text":"<pre><code>ctx_wrapper: RunContextWrapper[Any]\n</code></pre> <p>The run context.</p>"},{"location":"ref/tool/#agents.tool.LocalShellCommandRequest.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: LocalShellCall\n</code></pre> <p>The data from the local shell tool call.</p>"},{"location":"ref/tool/#agents.tool.LocalShellTool","title":"LocalShellTool  <code>dataclass</code>","text":"<p>A tool that allows the LLM to execute commands on a shell.</p> Source code in <code>src/agents/tool.py</code> <pre><code>@dataclass\nclass LocalShellTool:\n    \"\"\"A tool that allows the LLM to execute commands on a shell.\"\"\"\n\n    executor: LocalShellExecutor\n    \"\"\"A function that executes a command on a shell.\"\"\"\n\n    @property\n    def name(self):\n        return \"local_shell\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.LocalShellTool.executor","title":"executor  <code>instance-attribute</code>","text":"<pre><code>executor: LocalShellExecutor\n</code></pre> <p>A function that executes a command on a shell.</p>"},{"location":"ref/tool/#agents.tool.default_tool_error_function","title":"default_tool_error_function","text":"<pre><code>default_tool_error_function(\n    ctx: RunContextWrapper[Any], error: Exception\n) -&gt; str\n</code></pre> <p>The default tool error function, which just returns a generic error message.</p> Source code in <code>src/agents/tool.py</code> <pre><code>def default_tool_error_function(ctx: RunContextWrapper[Any], error: Exception) -&gt; str:\n    \"\"\"The default tool error function, which just returns a generic error message.\"\"\"\n    return f\"An error occurred while running the tool. Please try again. Error: {str(error)}\"\n</code></pre>"},{"location":"ref/tool/#agents.tool.function_tool","title":"function_tool","text":"<pre><code>function_tool(\n    func: ToolFunction[...],\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction | None = None,\n    strict_mode: bool = True,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; FunctionTool\n</code></pre><pre><code>function_tool(\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction | None = None,\n    strict_mode: bool = True,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; Callable[[ToolFunction[...]], FunctionTool]\n</code></pre> <pre><code>function_tool(\n    func: ToolFunction[...] | None = None,\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction\n    | None = default_tool_error_function,\n    strict_mode: bool = True,\n    is_enabled: bool\n    | Callable[\n        [RunContextWrapper[Any], AgentBase],\n        MaybeAwaitable[bool],\n    ] = True,\n) -&gt; (\n    FunctionTool\n    | Callable[[ToolFunction[...]], FunctionTool]\n)\n</code></pre> <p>Decorator to create a FunctionTool from a function. By default, we will: 1. Parse the function signature to create a JSON schema for the tool's parameters. 2. Use the function's docstring to populate the tool's description. 3. Use the function's docstring to populate argument descriptions. The docstring style is detected automatically, but you can override it.</p> <p>If the function takes a <code>RunContextWrapper</code> as the first argument, it must match the context type of the agent that uses the tool.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>ToolFunction[...] | None</code> <p>The function to wrap.</p> <code>None</code> <code>name_override</code> <code>str | None</code> <p>If provided, use this name for the tool instead of the function's name.</p> <code>None</code> <code>description_override</code> <code>str | None</code> <p>If provided, use this description for the tool instead of the function's docstring.</p> <code>None</code> <code>docstring_style</code> <code>DocstringStyle | None</code> <p>If provided, use this style for the tool's docstring. If not provided, we will attempt to auto-detect the style.</p> <code>None</code> <code>use_docstring_info</code> <code>bool</code> <p>If True, use the function's docstring to populate the tool's description and argument descriptions.</p> <code>True</code> <code>failure_error_function</code> <code>ToolErrorFunction | None</code> <p>If provided, use this function to generate an error message when the tool call fails. The error message is sent to the LLM. If you pass None, then no error message will be sent and instead an Exception will be raised.</p> <code>default_tool_error_function</code> <code>strict_mode</code> <code>bool</code> <p>Whether to enable strict mode for the tool's JSON schema. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. If False, it allows non-strict JSON schemas. For example, if a parameter has a default value, it will be optional, additional properties are allowed, etc. See here for more: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas</p> <code>True</code> <code>is_enabled</code> <code>bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]]</code> <p>Whether the tool is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the tool is enabled. Disabled tools are hidden from the LLM at runtime.</p> <code>True</code> Source code in <code>src/agents/tool.py</code> <pre><code>def function_tool(\n    func: ToolFunction[...] | None = None,\n    *,\n    name_override: str | None = None,\n    description_override: str | None = None,\n    docstring_style: DocstringStyle | None = None,\n    use_docstring_info: bool = True,\n    failure_error_function: ToolErrorFunction | None = default_tool_error_function,\n    strict_mode: bool = True,\n    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]] = True,\n) -&gt; FunctionTool | Callable[[ToolFunction[...]], FunctionTool]:\n    \"\"\"\n    Decorator to create a FunctionTool from a function. By default, we will:\n    1. Parse the function signature to create a JSON schema for the tool's parameters.\n    2. Use the function's docstring to populate the tool's description.\n    3. Use the function's docstring to populate argument descriptions.\n    The docstring style is detected automatically, but you can override it.\n\n    If the function takes a `RunContextWrapper` as the first argument, it *must* match the\n    context type of the agent that uses the tool.\n\n    Args:\n        func: The function to wrap.\n        name_override: If provided, use this name for the tool instead of the function's name.\n        description_override: If provided, use this description for the tool instead of the\n            function's docstring.\n        docstring_style: If provided, use this style for the tool's docstring. If not provided,\n            we will attempt to auto-detect the style.\n        use_docstring_info: If True, use the function's docstring to populate the tool's\n            description and argument descriptions.\n        failure_error_function: If provided, use this function to generate an error message when\n            the tool call fails. The error message is sent to the LLM. If you pass None, then no\n            error message will be sent and instead an Exception will be raised.\n        strict_mode: Whether to enable strict mode for the tool's JSON schema. We *strongly*\n            recommend setting this to True, as it increases the likelihood of correct JSON input.\n            If False, it allows non-strict JSON schemas. For example, if a parameter has a default\n            value, it will be optional, additional properties are allowed, etc. See here for more:\n            https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas\n        is_enabled: Whether the tool is enabled. Can be a bool or a callable that takes the run\n            context and agent and returns whether the tool is enabled. Disabled tools are hidden\n            from the LLM at runtime.\n    \"\"\"\n\n    def _create_function_tool(the_func: ToolFunction[...]) -&gt; FunctionTool:\n        schema = function_schema(\n            func=the_func,\n            name_override=name_override,\n            description_override=description_override,\n            docstring_style=docstring_style,\n            use_docstring_info=use_docstring_info,\n            strict_json_schema=strict_mode,\n        )\n\n        async def _on_invoke_tool_impl(ctx: ToolContext[Any], input: str) -&gt; Any:\n            try:\n                json_data: dict[str, Any] = json.loads(input) if input else {}\n            except Exception as e:\n                if _debug.DONT_LOG_TOOL_DATA:\n                    logger.debug(f\"Invalid JSON input for tool {schema.name}\")\n                else:\n                    logger.debug(f\"Invalid JSON input for tool {schema.name}: {input}\")\n                raise ModelBehaviorError(\n                    f\"Invalid JSON input for tool {schema.name}: {input}\"\n                ) from e\n\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Invoking tool {schema.name}\")\n            else:\n                logger.debug(f\"Invoking tool {schema.name} with input {input}\")\n\n            try:\n                parsed = (\n                    schema.params_pydantic_model(**json_data)\n                    if json_data\n                    else schema.params_pydantic_model()\n                )\n            except ValidationError as e:\n                raise ModelBehaviorError(f\"Invalid JSON input for tool {schema.name}: {e}\") from e\n\n            args, kwargs_dict = schema.to_call_args(parsed)\n\n            if not _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Tool call args: {args}, kwargs: {kwargs_dict}\")\n\n            if inspect.iscoroutinefunction(the_func):\n                if schema.takes_context:\n                    result = await the_func(ctx, *args, **kwargs_dict)\n                else:\n                    result = await the_func(*args, **kwargs_dict)\n            else:\n                if schema.takes_context:\n                    result = the_func(ctx, *args, **kwargs_dict)\n                else:\n                    result = the_func(*args, **kwargs_dict)\n\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Tool {schema.name} completed.\")\n            else:\n                logger.debug(f\"Tool {schema.name} returned {result}\")\n\n            return result\n\n        async def _on_invoke_tool(ctx: ToolContext[Any], input: str) -&gt; Any:\n            try:\n                return await _on_invoke_tool_impl(ctx, input)\n            except Exception as e:\n                if failure_error_function is None:\n                    raise\n\n                result = failure_error_function(ctx, e)\n                if inspect.isawaitable(result):\n                    return await result\n\n                _error_tracing.attach_error_to_current_span(\n                    SpanError(\n                        message=\"Error running tool (non-fatal)\",\n                        data={\n                            \"tool_name\": schema.name,\n                            \"error\": str(e),\n                        },\n                    )\n                )\n                return result\n\n        return FunctionTool(\n            name=schema.name,\n            description=schema.description or \"\",\n            params_json_schema=schema.params_json_schema,\n            on_invoke_tool=_on_invoke_tool,\n            strict_json_schema=strict_mode,\n            is_enabled=is_enabled,\n        )\n\n    # If func is actually a callable, we were used as @function_tool with no parentheses\n    if callable(func):\n        return _create_function_tool(func)\n\n    # Otherwise, we were used as @function_tool(...), so return a decorator\n    def decorator(real_func: ToolFunction[...]) -&gt; FunctionTool:\n        return _create_function_tool(real_func)\n\n    return decorator\n</code></pre>"},{"location":"ref/usage/","title":"<code>Usage</code>","text":""},{"location":"ref/usage/#agents.usage.Usage","title":"Usage","text":"Source code in <code>src/agents/usage.py</code> <pre><code>@dataclass\nclass Usage:\n    requests: int = 0\n    \"\"\"Total requests made to the LLM API.\"\"\"\n\n    input_tokens: int = 0\n    \"\"\"Total input tokens sent, across all requests.\"\"\"\n\n    input_tokens_details: InputTokensDetails = field(\n        default_factory=lambda: InputTokensDetails(cached_tokens=0)\n    )\n    \"\"\"Details about the input tokens, matching responses API usage details.\"\"\"\n    output_tokens: int = 0\n    \"\"\"Total output tokens received, across all requests.\"\"\"\n\n    output_tokens_details: OutputTokensDetails = field(\n        default_factory=lambda: OutputTokensDetails(reasoning_tokens=0)\n    )\n    \"\"\"Details about the output tokens, matching responses API usage details.\"\"\"\n\n    total_tokens: int = 0\n    \"\"\"Total tokens sent and received, across all requests.\"\"\"\n\n    def add(self, other: \"Usage\") -&gt; None:\n        self.requests += other.requests if other.requests else 0\n        self.input_tokens += other.input_tokens if other.input_tokens else 0\n        self.output_tokens += other.output_tokens if other.output_tokens else 0\n        self.total_tokens += other.total_tokens if other.total_tokens else 0\n        self.input_tokens_details = InputTokensDetails(\n            cached_tokens=self.input_tokens_details.cached_tokens\n            + other.input_tokens_details.cached_tokens\n        )\n\n        self.output_tokens_details = OutputTokensDetails(\n            reasoning_tokens=self.output_tokens_details.reasoning_tokens\n            + other.output_tokens_details.reasoning_tokens\n        )\n</code></pre>"},{"location":"ref/usage/#agents.usage.Usage.requests","title":"requests  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requests: int = 0\n</code></pre> <p>Total requests made to the LLM API.</p>"},{"location":"ref/usage/#agents.usage.Usage.input_tokens","title":"input_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_tokens: int = 0\n</code></pre> <p>Total input tokens sent, across all requests.</p>"},{"location":"ref/usage/#agents.usage.Usage.input_tokens_details","title":"input_tokens_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_tokens_details: InputTokensDetails = field(\n    default_factory=lambda: InputTokensDetails(\n        cached_tokens=0\n    )\n)\n</code></pre> <p>Details about the input tokens, matching responses API usage details.</p>"},{"location":"ref/usage/#agents.usage.Usage.output_tokens","title":"output_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_tokens: int = 0\n</code></pre> <p>Total output tokens received, across all requests.</p>"},{"location":"ref/usage/#agents.usage.Usage.output_tokens_details","title":"output_tokens_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_tokens_details: OutputTokensDetails = field(\n    default_factory=lambda: OutputTokensDetails(\n        reasoning_tokens=0\n    )\n)\n</code></pre> <p>Details about the output tokens, matching responses API usage details.</p>"},{"location":"ref/usage/#agents.usage.Usage.total_tokens","title":"total_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_tokens: int = 0\n</code></pre> <p>Total tokens sent and received, across all requests.</p>"},{"location":"ref/extensions/handoff_filters/","title":"<code>Handoff filters</code>","text":""},{"location":"ref/extensions/handoff_filters/#agents.extensions.handoff_filters.remove_all_tools","title":"remove_all_tools","text":"<pre><code>remove_all_tools(\n    handoff_input_data: HandoffInputData,\n) -&gt; HandoffInputData\n</code></pre> <p>Filters out all tool items: file search, web search and function calls+output.</p> Source code in <code>src/agents/extensions/handoff_filters.py</code> <pre><code>def remove_all_tools(handoff_input_data: HandoffInputData) -&gt; HandoffInputData:\n    \"\"\"Filters out all tool items: file search, web search and function calls+output.\"\"\"\n\n    history = handoff_input_data.input_history\n    new_items = handoff_input_data.new_items\n\n    filtered_history = (\n        _remove_tool_types_from_input(history) if isinstance(history, tuple) else history\n    )\n    filtered_pre_handoff_items = _remove_tools_from_items(handoff_input_data.pre_handoff_items)\n    filtered_new_items = _remove_tools_from_items(new_items)\n\n    return HandoffInputData(\n        input_history=filtered_history,\n        pre_handoff_items=filtered_pre_handoff_items,\n        new_items=filtered_new_items,\n    )\n</code></pre>"},{"location":"ref/extensions/handoff_prompt/","title":"<code>Handoff prompt</code>","text":""},{"location":"ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX","title":"RECOMMENDED_PROMPT_PREFIX  <code>module-attribute</code>","text":"<pre><code>RECOMMENDED_PROMPT_PREFIX = \"# System context\\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_&lt;agent_name&gt;`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\\n\"\n</code></pre>"},{"location":"ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.prompt_with_handoff_instructions","title":"prompt_with_handoff_instructions","text":"<pre><code>prompt_with_handoff_instructions(prompt: str) -&gt; str\n</code></pre> <p>Add recommended instructions to the prompt for agents that use handoffs.</p> Source code in <code>src/agents/extensions/handoff_prompt.py</code> <pre><code>def prompt_with_handoff_instructions(prompt: str) -&gt; str:\n    \"\"\"\n    Add recommended instructions to the prompt for agents that use handoffs.\n    \"\"\"\n    return f\"{RECOMMENDED_PROMPT_PREFIX}\\n\\n{prompt}\"\n</code></pre>"},{"location":"ref/extensions/litellm/","title":"<code>LiteLLM Models</code>","text":""},{"location":"ref/extensions/litellm/#agents.extensions.models.litellm_model.LitellmModel","title":"LitellmModel","text":"<p>               Bases: <code>Model</code></p> <p>This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI, Anthropic, Gemini, Mistral, and many other models. See supported models here: litellm models.</p> Source code in <code>src/agents/extensions/models/litellm_model.py</code> <pre><code>class LitellmModel(Model):\n    \"\"\"This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,\n    Anthropic, Gemini, Mistral, and many other models.\n    See supported models here: [litellm models](https://docs.litellm.ai/docs/providers).\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        base_url: str | None = None,\n        api_key: str | None = None,\n    ):\n        self.model = model\n        self.base_url = base_url\n        self.api_key = api_key\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None,\n        prompt: Any | None = None,\n    ) -&gt; ModelResponse:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict()\n            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=False,\n                prompt=prompt,\n            )\n\n            assert isinstance(response.choices[0], litellm.types.utils.Choices)\n\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Received model response\")\n            else:\n                logger.debug(\n                    f\"\"\"LLM resp:\\n{\n                        json.dumps(\n                            response.choices[0].message.model_dump(), indent=2, ensure_ascii=False\n                        )\n                    }\\n\"\"\"\n                )\n\n            if hasattr(response, \"usage\"):\n                response_usage = response.usage\n                usage = (\n                    Usage(\n                        requests=1,\n                        input_tokens=response_usage.prompt_tokens,\n                        output_tokens=response_usage.completion_tokens,\n                        total_tokens=response_usage.total_tokens,\n                        input_tokens_details=InputTokensDetails(\n                            cached_tokens=getattr(\n                                response_usage.prompt_tokens_details, \"cached_tokens\", 0\n                            )\n                            or 0\n                        ),\n                        output_tokens_details=OutputTokensDetails(\n                            reasoning_tokens=getattr(\n                                response_usage.completion_tokens_details, \"reasoning_tokens\", 0\n                            )\n                            or 0\n                        ),\n                    )\n                    if response.usage\n                    else Usage()\n                )\n            else:\n                usage = Usage()\n                logger.warning(\"No usage information returned from Litellm\")\n\n            if tracing.include_data():\n                span_generation.span_data.output = [response.choices[0].message.model_dump()]\n            span_generation.span_data.usage = {\n                \"input_tokens\": usage.input_tokens,\n                \"output_tokens\": usage.output_tokens,\n            }\n\n            items = Converter.message_to_output_items(\n                LitellmConverter.convert_message_to_openai(response.choices[0].message)\n            )\n\n            return ModelResponse(\n                output=items,\n                usage=usage,\n                response_id=None,\n            )\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None,\n        prompt: Any | None = None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict()\n            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response, stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n            async for chunk in ChatCmplStreamHandler.handle_stream(response, stream):\n                yield chunk\n\n                if chunk.type == \"response.completed\":\n                    final_response = chunk.response\n\n            if tracing.include_data() and final_response:\n                span_generation.span_data.output = [final_response.model_dump()]\n\n            if final_response and final_response.usage:\n                span_generation.span_data.usage = {\n                    \"input_tokens\": final_response.usage.input_tokens,\n                    \"output_tokens\": final_response.usage.output_tokens,\n                }\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[True],\n        prompt: Any | None = None,\n    ) -&gt; tuple[Response, AsyncStream[ChatCompletionChunk]]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[False],\n        prompt: Any | None = None,\n    ) -&gt; litellm.types.utils.ModelResponse: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: bool = False,\n        prompt: Any | None = None,\n    ) -&gt; litellm.types.utils.ModelResponse | tuple[Response, AsyncStream[ChatCompletionChunk]]:\n        converted_messages = Converter.items_to_messages(input)\n\n        if system_instructions:\n            converted_messages.insert(\n                0,\n                {\n                    \"content\": system_instructions,\n                    \"role\": \"system\",\n                },\n            )\n        if tracing.include_data():\n            span.span_data.input = converted_messages\n\n        parallel_tool_calls = (\n            True\n            if model_settings.parallel_tool_calls and tools and len(tools) &gt; 0\n            else False\n            if model_settings.parallel_tool_calls is False\n            else None\n        )\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        response_format = Converter.convert_response_format(output_schema)\n\n        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []\n\n        for handoff in handoffs:\n            converted_tools.append(Converter.convert_handoff_tool(handoff))\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            logger.debug(\n                f\"Calling Litellm model: {self.model}\\n\"\n                f\"{json.dumps(converted_messages, indent=2, ensure_ascii=False)}\\n\"\n                f\"Tools:\\n{json.dumps(converted_tools, indent=2, ensure_ascii=False)}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n            )\n\n        reasoning_effort = model_settings.reasoning.effort if model_settings.reasoning else None\n\n        stream_options = None\n        if stream and model_settings.include_usage is not None:\n            stream_options = {\"include_usage\": model_settings.include_usage}\n\n        extra_kwargs = {}\n        if model_settings.extra_query:\n            extra_kwargs[\"extra_query\"] = model_settings.extra_query\n        if model_settings.metadata:\n            extra_kwargs[\"metadata\"] = model_settings.metadata\n        if model_settings.extra_body and isinstance(model_settings.extra_body, dict):\n            extra_kwargs.update(model_settings.extra_body)\n\n        # Add kwargs from model_settings.extra_args, filtering out None values\n        if model_settings.extra_args:\n            extra_kwargs.update(model_settings.extra_args)\n\n        ret = await litellm.acompletion(\n            model=self.model,\n            messages=converted_messages,\n            tools=converted_tools or None,\n            temperature=model_settings.temperature,\n            top_p=model_settings.top_p,\n            frequency_penalty=model_settings.frequency_penalty,\n            presence_penalty=model_settings.presence_penalty,\n            max_tokens=model_settings.max_tokens,\n            tool_choice=self._remove_not_given(tool_choice),\n            response_format=self._remove_not_given(response_format),\n            parallel_tool_calls=parallel_tool_calls,\n            stream=stream,\n            stream_options=stream_options,\n            reasoning_effort=reasoning_effort,\n            top_logprobs=model_settings.top_logprobs,\n            extra_headers={**HEADERS, **(model_settings.extra_headers or {})},\n            api_key=self.api_key,\n            base_url=self.base_url,\n            **extra_kwargs,\n        )\n\n        if isinstance(ret, litellm.types.utils.ModelResponse):\n            return ret\n\n        response = Response(\n            id=FAKE_RESPONSES_ID,\n            created_at=time.time(),\n            model=self.model,\n            object=\"response\",\n            output=[],\n            tool_choice=cast(Literal[\"auto\", \"required\", \"none\"], tool_choice)\n            if tool_choice != NOT_GIVEN\n            else \"auto\",\n            top_p=model_settings.top_p,\n            temperature=model_settings.temperature,\n            tools=[],\n            parallel_tool_calls=parallel_tool_calls or False,\n            reasoning=model_settings.reasoning,\n        )\n        return response, ret\n\n    def _remove_not_given(self, value: Any) -&gt; Any:\n        if isinstance(value, NotGiven):\n            return None\n        return value\n</code></pre>"},{"location":"ref/mcp/server/","title":"<code>MCP Servers</code>","text":""},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer","title":"MCPServer","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for Model Context Protocol servers.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServer(abc.ABC):\n    \"\"\"Base class for Model Context Protocol servers.\"\"\"\n\n    def __init__(self, use_structured_content: bool = False):\n        \"\"\"\n        Args:\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool.Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n        \"\"\"\n        self.use_structured_content = use_structured_content\n\n    @abc.abstractmethod\n    async def connect(self):\n        \"\"\"Connect to the server. For example, this might mean spawning a subprocess or\n        opening a network connection. The server is expected to remain connected until\n        `cleanup()` is called.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def cleanup(self):\n        \"\"\"Cleanup the server. For example, this might mean closing a subprocess or\n        closing a network connection.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def list_tools(\n        self,\n        run_context: RunContextWrapper[Any] | None = None,\n        agent: AgentBase | None = None,\n    ) -&gt; list[MCPTool]:\n        \"\"\"List the tools available on the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def call_tool(self, tool_name: str, arguments: dict[str, Any] | None) -&gt; CallToolResult:\n        \"\"\"Invoke a tool on the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def list_prompts(\n        self,\n    ) -&gt; ListPromptsResult:\n        \"\"\"List the prompts available on the server.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -&gt; GetPromptResult:\n        \"\"\"Get a specific prompt from the server.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.__init__","title":"__init__","text":"<pre><code>__init__(use_structured_content: bool = False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool.Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(self, use_structured_content: bool = False):\n    \"\"\"\n    Args:\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool.Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n    \"\"\"\n    self.use_structured_content = use_structured_content\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.connect","title":"connect  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server. For example, this might mean spawning a subprocess or opening a network connection. The server is expected to remain connected until <code>cleanup()</code> is called.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def connect(self):\n    \"\"\"Connect to the server. For example, this might mean spawning a subprocess or\n    opening a network connection. The server is expected to remain connected until\n    `cleanup()` is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.cleanup","title":"cleanup  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server. For example, this might mean closing a subprocess or closing a network connection.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def cleanup(self):\n    \"\"\"Cleanup the server. For example, this might mean closing a subprocess or\n    closing a network connection.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.list_tools","title":"list_tools  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.call_tool","title":"call_tool  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str, arguments: dict[str, Any] | None\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def call_tool(self, tool_name: str, arguments: dict[str, Any] | None) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.list_prompts","title":"list_prompts  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServer.get_prompt","title":"get_prompt  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>@abc.abstractmethod\nasync def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams","title":"MCPServerStdioParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Mirrors <code>mcp.client.stdio.StdioServerParameters</code>, but lets you pass params without another import.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStdioParams(TypedDict):\n    \"\"\"Mirrors `mcp.client.stdio.StdioServerParameters`, but lets you pass params without another\n    import.\n    \"\"\"\n\n    command: str\n    \"\"\"The executable to run to start the server. For example, `python` or `node`.\"\"\"\n\n    args: NotRequired[list[str]]\n    \"\"\"Command line args to pass to the `command` executable. For example, `['foo.py']` or\n    `['server.js', '--port', '8080']`.\"\"\"\n\n    env: NotRequired[dict[str, str]]\n    \"\"\"The environment variables to set for the server. .\"\"\"\n\n    cwd: NotRequired[str | Path]\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    encoding: NotRequired[str]\n    \"\"\"The text encoding used when sending/receiving messages to the server. Defaults to `utf-8`.\"\"\"\n\n    encoding_error_handler: NotRequired[Literal[\"strict\", \"ignore\", \"replace\"]]\n    \"\"\"The text encoding error handler. Defaults to `strict`.\n\n    See https://docs.python.org/3/library/codecs.html#codec-base-classes for\n    explanations of possible values.\n    \"\"\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.command","title":"command  <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre> <p>The executable to run to start the server. For example, <code>python</code> or <code>node</code>.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.args","title":"args  <code>instance-attribute</code>","text":"<pre><code>args: NotRequired[list[str]]\n</code></pre> <p>Command line args to pass to the <code>command</code> executable. For example, <code>['foo.py']</code> or <code>['server.js', '--port', '8080']</code>.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env: NotRequired[dict[str, str]]\n</code></pre> <p>The environment variables to set for the server. .</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.cwd","title":"cwd  <code>instance-attribute</code>","text":"<pre><code>cwd: NotRequired[str | Path]\n</code></pre> <p>The working directory to use when spawning the process.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding: NotRequired[str]\n</code></pre> <p>The text encoding used when sending/receiving messages to the server. Defaults to <code>utf-8</code>.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdioParams.encoding_error_handler","title":"encoding_error_handler  <code>instance-attribute</code>","text":"<pre><code>encoding_error_handler: NotRequired[\n    Literal[\"strict\", \"ignore\", \"replace\"]\n]\n</code></pre> <p>The text encoding error handler. Defaults to <code>strict</code>.</p> <p>See https://docs.python.org/3/library/codecs.html#codec-base-classes for explanations of possible values.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio","title":"MCPServerStdio","text":"<p>               Bases: <code>_MCPServerWithClientSession</code></p> <p>MCP server implementation that uses the stdio transport. See the [spec] (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) for details.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStdio(_MCPServerWithClientSession):\n    \"\"\"MCP server implementation that uses the stdio transport. See the [spec]\n    (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) for\n    details.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: MCPServerStdioParams,\n        cache_tools_list: bool = False,\n        name: str | None = None,\n        client_session_timeout_seconds: float | None = 5,\n        tool_filter: ToolFilter = None,\n        use_structured_content: bool = False,\n    ):\n        \"\"\"Create a new MCP server based on the stdio transport.\n\n        Args:\n            params: The params that configure the server. This includes the command to run to\n                start the server, the args to pass to the command, the environment variables to\n                set for the server, the working directory to use when spawning the process, and\n                the text encoding used when sending/receiving messages to the server.\n            cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n                cached and only fetched from the server once. If `False`, the tools list will be\n                fetched from the server on each call to `list_tools()`. The cache can be\n                invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n                if you know the server will not change its tools list, because it can drastically\n                improve latency (by avoiding a round-trip to the server every time).\n            name: A readable name for the server. If not provided, we'll create one from the\n                command.\n            client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n            tool_filter: The tool filter to use for filtering tools.\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n        \"\"\"\n        super().__init__(\n            cache_tools_list,\n            client_session_timeout_seconds,\n            tool_filter,\n            use_structured_content,\n        )\n\n        self.params = StdioServerParameters(\n            command=params[\"command\"],\n            args=params.get(\"args\", []),\n            env=params.get(\"env\"),\n            cwd=params.get(\"cwd\"),\n            encoding=params.get(\"encoding\", \"utf-8\"),\n            encoding_error_handler=params.get(\"encoding_error_handler\", \"strict\"),\n        )\n\n        self._name = name or f\"stdio: {self.params.command}\"\n\n    def create_streams(\n        self,\n    ) -&gt; AbstractAsyncContextManager[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n            GetSessionIdCallback | None,\n        ]\n    ]:\n        \"\"\"Create the streams for the server.\"\"\"\n        return stdio_client(self.params)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        return self._name\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: MCPServerStdioParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n)\n</code></pre> <p>Create a new MCP server based on the stdio transport.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MCPServerStdioParams</code> <p>The params that configure the server. This includes the command to run to start the server, the args to pass to the command, the environment variables to set for the server, the working directory to use when spawning the process, and the text encoding used when sending/receiving messages to the server.</p> required <code>cache_tools_list</code> <code>bool</code> <p>Whether to cache the tools list. If <code>True</code>, the tools list will be cached and only fetched from the server once. If <code>False</code>, the tools list will be fetched from the server on each call to <code>list_tools()</code>. The cache can be invalidated by calling <code>invalidate_tools_cache()</code>. You should set this to <code>True</code> if you know the server will not change its tools list, because it can drastically improve latency (by avoiding a round-trip to the server every time).</p> <code>False</code> <code>name</code> <code>str | None</code> <p>A readable name for the server. If not provided, we'll create one from the command.</p> <code>None</code> <code>client_session_timeout_seconds</code> <code>float | None</code> <p>the read timeout passed to the MCP ClientSession.</p> <code>5</code> <code>tool_filter</code> <code>ToolFilter</code> <p>The tool filter to use for filtering tools.</p> <code>None</code> <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool. Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    params: MCPServerStdioParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n):\n    \"\"\"Create a new MCP server based on the stdio transport.\n\n    Args:\n        params: The params that configure the server. This includes the command to run to\n            start the server, the args to pass to the command, the environment variables to\n            set for the server, the working directory to use when spawning the process, and\n            the text encoding used when sending/receiving messages to the server.\n        cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n            cached and only fetched from the server once. If `False`, the tools list will be\n            fetched from the server on each call to `list_tools()`. The cache can be\n            invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n            if you know the server will not change its tools list, because it can drastically\n            improve latency (by avoiding a round-trip to the server every time).\n        name: A readable name for the server. If not provided, we'll create one from the\n            command.\n        client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n        tool_filter: The tool filter to use for filtering tools.\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n    \"\"\"\n    super().__init__(\n        cache_tools_list,\n        client_session_timeout_seconds,\n        tool_filter,\n        use_structured_content,\n    )\n\n    self.params = StdioServerParameters(\n        command=params[\"command\"],\n        args=params.get(\"args\", []),\n        env=params.get(\"env\"),\n        cwd=params.get(\"cwd\"),\n        encoding=params.get(\"encoding\", \"utf-8\"),\n        encoding_error_handler=params.get(\"encoding_error_handler\", \"strict\"),\n    )\n\n    self._name = name or f\"stdio: {self.params.command}\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.create_streams","title":"create_streams","text":"<pre><code>create_streams() -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]\n</code></pre> <p>Create the streams for the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def create_streams(\n    self,\n) -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]:\n    \"\"\"Create the streams for the server.\"\"\"\n    return stdio_client(self.params)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def connect(self):\n    \"\"\"Connect to the server.\"\"\"\n    try:\n        transport = await self.exit_stack.enter_async_context(self.create_streams())\n        # streamablehttp_client returns (read, write, get_session_id)\n        # sse_client returns (read, write)\n\n        read, write, *_ = transport\n\n        session = await self.exit_stack.enter_async_context(\n            ClientSession(\n                read,\n                write,\n                timedelta(seconds=self.client_session_timeout_seconds)\n                if self.client_session_timeout_seconds\n                else None,\n            )\n        )\n        server_result = await session.initialize()\n        self.server_initialize_result = server_result\n        self.session = session\n    except Exception as e:\n        logger.error(f\"Error initializing MCP server: {e}\")\n        await self.cleanup()\n        raise\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the server.\"\"\"\n    async with self._cleanup_lock:\n        try:\n            await self.exit_stack.aclose()\n        except Exception as e:\n            logger.error(f\"Error cleaning up server: {e}\")\n        finally:\n            self.session = None\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.list_tools","title":"list_tools  <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    # Return from cache if caching is enabled, we have tools, and the cache is not dirty\n    if self.cache_tools_list and not self._cache_dirty and self._tools_list:\n        tools = self._tools_list\n    else:\n        # Reset the cache dirty to False\n        self._cache_dirty = False\n        # Fetch the tools from the server\n        self._tools_list = (await self.session.list_tools()).tools\n        tools = self._tools_list\n\n    # Filter tools based on tool_filter\n    filtered_tools = tools\n    if self.tool_filter is not None:\n        if run_context is None or agent is None:\n            raise UserError(\"run_context and agent are required for dynamic tool filtering\")\n        filtered_tools = await self._apply_tool_filter(filtered_tools, run_context, agent)\n    return filtered_tools\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str, arguments: dict[str, Any] | None\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def call_tool(self, tool_name: str, arguments: dict[str, Any] | None) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.call_tool(tool_name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.list_prompts","title":"list_prompts  <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.list_prompts()\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.get_prompt(name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStdio.invalidate_tools_cache","title":"invalidate_tools_cache","text":"<pre><code>invalidate_tools_cache()\n</code></pre> <p>Invalidate the tools cache.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def invalidate_tools_cache(self):\n    \"\"\"Invalidate the tools cache.\"\"\"\n    self._cache_dirty = True\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams","title":"MCPServerSseParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Mirrors the params in<code>mcp.client.sse.sse_client</code>.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerSseParams(TypedDict):\n    \"\"\"Mirrors the params in`mcp.client.sse.sse_client`.\"\"\"\n\n    url: str\n    \"\"\"The URL of the server.\"\"\"\n\n    headers: NotRequired[dict[str, str]]\n    \"\"\"The headers to send to the server.\"\"\"\n\n    timeout: NotRequired[float]\n    \"\"\"The timeout for the HTTP request. Defaults to 5 seconds.\"\"\"\n\n    sse_read_timeout: NotRequired[float]\n    \"\"\"The timeout for the SSE connection, in seconds. Defaults to 5 minutes.\"\"\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The URL of the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: NotRequired[dict[str, str]]\n</code></pre> <p>The headers to send to the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: NotRequired[float]\n</code></pre> <p>The timeout for the HTTP request. Defaults to 5 seconds.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSseParams.sse_read_timeout","title":"sse_read_timeout  <code>instance-attribute</code>","text":"<pre><code>sse_read_timeout: NotRequired[float]\n</code></pre> <p>The timeout for the SSE connection, in seconds. Defaults to 5 minutes.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse","title":"MCPServerSse","text":"<p>               Bases: <code>_MCPServerWithClientSession</code></p> <p>MCP server implementation that uses the HTTP with SSE transport. See the [spec] (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) for details.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerSse(_MCPServerWithClientSession):\n    \"\"\"MCP server implementation that uses the HTTP with SSE transport. See the [spec]\n    (https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse)\n    for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: MCPServerSseParams,\n        cache_tools_list: bool = False,\n        name: str | None = None,\n        client_session_timeout_seconds: float | None = 5,\n        tool_filter: ToolFilter = None,\n        use_structured_content: bool = False,\n    ):\n        \"\"\"Create a new MCP server based on the HTTP with SSE transport.\n\n        Args:\n            params: The params that configure the server. This includes the URL of the server,\n                the headers to send to the server, the timeout for the HTTP request, and the\n                timeout for the SSE connection.\n\n            cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n                cached and only fetched from the server once. If `False`, the tools list will be\n                fetched from the server on each call to `list_tools()`. The cache can be\n                invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n                if you know the server will not change its tools list, because it can drastically\n                improve latency (by avoiding a round-trip to the server every time).\n\n            name: A readable name for the server. If not provided, we'll create one from the\n                URL.\n\n            client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n            tool_filter: The tool filter to use for filtering tools.\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n        \"\"\"\n        super().__init__(\n            cache_tools_list,\n            client_session_timeout_seconds,\n            tool_filter,\n            use_structured_content,\n        )\n\n        self.params = params\n        self._name = name or f\"sse: {self.params['url']}\"\n\n    def create_streams(\n        self,\n    ) -&gt; AbstractAsyncContextManager[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n            GetSessionIdCallback | None,\n        ]\n    ]:\n        \"\"\"Create the streams for the server.\"\"\"\n        return sse_client(\n            url=self.params[\"url\"],\n            headers=self.params.get(\"headers\", None),\n            timeout=self.params.get(\"timeout\", 5),\n            sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        return self._name\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: MCPServerSseParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n)\n</code></pre> <p>Create a new MCP server based on the HTTP with SSE transport.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MCPServerSseParams</code> <p>The params that configure the server. This includes the URL of the server, the headers to send to the server, the timeout for the HTTP request, and the timeout for the SSE connection.</p> required <code>cache_tools_list</code> <code>bool</code> <p>Whether to cache the tools list. If <code>True</code>, the tools list will be cached and only fetched from the server once. If <code>False</code>, the tools list will be fetched from the server on each call to <code>list_tools()</code>. The cache can be invalidated by calling <code>invalidate_tools_cache()</code>. You should set this to <code>True</code> if you know the server will not change its tools list, because it can drastically improve latency (by avoiding a round-trip to the server every time).</p> <code>False</code> <code>name</code> <code>str | None</code> <p>A readable name for the server. If not provided, we'll create one from the URL.</p> <code>None</code> <code>client_session_timeout_seconds</code> <code>float | None</code> <p>the read timeout passed to the MCP ClientSession.</p> <code>5</code> <code>tool_filter</code> <code>ToolFilter</code> <p>The tool filter to use for filtering tools.</p> <code>None</code> <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool. Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    params: MCPServerSseParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n):\n    \"\"\"Create a new MCP server based on the HTTP with SSE transport.\n\n    Args:\n        params: The params that configure the server. This includes the URL of the server,\n            the headers to send to the server, the timeout for the HTTP request, and the\n            timeout for the SSE connection.\n\n        cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n            cached and only fetched from the server once. If `False`, the tools list will be\n            fetched from the server on each call to `list_tools()`. The cache can be\n            invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n            if you know the server will not change its tools list, because it can drastically\n            improve latency (by avoiding a round-trip to the server every time).\n\n        name: A readable name for the server. If not provided, we'll create one from the\n            URL.\n\n        client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n        tool_filter: The tool filter to use for filtering tools.\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n    \"\"\"\n    super().__init__(\n        cache_tools_list,\n        client_session_timeout_seconds,\n        tool_filter,\n        use_structured_content,\n    )\n\n    self.params = params\n    self._name = name or f\"sse: {self.params['url']}\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.create_streams","title":"create_streams","text":"<pre><code>create_streams() -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]\n</code></pre> <p>Create the streams for the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def create_streams(\n    self,\n) -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]:\n    \"\"\"Create the streams for the server.\"\"\"\n    return sse_client(\n        url=self.params[\"url\"],\n        headers=self.params.get(\"headers\", None),\n        timeout=self.params.get(\"timeout\", 5),\n        sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n    )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def connect(self):\n    \"\"\"Connect to the server.\"\"\"\n    try:\n        transport = await self.exit_stack.enter_async_context(self.create_streams())\n        # streamablehttp_client returns (read, write, get_session_id)\n        # sse_client returns (read, write)\n\n        read, write, *_ = transport\n\n        session = await self.exit_stack.enter_async_context(\n            ClientSession(\n                read,\n                write,\n                timedelta(seconds=self.client_session_timeout_seconds)\n                if self.client_session_timeout_seconds\n                else None,\n            )\n        )\n        server_result = await session.initialize()\n        self.server_initialize_result = server_result\n        self.session = session\n    except Exception as e:\n        logger.error(f\"Error initializing MCP server: {e}\")\n        await self.cleanup()\n        raise\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the server.\"\"\"\n    async with self._cleanup_lock:\n        try:\n            await self.exit_stack.aclose()\n        except Exception as e:\n            logger.error(f\"Error cleaning up server: {e}\")\n        finally:\n            self.session = None\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.list_tools","title":"list_tools  <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    # Return from cache if caching is enabled, we have tools, and the cache is not dirty\n    if self.cache_tools_list and not self._cache_dirty and self._tools_list:\n        tools = self._tools_list\n    else:\n        # Reset the cache dirty to False\n        self._cache_dirty = False\n        # Fetch the tools from the server\n        self._tools_list = (await self.session.list_tools()).tools\n        tools = self._tools_list\n\n    # Filter tools based on tool_filter\n    filtered_tools = tools\n    if self.tool_filter is not None:\n        if run_context is None or agent is None:\n            raise UserError(\"run_context and agent are required for dynamic tool filtering\")\n        filtered_tools = await self._apply_tool_filter(filtered_tools, run_context, agent)\n    return filtered_tools\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str, arguments: dict[str, Any] | None\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def call_tool(self, tool_name: str, arguments: dict[str, Any] | None) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.call_tool(tool_name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.list_prompts","title":"list_prompts  <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.list_prompts()\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.get_prompt(name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerSse.invalidate_tools_cache","title":"invalidate_tools_cache","text":"<pre><code>invalidate_tools_cache()\n</code></pre> <p>Invalidate the tools cache.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def invalidate_tools_cache(self):\n    \"\"\"Invalidate the tools cache.\"\"\"\n    self._cache_dirty = True\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams","title":"MCPServerStreamableHttpParams","text":"<p>               Bases: <code>TypedDict</code></p> <p>Mirrors the params in<code>mcp.client.streamable_http.streamablehttp_client</code>.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStreamableHttpParams(TypedDict):\n    \"\"\"Mirrors the params in`mcp.client.streamable_http.streamablehttp_client`.\"\"\"\n\n    url: str\n    \"\"\"The URL of the server.\"\"\"\n\n    headers: NotRequired[dict[str, str]]\n    \"\"\"The headers to send to the server.\"\"\"\n\n    timeout: NotRequired[timedelta | float]\n    \"\"\"The timeout for the HTTP request. Defaults to 5 seconds.\"\"\"\n\n    sse_read_timeout: NotRequired[timedelta | float]\n    \"\"\"The timeout for the SSE connection, in seconds. Defaults to 5 minutes.\"\"\"\n\n    terminate_on_close: NotRequired[bool]\n    \"\"\"Terminate on close\"\"\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The URL of the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers: NotRequired[dict[str, str]]\n</code></pre> <p>The headers to send to the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.timeout","title":"timeout  <code>instance-attribute</code>","text":"<pre><code>timeout: NotRequired[timedelta | float]\n</code></pre> <p>The timeout for the HTTP request. Defaults to 5 seconds.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.sse_read_timeout","title":"sse_read_timeout  <code>instance-attribute</code>","text":"<pre><code>sse_read_timeout: NotRequired[timedelta | float]\n</code></pre> <p>The timeout for the SSE connection, in seconds. Defaults to 5 minutes.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttpParams.terminate_on_close","title":"terminate_on_close  <code>instance-attribute</code>","text":"<pre><code>terminate_on_close: NotRequired[bool]\n</code></pre> <p>Terminate on close</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp","title":"MCPServerStreamableHttp","text":"<p>               Bases: <code>_MCPServerWithClientSession</code></p> <p>MCP server implementation that uses the Streamable HTTP transport. See the [spec] (https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) for details.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>class MCPServerStreamableHttp(_MCPServerWithClientSession):\n    \"\"\"MCP server implementation that uses the Streamable HTTP transport. See the [spec]\n    (https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http)\n    for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: MCPServerStreamableHttpParams,\n        cache_tools_list: bool = False,\n        name: str | None = None,\n        client_session_timeout_seconds: float | None = 5,\n        tool_filter: ToolFilter = None,\n        use_structured_content: bool = False,\n    ):\n        \"\"\"Create a new MCP server based on the Streamable HTTP transport.\n\n        Args:\n            params: The params that configure the server. This includes the URL of the server,\n                the headers to send to the server, the timeout for the HTTP request, and the\n                timeout for the Streamable HTTP connection and whether we need to\n                terminate on close.\n\n            cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n                cached and only fetched from the server once. If `False`, the tools list will be\n                fetched from the server on each call to `list_tools()`. The cache can be\n                invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n                if you know the server will not change its tools list, because it can drastically\n                improve latency (by avoiding a round-trip to the server every time).\n\n            name: A readable name for the server. If not provided, we'll create one from the\n                URL.\n\n            client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n            tool_filter: The tool filter to use for filtering tools.\n            use_structured_content: Whether to use `tool_result.structured_content` when calling an\n                MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n                include the structured content in the `tool_result.content`, and using it by\n                default will cause duplicate content. You can set this to True if you know the\n                server will not duplicate the structured content in the `tool_result.content`.\n        \"\"\"\n        super().__init__(\n            cache_tools_list,\n            client_session_timeout_seconds,\n            tool_filter,\n            use_structured_content,\n        )\n\n        self.params = params\n        self._name = name or f\"streamable_http: {self.params['url']}\"\n\n    def create_streams(\n        self,\n    ) -&gt; AbstractAsyncContextManager[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n            GetSessionIdCallback | None,\n        ]\n    ]:\n        \"\"\"Create the streams for the server.\"\"\"\n        return streamablehttp_client(\n            url=self.params[\"url\"],\n            headers=self.params.get(\"headers\", None),\n            timeout=self.params.get(\"timeout\", 5),\n            sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n            terminate_on_close=self.params.get(\"terminate_on_close\", True),\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"A readable name for the server.\"\"\"\n        return self._name\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>A readable name for the server.</p>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.__init__","title":"__init__","text":"<pre><code>__init__(\n    params: MCPServerStreamableHttpParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n)\n</code></pre> <p>Create a new MCP server based on the Streamable HTTP transport.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>MCPServerStreamableHttpParams</code> <p>The params that configure the server. This includes the URL of the server, the headers to send to the server, the timeout for the HTTP request, and the timeout for the Streamable HTTP connection and whether we need to terminate on close.</p> required <code>cache_tools_list</code> <code>bool</code> <p>Whether to cache the tools list. If <code>True</code>, the tools list will be cached and only fetched from the server once. If <code>False</code>, the tools list will be fetched from the server on each call to <code>list_tools()</code>. The cache can be invalidated by calling <code>invalidate_tools_cache()</code>. You should set this to <code>True</code> if you know the server will not change its tools list, because it can drastically improve latency (by avoiding a round-trip to the server every time).</p> <code>False</code> <code>name</code> <code>str | None</code> <p>A readable name for the server. If not provided, we'll create one from the URL.</p> <code>None</code> <code>client_session_timeout_seconds</code> <code>float | None</code> <p>the read timeout passed to the MCP ClientSession.</p> <code>5</code> <code>tool_filter</code> <code>ToolFilter</code> <p>The tool filter to use for filtering tools.</p> <code>None</code> <code>use_structured_content</code> <code>bool</code> <p>Whether to use <code>tool_result.structured_content</code> when calling an MCP tool. Defaults to False for backwards compatibility - most MCP servers still include the structured content in the <code>tool_result.content</code>, and using it by default will cause duplicate content. You can set this to True if you know the server will not duplicate the structured content in the <code>tool_result.content</code>.</p> <code>False</code> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def __init__(\n    self,\n    params: MCPServerStreamableHttpParams,\n    cache_tools_list: bool = False,\n    name: str | None = None,\n    client_session_timeout_seconds: float | None = 5,\n    tool_filter: ToolFilter = None,\n    use_structured_content: bool = False,\n):\n    \"\"\"Create a new MCP server based on the Streamable HTTP transport.\n\n    Args:\n        params: The params that configure the server. This includes the URL of the server,\n            the headers to send to the server, the timeout for the HTTP request, and the\n            timeout for the Streamable HTTP connection and whether we need to\n            terminate on close.\n\n        cache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\n            cached and only fetched from the server once. If `False`, the tools list will be\n            fetched from the server on each call to `list_tools()`. The cache can be\n            invalidated by calling `invalidate_tools_cache()`. You should set this to `True`\n            if you know the server will not change its tools list, because it can drastically\n            improve latency (by avoiding a round-trip to the server every time).\n\n        name: A readable name for the server. If not provided, we'll create one from the\n            URL.\n\n        client_session_timeout_seconds: the read timeout passed to the MCP ClientSession.\n        tool_filter: The tool filter to use for filtering tools.\n        use_structured_content: Whether to use `tool_result.structured_content` when calling an\n            MCP tool. Defaults to False for backwards compatibility - most MCP servers still\n            include the structured content in the `tool_result.content`, and using it by\n            default will cause duplicate content. You can set this to True if you know the\n            server will not duplicate the structured content in the `tool_result.content`.\n    \"\"\"\n    super().__init__(\n        cache_tools_list,\n        client_session_timeout_seconds,\n        tool_filter,\n        use_structured_content,\n    )\n\n    self.params = params\n    self._name = name or f\"streamable_http: {self.params['url']}\"\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.create_streams","title":"create_streams","text":"<pre><code>create_streams() -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]\n</code></pre> <p>Create the streams for the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def create_streams(\n    self,\n) -&gt; AbstractAsyncContextManager[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n        GetSessionIdCallback | None,\n    ]\n]:\n    \"\"\"Create the streams for the server.\"\"\"\n    return streamablehttp_client(\n        url=self.params[\"url\"],\n        headers=self.params.get(\"headers\", None),\n        timeout=self.params.get(\"timeout\", 5),\n        sse_read_timeout=self.params.get(\"sse_read_timeout\", 60 * 5),\n        terminate_on_close=self.params.get(\"terminate_on_close\", True),\n    )\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def connect(self):\n    \"\"\"Connect to the server.\"\"\"\n    try:\n        transport = await self.exit_stack.enter_async_context(self.create_streams())\n        # streamablehttp_client returns (read, write, get_session_id)\n        # sse_client returns (read, write)\n\n        read, write, *_ = transport\n\n        session = await self.exit_stack.enter_async_context(\n            ClientSession(\n                read,\n                write,\n                timedelta(seconds=self.client_session_timeout_seconds)\n                if self.client_session_timeout_seconds\n                else None,\n            )\n        )\n        server_result = await session.initialize()\n        self.server_initialize_result = server_result\n        self.session = session\n    except Exception as e:\n        logger.error(f\"Error initializing MCP server: {e}\")\n        await self.cleanup()\n        raise\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Cleanup the server.\"\"\"\n    async with self._cleanup_lock:\n        try:\n            await self.exit_stack.aclose()\n        except Exception as e:\n            logger.error(f\"Error cleaning up server: {e}\")\n        finally:\n            self.session = None\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.list_tools","title":"list_tools  <code>async</code>","text":"<pre><code>list_tools(\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[Tool]\n</code></pre> <p>List the tools available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_tools(\n    self,\n    run_context: RunContextWrapper[Any] | None = None,\n    agent: AgentBase | None = None,\n) -&gt; list[MCPTool]:\n    \"\"\"List the tools available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    # Return from cache if caching is enabled, we have tools, and the cache is not dirty\n    if self.cache_tools_list and not self._cache_dirty and self._tools_list:\n        tools = self._tools_list\n    else:\n        # Reset the cache dirty to False\n        self._cache_dirty = False\n        # Fetch the tools from the server\n        self._tools_list = (await self.session.list_tools()).tools\n        tools = self._tools_list\n\n    # Filter tools based on tool_filter\n    filtered_tools = tools\n    if self.tool_filter is not None:\n        if run_context is None or agent is None:\n            raise UserError(\"run_context and agent are required for dynamic tool filtering\")\n        filtered_tools = await self._apply_tool_filter(filtered_tools, run_context, agent)\n    return filtered_tools\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.call_tool","title":"call_tool  <code>async</code>","text":"<pre><code>call_tool(\n    tool_name: str, arguments: dict[str, Any] | None\n) -&gt; CallToolResult\n</code></pre> <p>Invoke a tool on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def call_tool(self, tool_name: str, arguments: dict[str, Any] | None) -&gt; CallToolResult:\n    \"\"\"Invoke a tool on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.call_tool(tool_name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.list_prompts","title":"list_prompts  <code>async</code>","text":"<pre><code>list_prompts() -&gt; ListPromptsResult\n</code></pre> <p>List the prompts available on the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def list_prompts(\n    self,\n) -&gt; ListPromptsResult:\n    \"\"\"List the prompts available on the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.list_prompts()\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.get_prompt","title":"get_prompt  <code>async</code>","text":"<pre><code>get_prompt(\n    name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult\n</code></pre> <p>Get a specific prompt from the server.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>async def get_prompt(\n    self, name: str, arguments: dict[str, Any] | None = None\n) -&gt; GetPromptResult:\n    \"\"\"Get a specific prompt from the server.\"\"\"\n    if not self.session:\n        raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n\n    return await self.session.get_prompt(name, arguments)\n</code></pre>"},{"location":"ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp.invalidate_tools_cache","title":"invalidate_tools_cache","text":"<pre><code>invalidate_tools_cache()\n</code></pre> <p>Invalidate the tools cache.</p> Source code in <code>src/agents/mcp/server.py</code> <pre><code>def invalidate_tools_cache(self):\n    \"\"\"Invalidate the tools cache.\"\"\"\n    self._cache_dirty = True\n</code></pre>"},{"location":"ref/mcp/util/","title":"<code>MCP Util</code>","text":""},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterCallable","title":"ToolFilterCallable  <code>module-attribute</code>","text":"<pre><code>ToolFilterCallable = Callable[\n    [\"ToolFilterContext\", \"MCPTool\"], MaybeAwaitable[bool]\n]\n</code></pre> <p>A function that determines whether a tool should be available.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>The context information including run context, agent, and server name.</p> required <code>tool</code> <p>The MCP tool to filter.</p> required <p>Returns:</p> Type Description <p>Whether the tool should be available (True) or filtered out (False).</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilter","title":"ToolFilter  <code>module-attribute</code>","text":"<pre><code>ToolFilter = Union[\n    ToolFilterCallable, ToolFilterStatic, None\n]\n</code></pre> <p>A tool filter that can be either a function, static configuration, or None (no filtering).</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext","title":"ToolFilterContext  <code>dataclass</code>","text":"<p>Context information available to tool filter functions.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@dataclass\nclass ToolFilterContext:\n    \"\"\"Context information available to tool filter functions.\"\"\"\n\n    run_context: RunContextWrapper[Any]\n    \"\"\"The current run context.\"\"\"\n\n    agent: \"AgentBase\"\n    \"\"\"The agent that is requesting the tool list.\"\"\"\n\n    server_name: str\n    \"\"\"The name of the MCP server.\"\"\"\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext.run_context","title":"run_context  <code>instance-attribute</code>","text":"<pre><code>run_context: RunContextWrapper[Any]\n</code></pre> <p>The current run context.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: AgentBase\n</code></pre> <p>The agent that is requesting the tool list.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterContext.server_name","title":"server_name  <code>instance-attribute</code>","text":"<pre><code>server_name: str\n</code></pre> <p>The name of the MCP server.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterStatic","title":"ToolFilterStatic","text":"<p>               Bases: <code>TypedDict</code></p> <p>Static tool filter configuration using allowlists and blocklists.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>class ToolFilterStatic(TypedDict):\n    \"\"\"Static tool filter configuration using allowlists and blocklists.\"\"\"\n\n    allowed_tool_names: NotRequired[list[str]]\n    \"\"\"Optional list of tool names to allow (whitelist).\n    If set, only these tools will be available.\"\"\"\n\n    blocked_tool_names: NotRequired[list[str]]\n    \"\"\"Optional list of tool names to exclude (blacklist).\n    If set, these tools will be filtered out.\"\"\"\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterStatic.allowed_tool_names","title":"allowed_tool_names  <code>instance-attribute</code>","text":"<pre><code>allowed_tool_names: NotRequired[list[str]]\n</code></pre> <p>Optional list of tool names to allow (whitelist). If set, only these tools will be available.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.ToolFilterStatic.blocked_tool_names","title":"blocked_tool_names  <code>instance-attribute</code>","text":"<pre><code>blocked_tool_names: NotRequired[list[str]]\n</code></pre> <p>Optional list of tool names to exclude (blacklist). If set, these tools will be filtered out.</p>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil","title":"MCPUtil","text":"<p>Set of utilities for interop between MCP and Agents SDK tools.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>class MCPUtil:\n    \"\"\"Set of utilities for interop between MCP and Agents SDK tools.\"\"\"\n\n    @classmethod\n    async def get_all_function_tools(\n        cls,\n        servers: list[\"MCPServer\"],\n        convert_schemas_to_strict: bool,\n        run_context: RunContextWrapper[Any],\n        agent: \"AgentBase\",\n    ) -&gt; list[Tool]:\n        \"\"\"Get all function tools from a list of MCP servers.\"\"\"\n        tools = []\n        tool_names: set[str] = set()\n        for server in servers:\n            server_tools = await cls.get_function_tools(\n                server, convert_schemas_to_strict, run_context, agent\n            )\n            server_tool_names = {tool.name for tool in server_tools}\n            if len(server_tool_names &amp; tool_names) &gt; 0:\n                raise UserError(\n                    f\"Duplicate tool names found across MCP servers: \"\n                    f\"{server_tool_names &amp; tool_names}\"\n                )\n            tool_names.update(server_tool_names)\n            tools.extend(server_tools)\n\n        return tools\n\n    @classmethod\n    async def get_function_tools(\n        cls,\n        server: \"MCPServer\",\n        convert_schemas_to_strict: bool,\n        run_context: RunContextWrapper[Any],\n        agent: \"AgentBase\",\n    ) -&gt; list[Tool]:\n        \"\"\"Get all function tools from a single MCP server.\"\"\"\n\n        with mcp_tools_span(server=server.name) as span:\n            tools = await server.list_tools(run_context, agent)\n            span.span_data.result = [tool.name for tool in tools]\n\n        return [cls.to_function_tool(tool, server, convert_schemas_to_strict) for tool in tools]\n\n    @classmethod\n    def to_function_tool(\n        cls, tool: \"MCPTool\", server: \"MCPServer\", convert_schemas_to_strict: bool\n    ) -&gt; FunctionTool:\n        \"\"\"Convert an MCP tool to an Agents SDK function tool.\"\"\"\n        invoke_func = functools.partial(cls.invoke_mcp_tool, server, tool)\n        schema, is_strict = tool.inputSchema, False\n\n        # MCP spec doesn't require the inputSchema to have `properties`, but OpenAI spec does.\n        if \"properties\" not in schema:\n            schema[\"properties\"] = {}\n\n        if convert_schemas_to_strict:\n            try:\n                schema = ensure_strict_json_schema(schema)\n                is_strict = True\n            except Exception as e:\n                logger.info(f\"Error converting MCP schema to strict mode: {e}\")\n\n        return FunctionTool(\n            name=tool.name,\n            description=tool.description or \"\",\n            params_json_schema=schema,\n            on_invoke_tool=invoke_func,\n            strict_json_schema=is_strict,\n        )\n\n    @classmethod\n    async def invoke_mcp_tool(\n        cls, server: \"MCPServer\", tool: \"MCPTool\", context: RunContextWrapper[Any], input_json: str\n    ) -&gt; str:\n        \"\"\"Invoke an MCP tool and return the result as a string.\"\"\"\n        try:\n            json_data: dict[str, Any] = json.loads(input_json) if input_json else {}\n        except Exception as e:\n            if _debug.DONT_LOG_TOOL_DATA:\n                logger.debug(f\"Invalid JSON input for tool {tool.name}\")\n            else:\n                logger.debug(f\"Invalid JSON input for tool {tool.name}: {input_json}\")\n            raise ModelBehaviorError(\n                f\"Invalid JSON input for tool {tool.name}: {input_json}\"\n            ) from e\n\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"Invoking MCP tool {tool.name}\")\n        else:\n            logger.debug(f\"Invoking MCP tool {tool.name} with input {input_json}\")\n\n        try:\n            result = await server.call_tool(tool.name, json_data)\n        except Exception as e:\n            logger.error(f\"Error invoking MCP tool {tool.name}: {e}\")\n            raise AgentsException(f\"Error invoking MCP tool {tool.name}: {e}\") from e\n\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"MCP tool {tool.name} completed.\")\n        else:\n            logger.debug(f\"MCP tool {tool.name} returned {result}\")\n\n        # The MCP tool result is a list of content items, whereas OpenAI tool outputs are a single\n        # string. We'll try to convert.\n        if len(result.content) == 1:\n            tool_output = result.content[0].model_dump_json()\n            # Append structured content if it exists and we're using it.\n            if server.use_structured_content and result.structuredContent:\n                tool_output = f\"{tool_output}\\n{json.dumps(result.structuredContent)}\"\n        elif len(result.content) &gt; 1:\n            tool_results = [item.model_dump(mode=\"json\") for item in result.content]\n            if server.use_structured_content and result.structuredContent:\n                tool_results.append(result.structuredContent)\n            tool_output = json.dumps(tool_results)\n        elif server.use_structured_content and result.structuredContent:\n            tool_output = json.dumps(result.structuredContent)\n        else:\n            # Empty content is a valid result (e.g., \"no results found\")\n            tool_output = \"[]\"\n\n        current_span = get_current_span()\n        if current_span:\n            if isinstance(current_span.span_data, FunctionSpanData):\n                current_span.span_data.output = tool_output\n                current_span.span_data.mcp_data = {\n                    \"server\": server.name,\n                }\n            else:\n                logger.warning(\n                    f\"Current span is not a FunctionSpanData, skipping tool output: {current_span}\"\n                )\n\n        return tool_output\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.get_all_function_tools","title":"get_all_function_tools  <code>async</code> <code>classmethod</code>","text":"<pre><code>get_all_function_tools(\n    servers: list[MCPServer],\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: AgentBase,\n) -&gt; list[Tool]\n</code></pre> <p>Get all function tools from a list of MCP servers.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\nasync def get_all_function_tools(\n    cls,\n    servers: list[\"MCPServer\"],\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: \"AgentBase\",\n) -&gt; list[Tool]:\n    \"\"\"Get all function tools from a list of MCP servers.\"\"\"\n    tools = []\n    tool_names: set[str] = set()\n    for server in servers:\n        server_tools = await cls.get_function_tools(\n            server, convert_schemas_to_strict, run_context, agent\n        )\n        server_tool_names = {tool.name for tool in server_tools}\n        if len(server_tool_names &amp; tool_names) &gt; 0:\n            raise UserError(\n                f\"Duplicate tool names found across MCP servers: \"\n                f\"{server_tool_names &amp; tool_names}\"\n            )\n        tool_names.update(server_tool_names)\n        tools.extend(server_tools)\n\n    return tools\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.get_function_tools","title":"get_function_tools  <code>async</code> <code>classmethod</code>","text":"<pre><code>get_function_tools(\n    server: MCPServer,\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: AgentBase,\n) -&gt; list[Tool]\n</code></pre> <p>Get all function tools from a single MCP server.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\nasync def get_function_tools(\n    cls,\n    server: \"MCPServer\",\n    convert_schemas_to_strict: bool,\n    run_context: RunContextWrapper[Any],\n    agent: \"AgentBase\",\n) -&gt; list[Tool]:\n    \"\"\"Get all function tools from a single MCP server.\"\"\"\n\n    with mcp_tools_span(server=server.name) as span:\n        tools = await server.list_tools(run_context, agent)\n        span.span_data.result = [tool.name for tool in tools]\n\n    return [cls.to_function_tool(tool, server, convert_schemas_to_strict) for tool in tools]\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.to_function_tool","title":"to_function_tool  <code>classmethod</code>","text":"<pre><code>to_function_tool(\n    tool: Tool,\n    server: MCPServer,\n    convert_schemas_to_strict: bool,\n) -&gt; FunctionTool\n</code></pre> <p>Convert an MCP tool to an Agents SDK function tool.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\ndef to_function_tool(\n    cls, tool: \"MCPTool\", server: \"MCPServer\", convert_schemas_to_strict: bool\n) -&gt; FunctionTool:\n    \"\"\"Convert an MCP tool to an Agents SDK function tool.\"\"\"\n    invoke_func = functools.partial(cls.invoke_mcp_tool, server, tool)\n    schema, is_strict = tool.inputSchema, False\n\n    # MCP spec doesn't require the inputSchema to have `properties`, but OpenAI spec does.\n    if \"properties\" not in schema:\n        schema[\"properties\"] = {}\n\n    if convert_schemas_to_strict:\n        try:\n            schema = ensure_strict_json_schema(schema)\n            is_strict = True\n        except Exception as e:\n            logger.info(f\"Error converting MCP schema to strict mode: {e}\")\n\n    return FunctionTool(\n        name=tool.name,\n        description=tool.description or \"\",\n        params_json_schema=schema,\n        on_invoke_tool=invoke_func,\n        strict_json_schema=is_strict,\n    )\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.MCPUtil.invoke_mcp_tool","title":"invoke_mcp_tool  <code>async</code> <code>classmethod</code>","text":"<pre><code>invoke_mcp_tool(\n    server: MCPServer,\n    tool: Tool,\n    context: RunContextWrapper[Any],\n    input_json: str,\n) -&gt; str\n</code></pre> <p>Invoke an MCP tool and return the result as a string.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>@classmethod\nasync def invoke_mcp_tool(\n    cls, server: \"MCPServer\", tool: \"MCPTool\", context: RunContextWrapper[Any], input_json: str\n) -&gt; str:\n    \"\"\"Invoke an MCP tool and return the result as a string.\"\"\"\n    try:\n        json_data: dict[str, Any] = json.loads(input_json) if input_json else {}\n    except Exception as e:\n        if _debug.DONT_LOG_TOOL_DATA:\n            logger.debug(f\"Invalid JSON input for tool {tool.name}\")\n        else:\n            logger.debug(f\"Invalid JSON input for tool {tool.name}: {input_json}\")\n        raise ModelBehaviorError(\n            f\"Invalid JSON input for tool {tool.name}: {input_json}\"\n        ) from e\n\n    if _debug.DONT_LOG_TOOL_DATA:\n        logger.debug(f\"Invoking MCP tool {tool.name}\")\n    else:\n        logger.debug(f\"Invoking MCP tool {tool.name} with input {input_json}\")\n\n    try:\n        result = await server.call_tool(tool.name, json_data)\n    except Exception as e:\n        logger.error(f\"Error invoking MCP tool {tool.name}: {e}\")\n        raise AgentsException(f\"Error invoking MCP tool {tool.name}: {e}\") from e\n\n    if _debug.DONT_LOG_TOOL_DATA:\n        logger.debug(f\"MCP tool {tool.name} completed.\")\n    else:\n        logger.debug(f\"MCP tool {tool.name} returned {result}\")\n\n    # The MCP tool result is a list of content items, whereas OpenAI tool outputs are a single\n    # string. We'll try to convert.\n    if len(result.content) == 1:\n        tool_output = result.content[0].model_dump_json()\n        # Append structured content if it exists and we're using it.\n        if server.use_structured_content and result.structuredContent:\n            tool_output = f\"{tool_output}\\n{json.dumps(result.structuredContent)}\"\n    elif len(result.content) &gt; 1:\n        tool_results = [item.model_dump(mode=\"json\") for item in result.content]\n        if server.use_structured_content and result.structuredContent:\n            tool_results.append(result.structuredContent)\n        tool_output = json.dumps(tool_results)\n    elif server.use_structured_content and result.structuredContent:\n        tool_output = json.dumps(result.structuredContent)\n    else:\n        # Empty content is a valid result (e.g., \"no results found\")\n        tool_output = \"[]\"\n\n    current_span = get_current_span()\n    if current_span:\n        if isinstance(current_span.span_data, FunctionSpanData):\n            current_span.span_data.output = tool_output\n            current_span.span_data.mcp_data = {\n                \"server\": server.name,\n            }\n        else:\n            logger.warning(\n                f\"Current span is not a FunctionSpanData, skipping tool output: {current_span}\"\n            )\n\n    return tool_output\n</code></pre>"},{"location":"ref/mcp/util/#agents.mcp.util.create_static_tool_filter","title":"create_static_tool_filter","text":"<pre><code>create_static_tool_filter(\n    allowed_tool_names: Optional[list[str]] = None,\n    blocked_tool_names: Optional[list[str]] = None,\n) -&gt; Optional[ToolFilterStatic]\n</code></pre> <p>Create a static tool filter from allowlist and blocklist parameters.</p> <p>This is a convenience function for creating a ToolFilterStatic.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_tool_names</code> <code>Optional[list[str]]</code> <p>Optional list of tool names to allow (whitelist).</p> <code>None</code> <code>blocked_tool_names</code> <code>Optional[list[str]]</code> <p>Optional list of tool names to exclude (blacklist).</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ToolFilterStatic]</code> <p>A ToolFilterStatic if any filtering is specified, None otherwise.</p> Source code in <code>src/agents/mcp/util.py</code> <pre><code>def create_static_tool_filter(\n    allowed_tool_names: Optional[list[str]] = None,\n    blocked_tool_names: Optional[list[str]] = None,\n) -&gt; Optional[ToolFilterStatic]:\n    \"\"\"Create a static tool filter from allowlist and blocklist parameters.\n\n    This is a convenience function for creating a ToolFilterStatic.\n\n    Args:\n        allowed_tool_names: Optional list of tool names to allow (whitelist).\n        blocked_tool_names: Optional list of tool names to exclude (blacklist).\n\n    Returns:\n        A ToolFilterStatic if any filtering is specified, None otherwise.\n    \"\"\"\n    if allowed_tool_names is None and blocked_tool_names is None:\n        return None\n\n    filter_dict: ToolFilterStatic = {}\n    if allowed_tool_names is not None:\n        filter_dict[\"allowed_tool_names\"] = allowed_tool_names\n    if blocked_tool_names is not None:\n        filter_dict[\"blocked_tool_names\"] = blocked_tool_names\n\n    return filter_dict\n</code></pre>"},{"location":"ref/models/interface/","title":"<code>Model interface</code>","text":""},{"location":"ref/models/interface/#agents.models.interface.ModelTracing","title":"ModelTracing","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>src/agents/models/interface.py</code> <pre><code>class ModelTracing(enum.Enum):\n    DISABLED = 0\n    \"\"\"Tracing is disabled entirely.\"\"\"\n\n    ENABLED = 1\n    \"\"\"Tracing is enabled, and all data is included.\"\"\"\n\n    ENABLED_WITHOUT_DATA = 2\n    \"\"\"Tracing is enabled, but inputs/outputs are not included.\"\"\"\n\n    def is_disabled(self) -&gt; bool:\n        return self == ModelTracing.DISABLED\n\n    def include_data(self) -&gt; bool:\n        return self == ModelTracing.ENABLED\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.ModelTracing.DISABLED","title":"DISABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED = 0\n</code></pre> <p>Tracing is disabled entirely.</p>"},{"location":"ref/models/interface/#agents.models.interface.ModelTracing.ENABLED","title":"ENABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENABLED = 1\n</code></pre> <p>Tracing is enabled, and all data is included.</p>"},{"location":"ref/models/interface/#agents.models.interface.ModelTracing.ENABLED_WITHOUT_DATA","title":"ENABLED_WITHOUT_DATA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENABLED_WITHOUT_DATA = 2\n</code></pre> <p>Tracing is enabled, but inputs/outputs are not included.</p>"},{"location":"ref/models/interface/#agents.models.interface.Model","title":"Model","text":"<p>               Bases: <code>ABC</code></p> <p>The base interface for calling an LLM.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>class Model(abc.ABC):\n    \"\"\"The base interface for calling an LLM.\"\"\"\n\n    @abc.abstractmethod\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        *,\n        previous_response_id: str | None,\n        prompt: ResponsePromptParam | None,\n    ) -&gt; ModelResponse:\n        \"\"\"Get a response from the model.\n\n        Args:\n            system_instructions: The system instructions to use.\n            input: The input items to the model, in OpenAI Responses format.\n            model_settings: The model settings to use.\n            tools: The tools available to the model.\n            output_schema: The output schema to use.\n            handoffs: The handoffs available to the model.\n            tracing: Tracing configuration.\n            previous_response_id: the ID of the previous response. Generally not used by the model,\n                except for the OpenAI Responses API.\n            prompt: The prompt config to use for the model.\n\n        Returns:\n            The full model response.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        *,\n        previous_response_id: str | None,\n        prompt: ResponsePromptParam | None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        \"\"\"Stream a response from the model.\n\n        Args:\n            system_instructions: The system instructions to use.\n            input: The input items to the model, in OpenAI Responses format.\n            model_settings: The model settings to use.\n            tools: The tools available to the model.\n            output_schema: The output schema to use.\n            handoffs: The handoffs available to the model.\n            tracing: Tracing configuration.\n            previous_response_id: the ID of the previous response. Generally not used by the model,\n                except for the OpenAI Responses API.\n            prompt: The prompt config to use for the model.\n\n        Returns:\n            An iterator of response stream events, in OpenAI Responses format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.Model.get_response","title":"get_response  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; ModelResponse\n</code></pre> <p>Get a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>system_instructions</code> <code>str | None</code> <p>The system instructions to use.</p> required <code>input</code> <code>str | list[TResponseInputItem]</code> <p>The input items to the model, in OpenAI Responses format.</p> required <code>model_settings</code> <code>ModelSettings</code> <p>The model settings to use.</p> required <code>tools</code> <code>list[Tool]</code> <p>The tools available to the model.</p> required <code>output_schema</code> <code>AgentOutputSchemaBase | None</code> <p>The output schema to use.</p> required <code>handoffs</code> <code>list[Handoff]</code> <p>The handoffs available to the model.</p> required <code>tracing</code> <code>ModelTracing</code> <p>Tracing configuration.</p> required <code>previous_response_id</code> <code>str | None</code> <p>the ID of the previous response. Generally not used by the model, except for the OpenAI Responses API.</p> required <code>prompt</code> <code>ResponsePromptParam | None</code> <p>The prompt config to use for the model.</p> required <p>Returns:</p> Type Description <code>ModelResponse</code> <p>The full model response.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>@abc.abstractmethod\nasync def get_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; ModelResponse:\n    \"\"\"Get a response from the model.\n\n    Args:\n        system_instructions: The system instructions to use.\n        input: The input items to the model, in OpenAI Responses format.\n        model_settings: The model settings to use.\n        tools: The tools available to the model.\n        output_schema: The output schema to use.\n        handoffs: The handoffs available to the model.\n        tracing: Tracing configuration.\n        previous_response_id: the ID of the previous response. Generally not used by the model,\n            except for the OpenAI Responses API.\n        prompt: The prompt config to use for the model.\n\n    Returns:\n        The full model response.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.Model.stream_response","title":"stream_response  <code>abstractmethod</code>","text":"<pre><code>stream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; AsyncIterator[TResponseStreamEvent]\n</code></pre> <p>Stream a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>system_instructions</code> <code>str | None</code> <p>The system instructions to use.</p> required <code>input</code> <code>str | list[TResponseInputItem]</code> <p>The input items to the model, in OpenAI Responses format.</p> required <code>model_settings</code> <code>ModelSettings</code> <p>The model settings to use.</p> required <code>tools</code> <code>list[Tool]</code> <p>The tools available to the model.</p> required <code>output_schema</code> <code>AgentOutputSchemaBase | None</code> <p>The output schema to use.</p> required <code>handoffs</code> <code>list[Handoff]</code> <p>The handoffs available to the model.</p> required <code>tracing</code> <code>ModelTracing</code> <p>Tracing configuration.</p> required <code>previous_response_id</code> <code>str | None</code> <p>the ID of the previous response. Generally not used by the model, except for the OpenAI Responses API.</p> required <code>prompt</code> <code>ResponsePromptParam | None</code> <p>The prompt config to use for the model.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[TResponseStreamEvent]</code> <p>An iterator of response stream events, in OpenAI Responses format.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>@abc.abstractmethod\ndef stream_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -&gt; AsyncIterator[TResponseStreamEvent]:\n    \"\"\"Stream a response from the model.\n\n    Args:\n        system_instructions: The system instructions to use.\n        input: The input items to the model, in OpenAI Responses format.\n        model_settings: The model settings to use.\n        tools: The tools available to the model.\n        output_schema: The output schema to use.\n        handoffs: The handoffs available to the model.\n        tracing: Tracing configuration.\n        previous_response_id: the ID of the previous response. Generally not used by the model,\n            except for the OpenAI Responses API.\n        prompt: The prompt config to use for the model.\n\n    Returns:\n        An iterator of response stream events, in OpenAI Responses format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.ModelProvider","title":"ModelProvider","text":"<p>               Bases: <code>ABC</code></p> <p>The base interface for a model provider.</p> <p>Model provider is responsible for looking up Models by name.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>class ModelProvider(abc.ABC):\n    \"\"\"The base interface for a model provider.\n\n    Model provider is responsible for looking up Models by name.\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_model(self, model_name: str | None) -&gt; Model:\n        \"\"\"Get a model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The model.\n        \"\"\"\n</code></pre>"},{"location":"ref/models/interface/#agents.models.interface.ModelProvider.get_model","title":"get_model  <code>abstractmethod</code>","text":"<pre><code>get_model(model_name: str | None) -&gt; Model\n</code></pre> <p>Get a model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The model.</p> Source code in <code>src/agents/models/interface.py</code> <pre><code>@abc.abstractmethod\ndef get_model(self, model_name: str | None) -&gt; Model:\n    \"\"\"Get a model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The model.\n    \"\"\"\n</code></pre>"},{"location":"ref/models/openai_chatcompletions/","title":"<code>OpenAI Chat Completions model</code>","text":""},{"location":"ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel","title":"OpenAIChatCompletionsModel","text":"<p>               Bases: <code>Model</code></p> Source code in <code>src/agents/models/openai_chatcompletions.py</code> <pre><code>class OpenAIChatCompletionsModel(Model):\n    def __init__(\n        self,\n        model: str | ChatModel,\n        openai_client: AsyncOpenAI,\n    ) -&gt; None:\n        self.model = model\n        self._client = openai_client\n\n    def _non_null_or_not_given(self, value: Any) -&gt; Any:\n        return value if value is not None else NOT_GIVEN\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ModelResponse:\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=False,\n                prompt=prompt,\n            )\n\n            message: ChatCompletionMessage | None = None\n            first_choice: Choice | None = None\n            if response.choices and len(response.choices) &gt; 0:\n                first_choice = response.choices[0]\n                message = first_choice.message\n\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Received model response\")\n            else:\n                if message is not None:\n                    logger.debug(\n                        \"LLM resp:\\n%s\\n\",\n                        json.dumps(message.model_dump(), indent=2, ensure_ascii=False),\n                    )\n                else:\n                    finish_reason = first_choice.finish_reason if first_choice else \"-\"\n                    logger.debug(f\"LLM resp had no message. finish_reason: {finish_reason}\")\n\n            usage = (\n                Usage(\n                    requests=1,\n                    input_tokens=response.usage.prompt_tokens,\n                    output_tokens=response.usage.completion_tokens,\n                    total_tokens=response.usage.total_tokens,\n                    input_tokens_details=InputTokensDetails(\n                        cached_tokens=getattr(\n                            response.usage.prompt_tokens_details, \"cached_tokens\", 0\n                        )\n                        or 0,\n                    ),\n                    output_tokens_details=OutputTokensDetails(\n                        reasoning_tokens=getattr(\n                            response.usage.completion_tokens_details, \"reasoning_tokens\", 0\n                        )\n                        or 0,\n                    ),\n                )\n                if response.usage\n                else Usage()\n            )\n            if tracing.include_data():\n                span_generation.span_data.output = (\n                    [message.model_dump()] if message is not None else []\n                )\n            span_generation.span_data.usage = {\n                \"input_tokens\": usage.input_tokens,\n                \"output_tokens\": usage.output_tokens,\n            }\n\n            items = Converter.message_to_output_items(message) if message is not None else []\n\n            return ModelResponse(\n                output=items,\n                usage=usage,\n                response_id=None,\n            )\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; AsyncIterator[TResponseStreamEvent]:\n        \"\"\"\n        Yields a partial message as it is generated, as well as the usage information.\n        \"\"\"\n        with generation_span(\n            model=str(self.model),\n            model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},\n            disabled=tracing.is_disabled(),\n        ) as span_generation:\n            response, stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                span_generation,\n                tracing,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n            async for chunk in ChatCmplStreamHandler.handle_stream(response, stream):\n                yield chunk\n\n                if chunk.type == \"response.completed\":\n                    final_response = chunk.response\n\n            if tracing.include_data() and final_response:\n                span_generation.span_data.output = [final_response.model_dump()]\n\n            if final_response and final_response.usage:\n                span_generation.span_data.usage = {\n                    \"input_tokens\": final_response.usage.input_tokens,\n                    \"output_tokens\": final_response.usage.output_tokens,\n                }\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[True],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; tuple[Response, AsyncStream[ChatCompletionChunk]]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: Literal[False],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ChatCompletion: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        span: Span[GenerationSpanData],\n        tracing: ModelTracing,\n        stream: bool = False,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ChatCompletion | tuple[Response, AsyncStream[ChatCompletionChunk]]:\n        converted_messages = Converter.items_to_messages(input)\n\n        if system_instructions:\n            converted_messages.insert(\n                0,\n                {\n                    \"content\": system_instructions,\n                    \"role\": \"system\",\n                },\n            )\n        if tracing.include_data():\n            span.span_data.input = converted_messages\n\n        parallel_tool_calls = (\n            True\n            if model_settings.parallel_tool_calls and tools and len(tools) &gt; 0\n            else False\n            if model_settings.parallel_tool_calls is False\n            else NOT_GIVEN\n        )\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        response_format = Converter.convert_response_format(output_schema)\n\n        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []\n\n        for handoff in handoffs:\n            converted_tools.append(Converter.convert_handoff_tool(handoff))\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            logger.debug(\n                f\"{json.dumps(converted_messages, indent=2, ensure_ascii=False)}\\n\"\n                f\"Tools:\\n{json.dumps(converted_tools, indent=2, ensure_ascii=False)}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n            )\n\n        reasoning_effort = model_settings.reasoning.effort if model_settings.reasoning else None\n        store = ChatCmplHelpers.get_store_param(self._get_client(), model_settings)\n\n        stream_options = ChatCmplHelpers.get_stream_options_param(\n            self._get_client(), model_settings, stream=stream\n        )\n\n        ret = await self._get_client().chat.completions.create(\n            model=self.model,\n            messages=converted_messages,\n            tools=converted_tools or NOT_GIVEN,\n            temperature=self._non_null_or_not_given(model_settings.temperature),\n            top_p=self._non_null_or_not_given(model_settings.top_p),\n            frequency_penalty=self._non_null_or_not_given(model_settings.frequency_penalty),\n            presence_penalty=self._non_null_or_not_given(model_settings.presence_penalty),\n            max_tokens=self._non_null_or_not_given(model_settings.max_tokens),\n            tool_choice=tool_choice,\n            response_format=response_format,\n            parallel_tool_calls=parallel_tool_calls,\n            stream=stream,\n            stream_options=self._non_null_or_not_given(stream_options),\n            store=self._non_null_or_not_given(store),\n            reasoning_effort=self._non_null_or_not_given(reasoning_effort),\n            top_logprobs=self._non_null_or_not_given(model_settings.top_logprobs),\n            extra_headers={**HEADERS, **(model_settings.extra_headers or {})},\n            extra_query=model_settings.extra_query,\n            extra_body=model_settings.extra_body,\n            metadata=self._non_null_or_not_given(model_settings.metadata),\n            **(model_settings.extra_args or {}),\n        )\n\n        if isinstance(ret, ChatCompletion):\n            return ret\n\n        response = Response(\n            id=FAKE_RESPONSES_ID,\n            created_at=time.time(),\n            model=self.model,\n            object=\"response\",\n            output=[],\n            tool_choice=cast(Literal[\"auto\", \"required\", \"none\"], tool_choice)\n            if tool_choice != NOT_GIVEN\n            else \"auto\",\n            top_p=model_settings.top_p,\n            temperature=model_settings.temperature,\n            tools=[],\n            parallel_tool_calls=parallel_tool_calls or False,\n            reasoning=model_settings.reasoning,\n        )\n        return response, ret\n\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = AsyncOpenAI()\n        return self._client\n</code></pre>"},{"location":"ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel.stream_response","title":"stream_response  <code>async</code>","text":"<pre><code>stream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[TResponseStreamEvent]\n</code></pre> <p>Yields a partial message as it is generated, as well as the usage information.</p> Source code in <code>src/agents/models/openai_chatcompletions.py</code> <pre><code>async def stream_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[TResponseStreamEvent]:\n    \"\"\"\n    Yields a partial message as it is generated, as well as the usage information.\n    \"\"\"\n    with generation_span(\n        model=str(self.model),\n        model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},\n        disabled=tracing.is_disabled(),\n    ) as span_generation:\n        response, stream = await self._fetch_response(\n            system_instructions,\n            input,\n            model_settings,\n            tools,\n            output_schema,\n            handoffs,\n            span_generation,\n            tracing,\n            stream=True,\n            prompt=prompt,\n        )\n\n        final_response: Response | None = None\n        async for chunk in ChatCmplStreamHandler.handle_stream(response, stream):\n            yield chunk\n\n            if chunk.type == \"response.completed\":\n                final_response = chunk.response\n\n        if tracing.include_data() and final_response:\n            span_generation.span_data.output = [final_response.model_dump()]\n\n        if final_response and final_response.usage:\n            span_generation.span_data.usage = {\n                \"input_tokens\": final_response.usage.input_tokens,\n                \"output_tokens\": final_response.usage.output_tokens,\n            }\n</code></pre>"},{"location":"ref/models/openai_responses/","title":"<code>OpenAI Responses model</code>","text":""},{"location":"ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel","title":"OpenAIResponsesModel","text":"<p>               Bases: <code>Model</code></p> <p>Implementation of <code>Model</code> that uses the OpenAI Responses API.</p> Source code in <code>src/agents/models/openai_responses.py</code> <pre><code>class OpenAIResponsesModel(Model):\n    \"\"\"\n    Implementation of `Model` that uses the OpenAI Responses API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str | ChatModel,\n        openai_client: AsyncOpenAI,\n    ) -&gt; None:\n        self.model = model\n        self._client = openai_client\n\n    def _non_null_or_not_given(self, value: Any) -&gt; Any:\n        return value if value is not None else NOT_GIVEN\n\n    async def get_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; ModelResponse:\n        with response_span(disabled=tracing.is_disabled()) as span_response:\n            try:\n                response = await self._fetch_response(\n                    system_instructions,\n                    input,\n                    model_settings,\n                    tools,\n                    output_schema,\n                    handoffs,\n                    previous_response_id,\n                    stream=False,\n                    prompt=prompt,\n                )\n\n                if _debug.DONT_LOG_MODEL_DATA:\n                    logger.debug(\"LLM responded\")\n                else:\n                    logger.debug(\n                        \"LLM resp:\\n\"\n                        f\"\"\"{\n                            json.dumps(\n                                [x.model_dump() for x in response.output],\n                                indent=2,\n                                ensure_ascii=False,\n                            )\n                        }\\n\"\"\"\n                    )\n\n                usage = (\n                    Usage(\n                        requests=1,\n                        input_tokens=response.usage.input_tokens,\n                        output_tokens=response.usage.output_tokens,\n                        total_tokens=response.usage.total_tokens,\n                        input_tokens_details=response.usage.input_tokens_details,\n                        output_tokens_details=response.usage.output_tokens_details,\n                    )\n                    if response.usage\n                    else Usage()\n                )\n\n                if tracing.include_data():\n                    span_response.span_data.response = response\n                    span_response.span_data.input = input\n            except Exception as e:\n                span_response.set_error(\n                    SpanError(\n                        message=\"Error getting response\",\n                        data={\n                            \"error\": str(e) if tracing.include_data() else e.__class__.__name__,\n                        },\n                    )\n                )\n                request_id = e.request_id if isinstance(e, APIStatusError) else None\n                logger.error(f\"Error getting response: {e}. (request_id: {request_id})\")\n                raise\n\n        return ModelResponse(\n            output=response.output,\n            usage=usage,\n            response_id=response.id,\n        )\n\n    async def stream_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        tracing: ModelTracing,\n        previous_response_id: str | None,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; AsyncIterator[ResponseStreamEvent]:\n        \"\"\"\n        Yields a partial message as it is generated, as well as the usage information.\n        \"\"\"\n        with response_span(disabled=tracing.is_disabled()) as span_response:\n            try:\n                stream = await self._fetch_response(\n                    system_instructions,\n                    input,\n                    model_settings,\n                    tools,\n                    output_schema,\n                    handoffs,\n                    previous_response_id,\n                    stream=True,\n                    prompt=prompt,\n                )\n\n                final_response: Response | None = None\n\n                async for chunk in stream:\n                    if isinstance(chunk, ResponseCompletedEvent):\n                        final_response = chunk.response\n                    yield chunk\n\n                if final_response and tracing.include_data():\n                    span_response.span_data.response = final_response\n                    span_response.span_data.input = input\n\n            except Exception as e:\n                span_response.set_error(\n                    SpanError(\n                        message=\"Error streaming response\",\n                        data={\n                            \"error\": str(e) if tracing.include_data() else e.__class__.__name__,\n                        },\n                    )\n                )\n                logger.error(f\"Error streaming response: {e}\")\n                raise\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        previous_response_id: str | None,\n        stream: Literal[True],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; AsyncStream[ResponseStreamEvent]: ...\n\n    @overload\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        previous_response_id: str | None,\n        stream: Literal[False],\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; Response: ...\n\n    async def _fetch_response(\n        self,\n        system_instructions: str | None,\n        input: str | list[TResponseInputItem],\n        model_settings: ModelSettings,\n        tools: list[Tool],\n        output_schema: AgentOutputSchemaBase | None,\n        handoffs: list[Handoff],\n        previous_response_id: str | None,\n        stream: Literal[True] | Literal[False] = False,\n        prompt: ResponsePromptParam | None = None,\n    ) -&gt; Response | AsyncStream[ResponseStreamEvent]:\n        list_input = ItemHelpers.input_to_new_input_list(input)\n\n        parallel_tool_calls = (\n            True\n            if model_settings.parallel_tool_calls and tools and len(tools) &gt; 0\n            else False\n            if model_settings.parallel_tool_calls is False\n            else NOT_GIVEN\n        )\n\n        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)\n        converted_tools = Converter.convert_tools(tools, handoffs)\n        response_format = Converter.get_response_format(output_schema)\n\n        include_set: set[str] = set(converted_tools.includes)\n        if model_settings.response_include is not None:\n            include_set.update(model_settings.response_include)\n        if model_settings.top_logprobs is not None:\n            include_set.add(\"message.output_text.logprobs\")\n        include = cast(list[ResponseIncludable], list(include_set))\n\n        if _debug.DONT_LOG_MODEL_DATA:\n            logger.debug(\"Calling LLM\")\n        else:\n            logger.debug(\n                f\"Calling LLM {self.model} with input:\\n\"\n                f\"{json.dumps(list_input, indent=2, ensure_ascii=False)}\\n\"\n                f\"Tools:\\n{json.dumps(converted_tools.tools, indent=2, ensure_ascii=False)}\\n\"\n                f\"Stream: {stream}\\n\"\n                f\"Tool choice: {tool_choice}\\n\"\n                f\"Response format: {response_format}\\n\"\n                f\"Previous response id: {previous_response_id}\\n\"\n            )\n\n        extra_args = dict(model_settings.extra_args or {})\n        if model_settings.top_logprobs is not None:\n            extra_args[\"top_logprobs\"] = model_settings.top_logprobs\n\n        return await self._client.responses.create(\n            previous_response_id=self._non_null_or_not_given(previous_response_id),\n            instructions=self._non_null_or_not_given(system_instructions),\n            model=self.model,\n            input=list_input,\n            include=include,\n            tools=converted_tools.tools,\n            prompt=self._non_null_or_not_given(prompt),\n            temperature=self._non_null_or_not_given(model_settings.temperature),\n            top_p=self._non_null_or_not_given(model_settings.top_p),\n            truncation=self._non_null_or_not_given(model_settings.truncation),\n            max_output_tokens=self._non_null_or_not_given(model_settings.max_tokens),\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            stream=stream,\n            extra_headers={**_HEADERS, **(model_settings.extra_headers or {})},\n            extra_query=model_settings.extra_query,\n            extra_body=model_settings.extra_body,\n            text=response_format,\n            store=self._non_null_or_not_given(model_settings.store),\n            reasoning=self._non_null_or_not_given(model_settings.reasoning),\n            metadata=self._non_null_or_not_given(model_settings.metadata),\n            **extra_args,\n        )\n\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = AsyncOpenAI()\n        return self._client\n</code></pre>"},{"location":"ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel.stream_response","title":"stream_response  <code>async</code>","text":"<pre><code>stream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[ResponseStreamEvent]\n</code></pre> <p>Yields a partial message as it is generated, as well as the usage information.</p> Source code in <code>src/agents/models/openai_responses.py</code> <pre><code>async def stream_response(\n    self,\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None = None,\n) -&gt; AsyncIterator[ResponseStreamEvent]:\n    \"\"\"\n    Yields a partial message as it is generated, as well as the usage information.\n    \"\"\"\n    with response_span(disabled=tracing.is_disabled()) as span_response:\n        try:\n            stream = await self._fetch_response(\n                system_instructions,\n                input,\n                model_settings,\n                tools,\n                output_schema,\n                handoffs,\n                previous_response_id,\n                stream=True,\n                prompt=prompt,\n            )\n\n            final_response: Response | None = None\n\n            async for chunk in stream:\n                if isinstance(chunk, ResponseCompletedEvent):\n                    final_response = chunk.response\n                yield chunk\n\n            if final_response and tracing.include_data():\n                span_response.span_data.response = final_response\n                span_response.span_data.input = input\n\n        except Exception as e:\n            span_response.set_error(\n                SpanError(\n                    message=\"Error streaming response\",\n                    data={\n                        \"error\": str(e) if tracing.include_data() else e.__class__.__name__,\n                    },\n                )\n            )\n            logger.error(f\"Error streaming response: {e}\")\n            raise\n</code></pre>"},{"location":"ref/models/openai_responses/#agents.models.openai_responses.Converter","title":"Converter","text":"Source code in <code>src/agents/models/openai_responses.py</code> <pre><code>class Converter:\n    @classmethod\n    def convert_tool_choice(\n        cls, tool_choice: Literal[\"auto\", \"required\", \"none\"] | str | MCPToolChoice | None\n    ) -&gt; response_create_params.ToolChoice | NotGiven:\n        if tool_choice is None:\n            return NOT_GIVEN\n        elif isinstance(tool_choice, MCPToolChoice):\n            return {\n                \"server_label\": tool_choice.server_label,\n                \"type\": \"mcp\",\n                \"name\": tool_choice.name,\n            }\n        elif tool_choice == \"required\":\n            return \"required\"\n        elif tool_choice == \"auto\":\n            return \"auto\"\n        elif tool_choice == \"none\":\n            return \"none\"\n        elif tool_choice == \"file_search\":\n            return {\n                \"type\": \"file_search\",\n            }\n        elif tool_choice == \"web_search_preview\":\n            return {\n                \"type\": \"web_search_preview\",\n            }\n        elif tool_choice == \"computer_use_preview\":\n            return {\n                \"type\": \"computer_use_preview\",\n            }\n        elif tool_choice == \"image_generation\":\n            return {\n                \"type\": \"image_generation\",\n            }\n        elif tool_choice == \"code_interpreter\":\n            return {\n                \"type\": \"code_interpreter\",\n            }\n        elif tool_choice == \"mcp\":\n            # Note that this is still here for backwards compatibility,\n            # but migrating to MCPToolChoice is recommended.\n            return {\"type\": \"mcp\"}  # type: ignore [typeddict-item]\n        else:\n            return {\n                \"type\": \"function\",\n                \"name\": tool_choice,\n            }\n\n    @classmethod\n    def get_response_format(\n        cls, output_schema: AgentOutputSchemaBase | None\n    ) -&gt; ResponseTextConfigParam | NotGiven:\n        if output_schema is None or output_schema.is_plain_text():\n            return NOT_GIVEN\n        else:\n            return {\n                \"format\": {\n                    \"type\": \"json_schema\",\n                    \"name\": \"final_output\",\n                    \"schema\": output_schema.json_schema(),\n                    \"strict\": output_schema.is_strict_json_schema(),\n                }\n            }\n\n    @classmethod\n    def convert_tools(\n        cls,\n        tools: list[Tool],\n        handoffs: list[Handoff[Any, Any]],\n    ) -&gt; ConvertedTools:\n        converted_tools: list[ToolParam] = []\n        includes: list[ResponseIncludable] = []\n\n        computer_tools = [tool for tool in tools if isinstance(tool, ComputerTool)]\n        if len(computer_tools) &gt; 1:\n            raise UserError(f\"You can only provide one computer tool. Got {len(computer_tools)}\")\n\n        for tool in tools:\n            converted_tool, include = cls._convert_tool(tool)\n            converted_tools.append(converted_tool)\n            if include:\n                includes.append(include)\n\n        for handoff in handoffs:\n            converted_tools.append(cls._convert_handoff_tool(handoff))\n\n        return ConvertedTools(tools=converted_tools, includes=includes)\n\n    @classmethod\n    def _convert_tool(cls, tool: Tool) -&gt; tuple[ToolParam, ResponseIncludable | None]:\n        \"\"\"Returns converted tool and includes\"\"\"\n\n        if isinstance(tool, FunctionTool):\n            converted_tool: ToolParam = {\n                \"name\": tool.name,\n                \"parameters\": tool.params_json_schema,\n                \"strict\": tool.strict_json_schema,\n                \"type\": \"function\",\n                \"description\": tool.description,\n            }\n            includes: ResponseIncludable | None = None\n        elif isinstance(tool, WebSearchTool):\n            ws: WebSearchToolParam = {\n                \"type\": \"web_search_preview\",\n                \"user_location\": tool.user_location,\n                \"search_context_size\": tool.search_context_size,\n            }\n            converted_tool = ws\n            includes = None\n        elif isinstance(tool, FileSearchTool):\n            converted_tool = {\n                \"type\": \"file_search\",\n                \"vector_store_ids\": tool.vector_store_ids,\n            }\n            if tool.max_num_results:\n                converted_tool[\"max_num_results\"] = tool.max_num_results\n            if tool.ranking_options:\n                converted_tool[\"ranking_options\"] = tool.ranking_options\n            if tool.filters:\n                converted_tool[\"filters\"] = tool.filters\n\n            includes = \"file_search_call.results\" if tool.include_search_results else None\n        elif isinstance(tool, ComputerTool):\n            converted_tool = {\n                \"type\": \"computer_use_preview\",\n                \"environment\": tool.computer.environment,\n                \"display_width\": tool.computer.dimensions[0],\n                \"display_height\": tool.computer.dimensions[1],\n            }\n            includes = None\n        elif isinstance(tool, HostedMCPTool):\n            converted_tool = tool.tool_config\n            includes = None\n        elif isinstance(tool, ImageGenerationTool):\n            converted_tool = tool.tool_config\n            includes = None\n        elif isinstance(tool, CodeInterpreterTool):\n            converted_tool = tool.tool_config\n            includes = None\n        elif isinstance(tool, LocalShellTool):\n            converted_tool = {\n                \"type\": \"local_shell\",\n            }\n            includes = None\n        else:\n            raise UserError(f\"Unknown tool type: {type(tool)}, tool\")\n\n        return converted_tool, includes\n\n    @classmethod\n    def _convert_handoff_tool(cls, handoff: Handoff) -&gt; ToolParam:\n        return {\n            \"name\": handoff.tool_name,\n            \"parameters\": handoff.input_json_schema,\n            \"strict\": handoff.strict_json_schema,\n            \"type\": \"function\",\n            \"description\": handoff.tool_description,\n        }\n</code></pre>"},{"location":"ref/realtime/agent/","title":"<code>RealtimeAgent</code>","text":"<p>               Bases: <code>AgentBase</code>, <code>Generic[TContext]</code></p> <p>A specialized agent instance that is meant to be used within a <code>RealtimeSession</code> to build voice agents. Due to the nature of this agent, some configuration options are not supported that are supported by regular <code>Agent</code> instances. For example: - <code>model</code> choice is not supported, as all RealtimeAgents will be handled by the same model   within a <code>RealtimeSession</code>. - <code>modelSettings</code> is not supported, as all RealtimeAgents will be handled by the same model   within a <code>RealtimeSession</code>. - <code>outputType</code> is not supported, as RealtimeAgents do not support structured outputs. - <code>toolUseBehavior</code> is not supported, as all RealtimeAgents will be handled by the same model   within a <code>RealtimeSession</code>. - <code>voice</code> can be configured on an <code>Agent</code> level; however, it cannot be changed after the first   agent within a <code>RealtimeSession</code> has spoken.</p> <p>See <code>AgentBase</code> for base parameters that are shared with <code>Agent</code>s.</p> Source code in <code>src/agents/realtime/agent.py</code> <pre><code>@dataclass\nclass RealtimeAgent(AgentBase, Generic[TContext]):\n    \"\"\"A specialized agent instance that is meant to be used within a `RealtimeSession` to build\n    voice agents. Due to the nature of this agent, some configuration options are not supported\n    that are supported by regular `Agent` instances. For example:\n    - `model` choice is not supported, as all RealtimeAgents will be handled by the same model\n      within a `RealtimeSession`.\n    - `modelSettings` is not supported, as all RealtimeAgents will be handled by the same model\n      within a `RealtimeSession`.\n    - `outputType` is not supported, as RealtimeAgents do not support structured outputs.\n    - `toolUseBehavior` is not supported, as all RealtimeAgents will be handled by the same model\n      within a `RealtimeSession`.\n    - `voice` can be configured on an `Agent` level; however, it cannot be changed after the first\n      agent within a `RealtimeSession` has spoken.\n\n    See `AgentBase` for base parameters that are shared with `Agent`s.\n    \"\"\"\n\n    instructions: (\n        str\n        | Callable[\n            [RunContextWrapper[TContext], RealtimeAgent[TContext]],\n            MaybeAwaitable[str],\n        ]\n        | None\n    ) = None\n    \"\"\"The instructions for the agent. Will be used as the \"system prompt\" when this agent is\n    invoked. Describes what the agent should do, and how it responds.\n\n    Can either be a string, or a function that dynamically generates instructions for the agent. If\n    you provide a function, it will be called with the context and the agent instance. It must\n    return a string.\n    \"\"\"\n\n    handoffs: list[RealtimeAgent[Any] | Handoff[TContext, RealtimeAgent[Any]]] = field(\n        default_factory=list\n    )\n    \"\"\"Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs,\n    and the agent can choose to delegate to them if relevant. Allows for separation of concerns and\n    modularity.\n    \"\"\"\n\n    hooks: RealtimeAgentHooks | None = None\n    \"\"\"A class that receives callbacks on various lifecycle events for this agent.\n    \"\"\"\n\n    def clone(self, **kwargs: Any) -&gt; RealtimeAgent[TContext]:\n        \"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n        ```\n        new_agent = agent.clone(instructions=\"New instructions\")\n        ```\n        \"\"\"\n        return dataclasses.replace(self, **kwargs)\n\n    async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n        \"\"\"Get the system prompt for the agent.\"\"\"\n        if isinstance(self.instructions, str):\n            return self.instructions\n        elif callable(self.instructions):\n            if inspect.iscoroutinefunction(self.instructions):\n                return await cast(Awaitable[str], self.instructions(run_context, self))\n            else:\n                return cast(str, self.instructions(run_context, self))\n        elif self.instructions is not None:\n            logger.error(f\"Instructions must be a string or a function, got {self.instructions}\")\n\n        return None\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: (\n    str\n    | Callable[\n        [\n            RunContextWrapper[TContext],\n            RealtimeAgent[TContext],\n        ],\n        MaybeAwaitable[str],\n    ]\n    | None\n) = None\n</code></pre> <p>The instructions for the agent. Will be used as the \"system prompt\" when this agent is invoked. Describes what the agent should do, and how it responds.</p> <p>Can either be a string, or a function that dynamically generates instructions for the agent. If you provide a function, it will be called with the context and the agent instance. It must return a string.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.handoffs","title":"handoffs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoffs: list[\n    RealtimeAgent[Any]\n    | Handoff[TContext, RealtimeAgent[Any]]\n] = field(default_factory=list)\n</code></pre> <p>Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs, and the agent can choose to delegate to them if relevant. Allows for separation of concerns and modularity.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.hooks","title":"hooks  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hooks: RealtimeAgentHooks | None = None\n</code></pre> <p>A class that receives callbacks on various lifecycle events for this agent.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the agent.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.handoff_description","title":"handoff_description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handoff_description: str | None = None\n</code></pre> <p>A description of the agent. This is used when the agent is used as a handoff, so that an LLM knows what it does and when to invoke it.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[Tool] = field(default_factory=list)\n</code></pre> <p>A list of tools that the agent can use.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: list[MCPServer] = field(default_factory=list)\n</code></pre> <p>A list of Model Context Protocol servers that the agent can use. Every time the agent runs, it will include tools from these servers in the list of available tools.</p> <p>NOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call <code>server.connect()</code> before passing it to the agent, and <code>server.cleanup()</code> when the server is no longer needed.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.mcp_config","title":"mcp_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_config: MCPConfig = field(\n    default_factory=lambda: MCPConfig()\n)\n</code></pre> <p>Configuration for MCP servers.</p>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.clone","title":"clone","text":"<pre><code>clone(**kwargs: Any) -&gt; RealtimeAgent[TContext]\n</code></pre> <p>Make a copy of the agent, with the given arguments changed. For example, you could do: <pre><code>new_agent = agent.clone(instructions=\"New instructions\")\n</code></pre></p> Source code in <code>src/agents/realtime/agent.py</code> <pre><code>def clone(self, **kwargs: Any) -&gt; RealtimeAgent[TContext]:\n    \"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n    ```\n    new_agent = agent.clone(instructions=\"New instructions\")\n    ```\n    \"\"\"\n    return dataclasses.replace(self, **kwargs)\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.get_system_prompt","title":"get_system_prompt  <code>async</code>","text":"<pre><code>get_system_prompt(\n    run_context: RunContextWrapper[TContext],\n) -&gt; str | None\n</code></pre> <p>Get the system prompt for the agent.</p> Source code in <code>src/agents/realtime/agent.py</code> <pre><code>async def get_system_prompt(self, run_context: RunContextWrapper[TContext]) -&gt; str | None:\n    \"\"\"Get the system prompt for the agent.\"\"\"\n    if isinstance(self.instructions, str):\n        return self.instructions\n    elif callable(self.instructions):\n        if inspect.iscoroutinefunction(self.instructions):\n            return await cast(Awaitable[str], self.instructions(run_context, self))\n        else:\n            return cast(str, self.instructions(run_context, self))\n    elif self.instructions is not None:\n        logger.error(f\"Instructions must be a string or a function, got {self.instructions}\")\n\n    return None\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.get_mcp_tools","title":"get_mcp_tools  <code>async</code>","text":"<pre><code>get_mcp_tools(\n    run_context: RunContextWrapper[TContext],\n) -&gt; list[Tool]\n</code></pre> <p>Fetches the available tools from the MCP servers.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_mcp_tools(self, run_context: RunContextWrapper[TContext]) -&gt; list[Tool]:\n    \"\"\"Fetches the available tools from the MCP servers.\"\"\"\n    convert_schemas_to_strict = self.mcp_config.get(\"convert_schemas_to_strict\", False)\n    return await MCPUtil.get_all_function_tools(\n        self.mcp_servers, convert_schemas_to_strict, run_context, self\n    )\n</code></pre>"},{"location":"ref/realtime/agent/#agents.realtime.agent.RealtimeAgent.get_all_tools","title":"get_all_tools  <code>async</code>","text":"<pre><code>get_all_tools(\n    run_context: RunContextWrapper[Any],\n) -&gt; list[Tool]\n</code></pre> <p>All agent tools, including MCP tools and function tools.</p> Source code in <code>src/agents/agent.py</code> <pre><code>async def get_all_tools(self, run_context: RunContextWrapper[Any]) -&gt; list[Tool]:\n    \"\"\"All agent tools, including MCP tools and function tools.\"\"\"\n    mcp_tools = await self.get_mcp_tools(run_context)\n\n    async def _check_tool_enabled(tool: Tool) -&gt; bool:\n        if not isinstance(tool, FunctionTool):\n            return True\n\n        attr = tool.is_enabled\n        if isinstance(attr, bool):\n            return attr\n        res = attr(run_context, self)\n        if inspect.isawaitable(res):\n            return bool(await res)\n        return bool(res)\n\n    results = await asyncio.gather(*(_check_tool_enabled(t) for t in self.tools))\n    enabled: list[Tool] = [t for t, ok in zip(self.tools, results) if ok]\n    return [*mcp_tools, *enabled]\n</code></pre>"},{"location":"ref/realtime/config/","title":"Realtime Configuration","text":""},{"location":"ref/realtime/config/#run-configuration","title":"Run Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for running a realtime agent session.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeRunConfig(TypedDict):\n    \"\"\"Configuration for running a realtime agent session.\"\"\"\n\n    model_settings: NotRequired[RealtimeSessionModelSettings]\n    \"\"\"Settings for the realtime model session.\"\"\"\n\n    output_guardrails: NotRequired[list[OutputGuardrail[Any]]]\n    \"\"\"List of output guardrails to run on the agent's responses.\"\"\"\n\n    guardrails_settings: NotRequired[RealtimeGuardrailsSettings]\n    \"\"\"Settings for guardrail execution.\"\"\"\n\n    tracing_disabled: NotRequired[bool]\n    \"\"\"Whether tracing is disabled for this run.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.model_settings","title":"model_settings  <code>instance-attribute</code>","text":"<pre><code>model_settings: NotRequired[RealtimeSessionModelSettings]\n</code></pre> <p>Settings for the realtime model session.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.output_guardrails","title":"output_guardrails  <code>instance-attribute</code>","text":"<pre><code>output_guardrails: NotRequired[list[OutputGuardrail[Any]]]\n</code></pre> <p>List of output guardrails to run on the agent's responses.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.guardrails_settings","title":"guardrails_settings  <code>instance-attribute</code>","text":"<pre><code>guardrails_settings: NotRequired[RealtimeGuardrailsSettings]\n</code></pre> <p>Settings for guardrail execution.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeRunConfig.tracing_disabled","title":"tracing_disabled  <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: NotRequired[bool]\n</code></pre> <p>Whether tracing is disabled for this run.</p>"},{"location":"ref/realtime/config/#model-settings","title":"Model Settings","text":"<p>               Bases: <code>TypedDict</code></p> <p>Model settings for a realtime model session.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeSessionModelSettings(TypedDict):\n    \"\"\"Model settings for a realtime model session.\"\"\"\n\n    model_name: NotRequired[RealtimeModelName]\n    \"\"\"The name of the realtime model to use.\"\"\"\n\n    instructions: NotRequired[str]\n    \"\"\"System instructions for the model.\"\"\"\n\n    modalities: NotRequired[list[Literal[\"text\", \"audio\"]]]\n    \"\"\"The modalities the model should support.\"\"\"\n\n    voice: NotRequired[str]\n    \"\"\"The voice to use for audio output.\"\"\"\n\n    input_audio_format: NotRequired[RealtimeAudioFormat]\n    \"\"\"The format for input audio streams.\"\"\"\n\n    output_audio_format: NotRequired[RealtimeAudioFormat]\n    \"\"\"The format for output audio streams.\"\"\"\n\n    input_audio_transcription: NotRequired[RealtimeInputAudioTranscriptionConfig]\n    \"\"\"Configuration for transcribing input audio.\"\"\"\n\n    turn_detection: NotRequired[RealtimeTurnDetectionConfig]\n    \"\"\"Configuration for detecting conversation turns.\"\"\"\n\n    tool_choice: NotRequired[ToolChoice]\n    \"\"\"How the model should choose which tools to call.\"\"\"\n\n    tools: NotRequired[list[Tool]]\n    \"\"\"List of tools available to the model.\"\"\"\n\n    handoffs: NotRequired[list[Handoff]]\n    \"\"\"List of handoff configurations.\"\"\"\n\n    tracing: NotRequired[RealtimeModelTracingConfig | None]\n    \"\"\"Configuration for request tracing.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: NotRequired[RealtimeModelName]\n</code></pre> <p>The name of the realtime model to use.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.instructions","title":"instructions  <code>instance-attribute</code>","text":"<pre><code>instructions: NotRequired[str]\n</code></pre> <p>System instructions for the model.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.modalities","title":"modalities  <code>instance-attribute</code>","text":"<pre><code>modalities: NotRequired[list[Literal['text', 'audio']]]\n</code></pre> <p>The modalities the model should support.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.voice","title":"voice  <code>instance-attribute</code>","text":"<pre><code>voice: NotRequired[str]\n</code></pre> <p>The voice to use for audio output.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.input_audio_format","title":"input_audio_format  <code>instance-attribute</code>","text":"<pre><code>input_audio_format: NotRequired[RealtimeAudioFormat]\n</code></pre> <p>The format for input audio streams.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.output_audio_format","title":"output_audio_format  <code>instance-attribute</code>","text":"<pre><code>output_audio_format: NotRequired[RealtimeAudioFormat]\n</code></pre> <p>The format for output audio streams.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.input_audio_transcription","title":"input_audio_transcription  <code>instance-attribute</code>","text":"<pre><code>input_audio_transcription: NotRequired[\n    RealtimeInputAudioTranscriptionConfig\n]\n</code></pre> <p>Configuration for transcribing input audio.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.turn_detection","title":"turn_detection  <code>instance-attribute</code>","text":"<pre><code>turn_detection: NotRequired[RealtimeTurnDetectionConfig]\n</code></pre> <p>Configuration for detecting conversation turns.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.tool_choice","title":"tool_choice  <code>instance-attribute</code>","text":"<pre><code>tool_choice: NotRequired[ToolChoice]\n</code></pre> <p>How the model should choose which tools to call.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools: NotRequired[list[Tool]]\n</code></pre> <p>List of tools available to the model.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.handoffs","title":"handoffs  <code>instance-attribute</code>","text":"<pre><code>handoffs: NotRequired[list[Handoff]]\n</code></pre> <p>List of handoff configurations.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeSessionModelSettings.tracing","title":"tracing  <code>instance-attribute</code>","text":"<pre><code>tracing: NotRequired[RealtimeModelTracingConfig | None]\n</code></pre> <p>Configuration for request tracing.</p>"},{"location":"ref/realtime/config/#audio-configuration","title":"Audio Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for audio transcription in realtime sessions.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeInputAudioTranscriptionConfig(TypedDict):\n    \"\"\"Configuration for audio transcription in realtime sessions.\"\"\"\n\n    language: NotRequired[str]\n    \"\"\"The language code for transcription.\"\"\"\n\n    model: NotRequired[Literal[\"gpt-4o-transcribe\", \"gpt-4o-mini-transcribe\", \"whisper-1\"] | str]\n    \"\"\"The transcription model to use.\"\"\"\n\n    prompt: NotRequired[str]\n    \"\"\"An optional prompt to guide transcription.\"\"\"\n</code></pre> <p>               Bases: <code>TypedDict</code></p> <p>Turn detection config. Allows extra vendor keys if needed.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeTurnDetectionConfig(TypedDict):\n    \"\"\"Turn detection config. Allows extra vendor keys if needed.\"\"\"\n\n    type: NotRequired[Literal[\"semantic_vad\", \"server_vad\"]]\n    \"\"\"The type of voice activity detection to use.\"\"\"\n\n    create_response: NotRequired[bool]\n    \"\"\"Whether to create a response when a turn is detected.\"\"\"\n\n    eagerness: NotRequired[Literal[\"auto\", \"low\", \"medium\", \"high\"]]\n    \"\"\"How eagerly to detect turn boundaries.\"\"\"\n\n    interrupt_response: NotRequired[bool]\n    \"\"\"Whether to allow interrupting the assistant's response.\"\"\"\n\n    prefix_padding_ms: NotRequired[int]\n    \"\"\"Padding time in milliseconds before turn detection.\"\"\"\n\n    silence_duration_ms: NotRequired[int]\n    \"\"\"Duration of silence in milliseconds to trigger turn detection.\"\"\"\n\n    threshold: NotRequired[float]\n    \"\"\"The threshold for voice activity detection.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioTranscriptionConfig.language","title":"language  <code>instance-attribute</code>","text":"<pre><code>language: NotRequired[str]\n</code></pre> <p>The language code for transcription.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioTranscriptionConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: NotRequired[\n    Literal[\n        \"gpt-4o-transcribe\",\n        \"gpt-4o-mini-transcribe\",\n        \"whisper-1\",\n    ]\n    | str\n]\n</code></pre> <p>The transcription model to use.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeInputAudioTranscriptionConfig.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: NotRequired[str]\n</code></pre> <p>An optional prompt to guide transcription.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: NotRequired[Literal['semantic_vad', 'server_vad']]\n</code></pre> <p>The type of voice activity detection to use.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.create_response","title":"create_response  <code>instance-attribute</code>","text":"<pre><code>create_response: NotRequired[bool]\n</code></pre> <p>Whether to create a response when a turn is detected.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.eagerness","title":"eagerness  <code>instance-attribute</code>","text":"<pre><code>eagerness: NotRequired[\n    Literal[\"auto\", \"low\", \"medium\", \"high\"]\n]\n</code></pre> <p>How eagerly to detect turn boundaries.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.interrupt_response","title":"interrupt_response  <code>instance-attribute</code>","text":"<pre><code>interrupt_response: NotRequired[bool]\n</code></pre> <p>Whether to allow interrupting the assistant's response.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.prefix_padding_ms","title":"prefix_padding_ms  <code>instance-attribute</code>","text":"<pre><code>prefix_padding_ms: NotRequired[int]\n</code></pre> <p>Padding time in milliseconds before turn detection.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.silence_duration_ms","title":"silence_duration_ms  <code>instance-attribute</code>","text":"<pre><code>silence_duration_ms: NotRequired[int]\n</code></pre> <p>Duration of silence in milliseconds to trigger turn detection.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeTurnDetectionConfig.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold: NotRequired[float]\n</code></pre> <p>The threshold for voice activity detection.</p>"},{"location":"ref/realtime/config/#guardrails-settings","title":"Guardrails Settings","text":"<p>               Bases: <code>TypedDict</code></p> <p>Settings for output guardrails in realtime sessions.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeGuardrailsSettings(TypedDict):\n    \"\"\"Settings for output guardrails in realtime sessions.\"\"\"\n\n    debounce_text_length: NotRequired[int]\n    \"\"\"\n    The minimum number of characters to accumulate before running guardrails on transcript\n    deltas. Defaults to 100. Guardrails run every time the accumulated text reaches\n    1x, 2x, 3x, etc. times this threshold.\n    \"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeGuardrailsSettings.debounce_text_length","title":"debounce_text_length  <code>instance-attribute</code>","text":"<pre><code>debounce_text_length: NotRequired[int]\n</code></pre> <p>The minimum number of characters to accumulate before running guardrails on transcript deltas. Defaults to 100. Guardrails run every time the accumulated text reaches 1x, 2x, 3x, etc. times this threshold.</p>"},{"location":"ref/realtime/config/#model-configuration","title":"Model Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Options for connecting to a realtime model.</p> Source code in <code>src/agents/realtime/model.py</code> <pre><code>class RealtimeModelConfig(TypedDict):\n    \"\"\"Options for connecting to a realtime model.\"\"\"\n\n    api_key: NotRequired[str | Callable[[], MaybeAwaitable[str]]]\n    \"\"\"The API key (or function that returns a key) to use when connecting. If unset, the model will\n    try to use a sane default. For example, the OpenAI Realtime model will try to use the\n    `OPENAI_API_KEY`  environment variable.\n    \"\"\"\n\n    url: NotRequired[str]\n    \"\"\"The URL to use when connecting. If unset, the model will use a sane default. For example,\n    the OpenAI Realtime model will use the default OpenAI WebSocket URL.\n    \"\"\"\n\n    initial_model_settings: NotRequired[RealtimeSessionModelSettings]\n    \"\"\"The initial model settings to use when connecting.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key: NotRequired[\n    str | Callable[[], MaybeAwaitable[str]]\n]\n</code></pre> <p>The API key (or function that returns a key) to use when connecting. If unset, the model will try to use a sane default. For example, the OpenAI Realtime model will try to use the <code>OPENAI_API_KEY</code>  environment variable.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: NotRequired[str]\n</code></pre> <p>The URL to use when connecting. If unset, the model will use a sane default. For example, the OpenAI Realtime model will use the default OpenAI WebSocket URL.</p>"},{"location":"ref/realtime/config/#agents.realtime.model.RealtimeModelConfig.initial_model_settings","title":"initial_model_settings  <code>instance-attribute</code>","text":"<pre><code>initial_model_settings: NotRequired[\n    RealtimeSessionModelSettings\n]\n</code></pre> <p>The initial model settings to use when connecting.</p>"},{"location":"ref/realtime/config/#tracing-configuration","title":"Tracing Configuration","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for tracing in realtime model sessions.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeModelTracingConfig(TypedDict):\n    \"\"\"Configuration for tracing in realtime model sessions.\"\"\"\n\n    workflow_name: NotRequired[str]\n    \"\"\"The workflow name to use for tracing.\"\"\"\n\n    group_id: NotRequired[str]\n    \"\"\"A group identifier to use for tracing, to link multiple traces together.\"\"\"\n\n    metadata: NotRequired[dict[str, Any]]\n    \"\"\"Additional metadata to include with the trace.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeModelTracingConfig.workflow_name","title":"workflow_name  <code>instance-attribute</code>","text":"<pre><code>workflow_name: NotRequired[str]\n</code></pre> <p>The workflow name to use for tracing.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeModelTracingConfig.group_id","title":"group_id  <code>instance-attribute</code>","text":"<pre><code>group_id: NotRequired[str]\n</code></pre> <p>A group identifier to use for tracing, to link multiple traces together.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeModelTracingConfig.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: NotRequired[dict[str, Any]]\n</code></pre> <p>Additional metadata to include with the trace.</p>"},{"location":"ref/realtime/config/#user-input-types","title":"User Input Types","text":"<p>User input that can be a string or structured message.</p> <p>               Bases: <code>TypedDict</code></p> <p>A text input from the user.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeUserInputText(TypedDict):\n    \"\"\"A text input from the user.\"\"\"\n\n    type: Literal[\"input_text\"]\n    \"\"\"The type identifier for text input.\"\"\"\n\n    text: str\n    \"\"\"The text content from the user.\"\"\"\n</code></pre> <p>               Bases: <code>TypedDict</code></p> <p>A message input from the user.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeUserInputMessage(TypedDict):\n    \"\"\"A message input from the user.\"\"\"\n\n    type: Literal[\"message\"]\n    \"\"\"The type identifier for message inputs.\"\"\"\n\n    role: Literal[\"user\"]\n    \"\"\"The role identifier for user messages.\"\"\"\n\n    content: list[RealtimeUserInputText]\n    \"\"\"List of text content items in the message.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputText.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['input_text']\n</code></pre> <p>The type identifier for text input.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputText.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text content from the user.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputMessage.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['message']\n</code></pre> <p>The type identifier for message inputs.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Literal['user']\n</code></pre> <p>The role identifier for user messages.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeUserInputMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[RealtimeUserInputText]\n</code></pre> <p>List of text content items in the message.</p>"},{"location":"ref/realtime/config/#client-messages","title":"Client Messages","text":"<p>               Bases: <code>TypedDict</code></p> <p>A raw message to be sent to the model.</p> Source code in <code>src/agents/realtime/config.py</code> <pre><code>class RealtimeClientMessage(TypedDict):\n    \"\"\"A raw message to be sent to the model.\"\"\"\n\n    type: str  # explicitly required\n    \"\"\"The type of the message.\"\"\"\n\n    other_data: NotRequired[dict[str, Any]]\n    \"\"\"Merged into the message body.\"\"\"\n</code></pre>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeClientMessage.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: str\n</code></pre> <p>The type of the message.</p>"},{"location":"ref/realtime/config/#agents.realtime.config.RealtimeClientMessage.other_data","title":"other_data  <code>instance-attribute</code>","text":"<pre><code>other_data: NotRequired[dict[str, Any]]\n</code></pre> <p>Merged into the message body.</p>"},{"location":"ref/realtime/config/#type-aliases","title":"Type Aliases","text":"<p>The name of a realtime model.</p> <p>The audio format for realtime audio streams.</p>"},{"location":"ref/realtime/events/","title":"Realtime Events","text":""},{"location":"ref/realtime/events/#session-events","title":"Session Events","text":"<p>An event emitted by the realtime session.</p>"},{"location":"ref/realtime/events/#event-types","title":"Event Types","text":""},{"location":"ref/realtime/events/#agent-events","title":"Agent Events","text":"<p>A new agent has started.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAgentStartEvent:\n    \"\"\"A new agent has started.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The new agent.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"agent_start\"] = \"agent_start\"\n</code></pre> <p>An agent has ended.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAgentEndEvent:\n    \"\"\"An agent has ended.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The agent that ended.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"agent_end\"] = \"agent_end\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentStartEvent.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The new agent.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentStartEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentEndEvent.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The agent that ended.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAgentEndEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#audio-events","title":"Audio Events","text":"<p>Triggered when the agent generates new audio to be played.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAudio:\n    \"\"\"Triggered when the agent generates new audio to be played.\"\"\"\n\n    audio: RealtimeModelAudioEvent\n    \"\"\"The audio event from the model layer.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"audio\"] = \"audio\"\n</code></pre> <p>Triggered when the agent stops generating audio.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAudioEnd:\n    \"\"\"Triggered when the agent stops generating audio.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"audio_end\"] = \"audio_end\"\n</code></pre> <p>Triggered when the agent is interrupted. Can be listened to by the user to stop audio playback or give visual indicators to the user.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeAudioInterrupted:\n    \"\"\"Triggered when the agent is interrupted. Can be listened to by the user to stop audio\n    playback or give visual indicators to the user.\n    \"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"audio_interrupted\"] = \"audio_interrupted\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudio.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: RealtimeModelAudioEvent\n</code></pre> <p>The audio event from the model layer.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudio.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioEnd.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeAudioInterrupted.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#tool-events","title":"Tool Events","text":"<p>An agent is starting a tool call.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeToolStart:\n    \"\"\"An agent is starting a tool call.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The agent that updated.\"\"\"\n\n    tool: Tool\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"tool_start\"] = \"tool_start\"\n</code></pre> <p>An agent has ended a tool call.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeToolEnd:\n    \"\"\"An agent has ended a tool call.\"\"\"\n\n    agent: RealtimeAgent\n    \"\"\"The agent that ended the tool call.\"\"\"\n\n    tool: Tool\n    \"\"\"The tool that was called.\"\"\"\n\n    output: Any\n    \"\"\"The output of the tool call.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"tool_end\"] = \"tool_end\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolStart.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The agent that updated.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolStart.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent: RealtimeAgent\n</code></pre> <p>The agent that ended the tool call.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.tool","title":"tool  <code>instance-attribute</code>","text":"<pre><code>tool: Tool\n</code></pre> <p>The tool that was called.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: Any\n</code></pre> <p>The output of the tool call.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeToolEnd.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#handoff-events","title":"Handoff Events","text":"<p>An agent has handed off to another agent.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeHandoffEvent:\n    \"\"\"An agent has handed off to another agent.\"\"\"\n\n    from_agent: RealtimeAgent\n    \"\"\"The agent that handed off.\"\"\"\n\n    to_agent: RealtimeAgent\n    \"\"\"The agent that was handed off to.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"handoff\"] = \"handoff\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHandoffEvent.from_agent","title":"from_agent  <code>instance-attribute</code>","text":"<pre><code>from_agent: RealtimeAgent\n</code></pre> <p>The agent that handed off.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHandoffEvent.to_agent","title":"to_agent  <code>instance-attribute</code>","text":"<pre><code>to_agent: RealtimeAgent\n</code></pre> <p>The agent that was handed off to.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHandoffEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#guardrail-events","title":"Guardrail Events","text":"<p>A guardrail has been tripped and the agent has been interrupted.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeGuardrailTripped:\n    \"\"\"A guardrail has been tripped and the agent has been interrupted.\"\"\"\n\n    guardrail_results: list[OutputGuardrailResult]\n    \"\"\"The results from all triggered guardrails.\"\"\"\n\n    message: str\n    \"\"\"The message that was being generated when the guardrail was triggered.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"guardrail_tripped\"] = \"guardrail_tripped\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeGuardrailTripped.guardrail_results","title":"guardrail_results  <code>instance-attribute</code>","text":"<pre><code>guardrail_results: list[OutputGuardrailResult]\n</code></pre> <p>The results from all triggered guardrails.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeGuardrailTripped.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>The message that was being generated when the guardrail was triggered.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeGuardrailTripped.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#history-events","title":"History Events","text":"<p>A new item has been added to the history.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeHistoryAdded:\n    \"\"\"A new item has been added to the history.\"\"\"\n\n    item: RealtimeItem\n    \"\"\"The new item that was added to the history.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"history_added\"] = \"history_added\"\n</code></pre> <p>The history has been updated. Contains the full history of the session.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeHistoryUpdated:\n    \"\"\"The history has been updated. Contains the full history of the session.\"\"\"\n\n    history: list[RealtimeItem]\n    \"\"\"The full history of the session.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"history_updated\"] = \"history_updated\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryAdded.item","title":"item  <code>instance-attribute</code>","text":"<pre><code>item: RealtimeItem\n</code></pre> <p>The new item that was added to the history.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryAdded.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryUpdated.history","title":"history  <code>instance-attribute</code>","text":"<pre><code>history: list[RealtimeItem]\n</code></pre> <p>The full history of the session.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeHistoryUpdated.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#error-events","title":"Error Events","text":"<p>An error has occurred.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeError:\n    \"\"\"An error has occurred.\"\"\"\n\n    error: Any\n    \"\"\"The error that occurred.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"error\"] = \"error\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeError.error","title":"error  <code>instance-attribute</code>","text":"<pre><code>error: Any\n</code></pre> <p>The error that occurred.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeError.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/events/#raw-model-events","title":"Raw Model Events","text":"<p>Forwards raw events from the model layer.</p> Source code in <code>src/agents/realtime/events.py</code> <pre><code>@dataclass\nclass RealtimeRawModelEvent:\n    \"\"\"Forwards raw events from the model layer.\"\"\"\n\n    data: RealtimeModelEvent\n    \"\"\"The raw data from the model layer.\"\"\"\n\n    info: RealtimeEventInfo\n    \"\"\"Common info for all events, such as the context.\"\"\"\n\n    type: Literal[\"raw_model_event\"] = \"raw_model_event\"\n</code></pre>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeRawModelEvent.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: RealtimeModelEvent\n</code></pre> <p>The raw data from the model layer.</p>"},{"location":"ref/realtime/events/#agents.realtime.events.RealtimeRawModelEvent.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: RealtimeEventInfo\n</code></pre> <p>Common info for all events, such as the context.</p>"},{"location":"ref/realtime/runner/","title":"<code>RealtimeRunner</code>","text":"<p>A <code>RealtimeRunner</code> is the equivalent of <code>Runner</code> for realtime agents. It automatically handles multiple turns by maintaining a persistent connection with the underlying model layer.</p> <p>The session manages the local history copy, executes tools, runs guardrails and facilitates handoffs between agents.</p> <p>Since this code runs on your server, it uses WebSockets by default. You can optionally create your own custom model layer by implementing the <code>RealtimeModel</code> interface.</p> Source code in <code>src/agents/realtime/runner.py</code> <pre><code>class RealtimeRunner:\n    \"\"\"A `RealtimeRunner` is the equivalent of `Runner` for realtime agents. It automatically\n    handles multiple turns by maintaining a persistent connection with the underlying model\n    layer.\n\n    The session manages the local history copy, executes tools, runs guardrails and facilitates\n    handoffs between agents.\n\n    Since this code runs on your server, it uses WebSockets by default. You can optionally create\n    your own custom model layer by implementing the `RealtimeModel` interface.\n    \"\"\"\n\n    def __init__(\n        self,\n        starting_agent: RealtimeAgent,\n        *,\n        model: RealtimeModel | None = None,\n        config: RealtimeRunConfig | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the realtime runner.\n\n        Args:\n            starting_agent: The agent to start the session with.\n            context: The context to use for the session.\n            model: The model to use. If not provided, will use a default OpenAI realtime model.\n            config: Override parameters to use for the entire run.\n        \"\"\"\n        self._starting_agent = starting_agent\n        self._config = config\n        self._model = model or OpenAIRealtimeWebSocketModel()\n\n    async def run(\n        self, *, context: TContext | None = None, model_config: RealtimeModelConfig | None = None\n    ) -&gt; RealtimeSession:\n        \"\"\"Start and returns a realtime session.\n\n        Returns:\n            RealtimeSession: A session object that allows bidirectional communication with the\n            realtime model.\n\n        Example:\n            ```python\n            runner = RealtimeRunner(agent)\n            async with await runner.run() as session:\n                await session.send_message(\"Hello\")\n                async for event in session:\n                    print(event)\n            ```\n        \"\"\"\n        # Create and return the connection\n        session = RealtimeSession(\n            model=self._model,\n            agent=self._starting_agent,\n            context=context,\n            model_config=model_config,\n            run_config=self._config,\n        )\n\n        return session\n</code></pre>"},{"location":"ref/realtime/runner/#agents.realtime.runner.RealtimeRunner.__init__","title":"__init__","text":"<pre><code>__init__(\n    starting_agent: RealtimeAgent,\n    *,\n    model: RealtimeModel | None = None,\n    config: RealtimeRunConfig | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the realtime runner.</p> <p>Parameters:</p> Name Type Description Default <code>starting_agent</code> <code>RealtimeAgent</code> <p>The agent to start the session with.</p> required <code>context</code> <p>The context to use for the session.</p> required <code>model</code> <code>RealtimeModel | None</code> <p>The model to use. If not provided, will use a default OpenAI realtime model.</p> <code>None</code> <code>config</code> <code>RealtimeRunConfig | None</code> <p>Override parameters to use for the entire run.</p> <code>None</code> Source code in <code>src/agents/realtime/runner.py</code> <pre><code>def __init__(\n    self,\n    starting_agent: RealtimeAgent,\n    *,\n    model: RealtimeModel | None = None,\n    config: RealtimeRunConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the realtime runner.\n\n    Args:\n        starting_agent: The agent to start the session with.\n        context: The context to use for the session.\n        model: The model to use. If not provided, will use a default OpenAI realtime model.\n        config: Override parameters to use for the entire run.\n    \"\"\"\n    self._starting_agent = starting_agent\n    self._config = config\n    self._model = model or OpenAIRealtimeWebSocketModel()\n</code></pre>"},{"location":"ref/realtime/runner/#agents.realtime.runner.RealtimeRunner.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    *,\n    context: TContext | None = None,\n    model_config: RealtimeModelConfig | None = None,\n) -&gt; RealtimeSession\n</code></pre> <p>Start and returns a realtime session.</p> <p>Returns:</p> Name Type Description <code>RealtimeSession</code> <code>RealtimeSession</code> <p>A session object that allows bidirectional communication with the</p> <code>RealtimeSession</code> <p>realtime model.</p> Example <pre><code>runner = RealtimeRunner(agent)\nasync with await runner.run() as session:\n    await session.send_message(\"Hello\")\n    async for event in session:\n        print(event)\n</code></pre> Source code in <code>src/agents/realtime/runner.py</code> <pre><code>async def run(\n    self, *, context: TContext | None = None, model_config: RealtimeModelConfig | None = None\n) -&gt; RealtimeSession:\n    \"\"\"Start and returns a realtime session.\n\n    Returns:\n        RealtimeSession: A session object that allows bidirectional communication with the\n        realtime model.\n\n    Example:\n        ```python\n        runner = RealtimeRunner(agent)\n        async with await runner.run() as session:\n            await session.send_message(\"Hello\")\n            async for event in session:\n                print(event)\n        ```\n    \"\"\"\n    # Create and return the connection\n    session = RealtimeSession(\n        model=self._model,\n        agent=self._starting_agent,\n        context=context,\n        model_config=model_config,\n        run_config=self._config,\n    )\n\n    return session\n</code></pre>"},{"location":"ref/realtime/session/","title":"<code>RealtimeSession</code>","text":"<p>               Bases: <code>RealtimeModelListener</code></p> <p>A connection to a realtime model. It streams events from the model to you, and allows you to send messages and audio to the model.</p> Example <pre><code>runner = RealtimeRunner(agent)\nasync with await runner.run() as session:\n    # Send messages\n    await session.send_message(\"Hello\")\n    await session.send_audio(audio_bytes)\n\n    # Stream events\n    async for event in session:\n        if event.type == \"audio\":\n            # Handle audio event\n            pass\n</code></pre> Source code in <code>src/agents/realtime/session.py</code> <pre><code>class RealtimeSession(RealtimeModelListener):\n    \"\"\"A connection to a realtime model. It streams events from the model to you, and allows you to\n    send messages and audio to the model.\n\n    Example:\n        ```python\n        runner = RealtimeRunner(agent)\n        async with await runner.run() as session:\n            # Send messages\n            await session.send_message(\"Hello\")\n            await session.send_audio(audio_bytes)\n\n            # Stream events\n            async for event in session:\n                if event.type == \"audio\":\n                    # Handle audio event\n                    pass\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model: RealtimeModel,\n        agent: RealtimeAgent,\n        context: TContext | None,\n        model_config: RealtimeModelConfig | None = None,\n        run_config: RealtimeRunConfig | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the session.\n\n        Args:\n            model: The model to use.\n            agent: The current agent.\n            context: The context object.\n            model_config: Model configuration.\n            run_config: Runtime configuration including guardrails.\n        \"\"\"\n        self._model = model\n        self._current_agent = agent\n        self._context_wrapper = RunContextWrapper(context)\n        self._event_info = RealtimeEventInfo(context=self._context_wrapper)\n        self._history: list[RealtimeItem] = []\n        self._model_config = model_config or {}\n        self._run_config = run_config or {}\n        self._event_queue: asyncio.Queue[RealtimeSessionEvent] = asyncio.Queue()\n        self._closed = False\n        self._stored_exception: Exception | None = None\n\n        # Guardrails state tracking\n        self._interrupted_by_guardrail = False\n        self._item_transcripts: dict[str, str] = {}  # item_id -&gt; accumulated transcript\n        self._item_guardrail_run_counts: dict[str, int] = {}  # item_id -&gt; run count\n        self._debounce_text_length = self._run_config.get(\"guardrails_settings\", {}).get(\n            \"debounce_text_length\", 100\n        )\n\n        self._guardrail_tasks: set[asyncio.Task[Any]] = set()\n\n    @property\n    def model(self) -&gt; RealtimeModel:\n        \"\"\"Access the underlying model for adding listeners or other direct interaction.\"\"\"\n        return self._model\n\n    async def __aenter__(self) -&gt; RealtimeSession:\n        \"\"\"Start the session by connecting to the model. After this, you will be able to stream\n        events from the model and send messages and audio to the model.\n        \"\"\"\n        # Add ourselves as a listener\n        self._model.add_listener(self)\n\n        model_config = self._model_config.copy()\n        model_config[\"initial_model_settings\"] = await self._get_updated_model_settings_from_agent(\n            self._current_agent\n        )\n\n        # Connect to the model\n        await self._model.connect(model_config)\n\n        # Emit initial history update\n        await self._put_event(\n            RealtimeHistoryUpdated(\n                history=self._history,\n                info=self._event_info,\n            )\n        )\n\n        return self\n\n    async def enter(self) -&gt; RealtimeSession:\n        \"\"\"Enter the async context manager. We strongly recommend using the async context manager\n        pattern instead of this method. If you use this, you need to manually call `close()` when\n        you are done.\n        \"\"\"\n        return await self.__aenter__()\n\n    async def __aexit__(self, _exc_type: Any, _exc_val: Any, _exc_tb: Any) -&gt; None:\n        \"\"\"End the session.\"\"\"\n        await self.close()\n\n    async def __aiter__(self) -&gt; AsyncIterator[RealtimeSessionEvent]:\n        \"\"\"Iterate over events from the session.\"\"\"\n        while not self._closed:\n            try:\n                # Check if there's a stored exception to raise\n                if self._stored_exception is not None:\n                    # Clean up resources before raising\n                    await self._cleanup()\n                    raise self._stored_exception\n\n                event = await self._event_queue.get()\n                yield event\n            except asyncio.CancelledError:\n                break\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the session.\"\"\"\n        await self._cleanup()\n\n    async def send_message(self, message: RealtimeUserInput) -&gt; None:\n        \"\"\"Send a message to the model.\"\"\"\n        await self._model.send_event(RealtimeModelSendUserInput(user_input=message))\n\n    async def send_audio(self, audio: bytes, *, commit: bool = False) -&gt; None:\n        \"\"\"Send a raw audio chunk to the model.\"\"\"\n        await self._model.send_event(RealtimeModelSendAudio(audio=audio, commit=commit))\n\n    async def interrupt(self) -&gt; None:\n        \"\"\"Interrupt the model.\"\"\"\n        await self._model.send_event(RealtimeModelSendInterrupt())\n\n    async def on_event(self, event: RealtimeModelEvent) -&gt; None:\n        await self._put_event(RealtimeRawModelEvent(data=event, info=self._event_info))\n\n        if event.type == \"error\":\n            await self._put_event(RealtimeError(info=self._event_info, error=event.error))\n        elif event.type == \"function_call\":\n            await self._handle_tool_call(event)\n        elif event.type == \"audio\":\n            await self._put_event(RealtimeAudio(info=self._event_info, audio=event))\n        elif event.type == \"audio_interrupted\":\n            await self._put_event(RealtimeAudioInterrupted(info=self._event_info))\n        elif event.type == \"audio_done\":\n            await self._put_event(RealtimeAudioEnd(info=self._event_info))\n        elif event.type == \"input_audio_transcription_completed\":\n            self._history = RealtimeSession._get_new_history(self._history, event)\n            await self._put_event(\n                RealtimeHistoryUpdated(info=self._event_info, history=self._history)\n            )\n        elif event.type == \"transcript_delta\":\n            # Accumulate transcript text for guardrail debouncing per item_id\n            item_id = event.item_id\n            if item_id not in self._item_transcripts:\n                self._item_transcripts[item_id] = \"\"\n                self._item_guardrail_run_counts[item_id] = 0\n\n            self._item_transcripts[item_id] += event.delta\n\n            # Check if we should run guardrails based on debounce threshold\n            current_length = len(self._item_transcripts[item_id])\n            threshold = self._debounce_text_length\n            next_run_threshold = (self._item_guardrail_run_counts[item_id] + 1) * threshold\n\n            if current_length &gt;= next_run_threshold:\n                self._item_guardrail_run_counts[item_id] += 1\n                self._enqueue_guardrail_task(self._item_transcripts[item_id])\n        elif event.type == \"item_updated\":\n            is_new = not any(item.item_id == event.item.item_id for item in self._history)\n            self._history = self._get_new_history(self._history, event.item)\n            if is_new:\n                new_item = next(\n                    item for item in self._history if item.item_id == event.item.item_id\n                )\n                await self._put_event(RealtimeHistoryAdded(info=self._event_info, item=new_item))\n            else:\n                await self._put_event(\n                    RealtimeHistoryUpdated(info=self._event_info, history=self._history)\n                )\n        elif event.type == \"item_deleted\":\n            deleted_id = event.item_id\n            self._history = [item for item in self._history if item.item_id != deleted_id]\n            await self._put_event(\n                RealtimeHistoryUpdated(info=self._event_info, history=self._history)\n            )\n        elif event.type == \"connection_status\":\n            pass\n        elif event.type == \"turn_started\":\n            await self._put_event(\n                RealtimeAgentStartEvent(\n                    agent=self._current_agent,\n                    info=self._event_info,\n                )\n            )\n        elif event.type == \"turn_ended\":\n            # Clear guardrail state for next turn\n            self._item_transcripts.clear()\n            self._item_guardrail_run_counts.clear()\n            self._interrupted_by_guardrail = False\n\n            await self._put_event(\n                RealtimeAgentEndEvent(\n                    agent=self._current_agent,\n                    info=self._event_info,\n                )\n            )\n        elif event.type == \"exception\":\n            # Store the exception to be raised in __aiter__\n            self._stored_exception = event.exception\n        elif event.type == \"other\":\n            pass\n        else:\n            assert_never(event)\n\n    async def _put_event(self, event: RealtimeSessionEvent) -&gt; None:\n        \"\"\"Put an event into the queue.\"\"\"\n        await self._event_queue.put(event)\n\n    async def _handle_tool_call(self, event: RealtimeModelToolCallEvent) -&gt; None:\n        \"\"\"Handle a tool call event.\"\"\"\n        tools, handoffs = await asyncio.gather(\n            self._current_agent.get_all_tools(self._context_wrapper),\n            self._get_handoffs(self._current_agent, self._context_wrapper),\n        )\n        function_map = {tool.name: tool for tool in tools if isinstance(tool, FunctionTool)}\n        handoff_map = {handoff.tool_name: handoff for handoff in handoffs}\n\n        if event.name in function_map:\n            await self._put_event(\n                RealtimeToolStart(\n                    info=self._event_info,\n                    tool=function_map[event.name],\n                    agent=self._current_agent,\n                )\n            )\n\n            func_tool = function_map[event.name]\n            tool_context = ToolContext(\n                context=self._context_wrapper.context,\n                usage=self._context_wrapper.usage,\n                tool_name=event.name,\n                tool_call_id=event.call_id,\n            )\n            result = await func_tool.on_invoke_tool(tool_context, event.arguments)\n\n            await self._model.send_event(\n                RealtimeModelSendToolOutput(\n                    tool_call=event, output=str(result), start_response=True\n                )\n            )\n\n            await self._put_event(\n                RealtimeToolEnd(\n                    info=self._event_info,\n                    tool=func_tool,\n                    output=result,\n                    agent=self._current_agent,\n                )\n            )\n        elif event.name in handoff_map:\n            handoff = handoff_map[event.name]\n            tool_context = ToolContext(\n                context=self._context_wrapper.context,\n                usage=self._context_wrapper.usage,\n                tool_name=event.name,\n                tool_call_id=event.call_id,\n            )\n\n            # Execute the handoff to get the new agent\n            result = await handoff.on_invoke_handoff(self._context_wrapper, event.arguments)\n            if not isinstance(result, RealtimeAgent):\n                raise UserError(\n                    f\"Handoff {handoff.tool_name} returned invalid result: {type(result)}\"\n                )\n\n            # Store previous agent for event\n            previous_agent = self._current_agent\n\n            # Update current agent\n            self._current_agent = result\n\n            # Get updated model settings from new agent\n            updated_settings = await self._get_updated_model_settings_from_agent(\n                self._current_agent\n            )\n\n            # Send handoff event\n            await self._put_event(\n                RealtimeHandoffEvent(\n                    from_agent=previous_agent,\n                    to_agent=self._current_agent,\n                    info=self._event_info,\n                )\n            )\n\n            # Send tool output to complete the handoff\n            await self._model.send_event(\n                RealtimeModelSendToolOutput(\n                    tool_call=event,\n                    output=f\"Handed off to {self._current_agent.name}\",\n                    start_response=True,\n                )\n            )\n\n            # Send session update to model\n            await self._model.send_event(\n                RealtimeModelSendSessionUpdate(session_settings=updated_settings)\n            )\n        else:\n            raise ModelBehaviorError(f\"Tool {event.name} not found\")\n\n    @classmethod\n    def _get_new_history(\n        cls,\n        old_history: list[RealtimeItem],\n        event: RealtimeModelInputAudioTranscriptionCompletedEvent | RealtimeItem,\n    ) -&gt; list[RealtimeItem]:\n        # Merge transcript into placeholder input_audio message.\n        if isinstance(event, RealtimeModelInputAudioTranscriptionCompletedEvent):\n            new_history: list[RealtimeItem] = []\n            for item in old_history:\n                if item.item_id == event.item_id and item.type == \"message\" and item.role == \"user\":\n                    content: list[InputText | InputAudio] = []\n                    for entry in item.content:\n                        if entry.type == \"input_audio\":\n                            copied_entry = entry.model_copy(update={\"transcript\": event.transcript})\n                            content.append(copied_entry)\n                        else:\n                            content.append(entry)  # type: ignore\n                    new_history.append(\n                        item.model_copy(update={\"content\": content, \"status\": \"completed\"})\n                    )\n                else:\n                    new_history.append(item)\n            return new_history\n\n        # Otherwise it's just a new item\n        # TODO (rm) Add support for audio storage config\n\n        # If the item already exists, update it\n        existing_index = next(\n            (i for i, item in enumerate(old_history) if item.item_id == event.item_id), None\n        )\n        if existing_index is not None:\n            new_history = old_history.copy()\n            new_history[existing_index] = event\n            return new_history\n        # Otherwise, insert it after the previous_item_id if that is set\n        elif event.previous_item_id:\n            # Insert the new item after the previous item\n            previous_index = next(\n                (i for i, item in enumerate(old_history) if item.item_id == event.previous_item_id),\n                None,\n            )\n            if previous_index is not None:\n                new_history = old_history.copy()\n                new_history.insert(previous_index + 1, event)\n                return new_history\n\n        # Otherwise, add it to the end\n        return old_history + [event]\n\n    async def _run_output_guardrails(self, text: str) -&gt; bool:\n        \"\"\"Run output guardrails on the given text. Returns True if any guardrail was triggered.\"\"\"\n        output_guardrails = self._run_config.get(\"output_guardrails\", [])\n        if not output_guardrails or self._interrupted_by_guardrail:\n            return False\n\n        triggered_results = []\n\n        for guardrail in output_guardrails:\n            try:\n                result = await guardrail.run(\n                    # TODO (rm) Remove this cast, it's wrong\n                    self._context_wrapper,\n                    cast(Agent[Any], self._current_agent),\n                    text,\n                )\n                if result.output.tripwire_triggered:\n                    triggered_results.append(result)\n            except Exception:\n                # Continue with other guardrails if one fails\n                continue\n\n        if triggered_results:\n            # Mark as interrupted to prevent multiple interrupts\n            self._interrupted_by_guardrail = True\n\n            # Emit guardrail tripped event\n            await self._put_event(\n                RealtimeGuardrailTripped(\n                    guardrail_results=triggered_results,\n                    message=text,\n                    info=self._event_info,\n                )\n            )\n\n            # Interrupt the model\n            await self._model.send_event(RealtimeModelSendInterrupt())\n\n            # Send guardrail triggered message\n            guardrail_names = [result.guardrail.get_name() for result in triggered_results]\n            await self._model.send_event(\n                RealtimeModelSendUserInput(\n                    user_input=f\"guardrail triggered: {', '.join(guardrail_names)}\"\n                )\n            )\n\n            return True\n\n        return False\n\n    def _enqueue_guardrail_task(self, text: str) -&gt; None:\n        # Runs the guardrails in a separate task to avoid blocking the main loop\n\n        task = asyncio.create_task(self._run_output_guardrails(text))\n        self._guardrail_tasks.add(task)\n\n        # Add callback to remove completed tasks and handle exceptions\n        task.add_done_callback(self._on_guardrail_task_done)\n\n    def _on_guardrail_task_done(self, task: asyncio.Task[Any]) -&gt; None:\n        \"\"\"Handle completion of a guardrail task.\"\"\"\n        # Remove from tracking set\n        self._guardrail_tasks.discard(task)\n\n        # Check for exceptions and propagate as events\n        if not task.cancelled():\n            exception = task.exception()\n            if exception:\n                # Create an exception event instead of raising\n                asyncio.create_task(\n                    self._put_event(\n                        RealtimeError(\n                            info=self._event_info,\n                            error={\"message\": f\"Guardrail task failed: {str(exception)}\"},\n                        )\n                    )\n                )\n\n    def _cleanup_guardrail_tasks(self) -&gt; None:\n        for task in self._guardrail_tasks:\n            if not task.done():\n                task.cancel()\n        self._guardrail_tasks.clear()\n\n    async def _cleanup(self) -&gt; None:\n        \"\"\"Clean up all resources and mark session as closed.\"\"\"\n        # Cancel and cleanup guardrail tasks\n        self._cleanup_guardrail_tasks()\n\n        # Remove ourselves as a listener\n        self._model.remove_listener(self)\n\n        # Close the model connection\n        await self._model.close()\n\n        # Mark as closed\n        self._closed = True\n\n    async def _get_updated_model_settings_from_agent(\n        self,\n        agent: RealtimeAgent,\n    ) -&gt; RealtimeSessionModelSettings:\n        updated_settings: RealtimeSessionModelSettings = {}\n        instructions, tools, handoffs = await asyncio.gather(\n            agent.get_system_prompt(self._context_wrapper),\n            agent.get_all_tools(self._context_wrapper),\n            self._get_handoffs(agent, self._context_wrapper),\n        )\n        updated_settings[\"instructions\"] = instructions or \"\"\n        updated_settings[\"tools\"] = tools or []\n        updated_settings[\"handoffs\"] = handoffs or []\n\n        # Override with initial settings\n        initial_settings = self._model_config.get(\"initial_model_settings\", {})\n        updated_settings.update(initial_settings)\n\n        disable_tracing = self._run_config.get(\"tracing_disabled\", False)\n        if disable_tracing:\n            updated_settings[\"tracing\"] = None\n\n        return updated_settings\n\n    @classmethod\n    async def _get_handoffs(\n        cls, agent: RealtimeAgent[Any], context_wrapper: RunContextWrapper[Any]\n    ) -&gt; list[Handoff[Any, RealtimeAgent[Any]]]:\n        handoffs: list[Handoff[Any, RealtimeAgent[Any]]] = []\n        for handoff_item in agent.handoffs:\n            if isinstance(handoff_item, Handoff):\n                handoffs.append(handoff_item)\n            elif isinstance(handoff_item, RealtimeAgent):\n                handoffs.append(realtime_handoff(handoff_item))\n\n        async def _check_handoff_enabled(handoff_obj: Handoff[Any, RealtimeAgent[Any]]) -&gt; bool:\n            attr = handoff_obj.is_enabled\n            if isinstance(attr, bool):\n                return attr\n            res = attr(context_wrapper, agent)\n            if inspect.isawaitable(res):\n                return await res\n            return res\n\n        results = await asyncio.gather(*(_check_handoff_enabled(h) for h in handoffs))\n        enabled = [h for h, ok in zip(handoffs, results) if ok]\n        return enabled\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.model","title":"model  <code>property</code>","text":"<pre><code>model: RealtimeModel\n</code></pre> <p>Access the underlying model for adding listeners or other direct interaction.</p>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: RealtimeModel,\n    agent: RealtimeAgent,\n    context: TContext | None,\n    model_config: RealtimeModelConfig | None = None,\n    run_config: RealtimeRunConfig | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the session.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RealtimeModel</code> <p>The model to use.</p> required <code>agent</code> <code>RealtimeAgent</code> <p>The current agent.</p> required <code>context</code> <code>TContext | None</code> <p>The context object.</p> required <code>model_config</code> <code>RealtimeModelConfig | None</code> <p>Model configuration.</p> <code>None</code> <code>run_config</code> <code>RealtimeRunConfig | None</code> <p>Runtime configuration including guardrails.</p> <code>None</code> Source code in <code>src/agents/realtime/session.py</code> <pre><code>def __init__(\n    self,\n    model: RealtimeModel,\n    agent: RealtimeAgent,\n    context: TContext | None,\n    model_config: RealtimeModelConfig | None = None,\n    run_config: RealtimeRunConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the session.\n\n    Args:\n        model: The model to use.\n        agent: The current agent.\n        context: The context object.\n        model_config: Model configuration.\n        run_config: Runtime configuration including guardrails.\n    \"\"\"\n    self._model = model\n    self._current_agent = agent\n    self._context_wrapper = RunContextWrapper(context)\n    self._event_info = RealtimeEventInfo(context=self._context_wrapper)\n    self._history: list[RealtimeItem] = []\n    self._model_config = model_config or {}\n    self._run_config = run_config or {}\n    self._event_queue: asyncio.Queue[RealtimeSessionEvent] = asyncio.Queue()\n    self._closed = False\n    self._stored_exception: Exception | None = None\n\n    # Guardrails state tracking\n    self._interrupted_by_guardrail = False\n    self._item_transcripts: dict[str, str] = {}  # item_id -&gt; accumulated transcript\n    self._item_guardrail_run_counts: dict[str, int] = {}  # item_id -&gt; run count\n    self._debounce_text_length = self._run_config.get(\"guardrails_settings\", {}).get(\n        \"debounce_text_length\", 100\n    )\n\n    self._guardrail_tasks: set[asyncio.Task[Any]] = set()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; RealtimeSession\n</code></pre> <p>Start the session by connecting to the model. After this, you will be able to stream events from the model and send messages and audio to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def __aenter__(self) -&gt; RealtimeSession:\n    \"\"\"Start the session by connecting to the model. After this, you will be able to stream\n    events from the model and send messages and audio to the model.\n    \"\"\"\n    # Add ourselves as a listener\n    self._model.add_listener(self)\n\n    model_config = self._model_config.copy()\n    model_config[\"initial_model_settings\"] = await self._get_updated_model_settings_from_agent(\n        self._current_agent\n    )\n\n    # Connect to the model\n    await self._model.connect(model_config)\n\n    # Emit initial history update\n    await self._put_event(\n        RealtimeHistoryUpdated(\n            history=self._history,\n            info=self._event_info,\n        )\n    )\n\n    return self\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.enter","title":"enter  <code>async</code>","text":"<pre><code>enter() -&gt; RealtimeSession\n</code></pre> <p>Enter the async context manager. We strongly recommend using the async context manager pattern instead of this method. If you use this, you need to manually call <code>close()</code> when you are done.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def enter(self) -&gt; RealtimeSession:\n    \"\"\"Enter the async context manager. We strongly recommend using the async context manager\n    pattern instead of this method. If you use this, you need to manually call `close()` when\n    you are done.\n    \"\"\"\n    return await self.__aenter__()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    _exc_type: Any, _exc_val: Any, _exc_tb: Any\n) -&gt; None\n</code></pre> <p>End the session.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def __aexit__(self, _exc_type: Any, _exc_val: Any, _exc_tb: Any) -&gt; None:\n    \"\"\"End the session.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.__aiter__","title":"__aiter__  <code>async</code>","text":"<pre><code>__aiter__() -&gt; AsyncIterator[RealtimeSessionEvent]\n</code></pre> <p>Iterate over events from the session.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def __aiter__(self) -&gt; AsyncIterator[RealtimeSessionEvent]:\n    \"\"\"Iterate over events from the session.\"\"\"\n    while not self._closed:\n        try:\n            # Check if there's a stored exception to raise\n            if self._stored_exception is not None:\n                # Clean up resources before raising\n                await self._cleanup()\n                raise self._stored_exception\n\n            event = await self._event_queue.get()\n            yield event\n        except asyncio.CancelledError:\n            break\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the session.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the session.\"\"\"\n    await self._cleanup()\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.send_message","title":"send_message  <code>async</code>","text":"<pre><code>send_message(message: RealtimeUserInput) -&gt; None\n</code></pre> <p>Send a message to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def send_message(self, message: RealtimeUserInput) -&gt; None:\n    \"\"\"Send a message to the model.\"\"\"\n    await self._model.send_event(RealtimeModelSendUserInput(user_input=message))\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.send_audio","title":"send_audio  <code>async</code>","text":"<pre><code>send_audio(audio: bytes, *, commit: bool = False) -&gt; None\n</code></pre> <p>Send a raw audio chunk to the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def send_audio(self, audio: bytes, *, commit: bool = False) -&gt; None:\n    \"\"\"Send a raw audio chunk to the model.\"\"\"\n    await self._model.send_event(RealtimeModelSendAudio(audio=audio, commit=commit))\n</code></pre>"},{"location":"ref/realtime/session/#agents.realtime.session.RealtimeSession.interrupt","title":"interrupt  <code>async</code>","text":"<pre><code>interrupt() -&gt; None\n</code></pre> <p>Interrupt the model.</p> Source code in <code>src/agents/realtime/session.py</code> <pre><code>async def interrupt(self) -&gt; None:\n    \"\"\"Interrupt the model.\"\"\"\n    await self._model.send_event(RealtimeModelSendInterrupt())\n</code></pre>"},{"location":"ref/tracing/","title":"Tracing module","text":""},{"location":"ref/tracing/#agents.tracing.TracingProcessor","title":"TracingProcessor","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for processing spans.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>class TracingProcessor(abc.ABC):\n    \"\"\"Interface for processing spans.\"\"\"\n\n    @abc.abstractmethod\n    def on_trace_start(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a trace is started.\n\n        Args:\n            trace: The trace that started.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_trace_end(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a trace is finished.\n\n        Args:\n            trace: The trace that finished.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_start(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a span is started.\n\n        Args:\n            span: The span that started.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_end(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a span is finished. Should not block or raise exceptions.\n\n        Args:\n            span: The span that finished.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Called when the application stops.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def force_flush(self) -&gt; None:\n        \"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_trace_start","title":"on_trace_start  <code>abstractmethod</code>","text":"<pre><code>on_trace_start(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace is started.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace that started.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_start(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a trace is started.\n\n    Args:\n        trace: The trace that started.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_trace_end","title":"on_trace_end  <code>abstractmethod</code>","text":"<pre><code>on_trace_end(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace is finished.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace that finished.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_end(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a trace is finished.\n\n    Args:\n        trace: The trace that finished.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_span_start","title":"on_span_start  <code>abstractmethod</code>","text":"<pre><code>on_span_start(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span is started.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The span that started.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_start(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a span is started.\n\n    Args:\n        span: The span that started.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.on_span_end","title":"on_span_end  <code>abstractmethod</code>","text":"<pre><code>on_span_end(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span is finished. Should not block or raise exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The span that finished.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_end(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a span is finished. Should not block or raise exceptions.\n\n    Args:\n        span: The span that finished.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Called when the application stops.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Called when the application stops.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TracingProcessor.force_flush","title":"force_flush  <code>abstractmethod</code>","text":"<pre><code>force_flush() -&gt; None\n</code></pre> <p>Forces an immediate flush of all queued spans/traces.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef force_flush(self) -&gt; None:\n    \"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider","title":"TraceProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for creating traces and spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>class TraceProvider(ABC):\n    \"\"\"Interface for creating traces and spans.\"\"\"\n\n    @abstractmethod\n    def register_processor(self, processor: TracingProcessor) -&gt; None:\n        \"\"\"Add a processor that will receive all traces and spans.\"\"\"\n\n    @abstractmethod\n    def set_processors(self, processors: list[TracingProcessor]) -&gt; None:\n        \"\"\"Replace the list of processors with ``processors``.\"\"\"\n\n    @abstractmethod\n    def get_current_trace(self) -&gt; Trace | None:\n        \"\"\"Return the currently active trace, if any.\"\"\"\n\n    @abstractmethod\n    def get_current_span(self) -&gt; Span[Any] | None:\n        \"\"\"Return the currently active span, if any.\"\"\"\n\n    @abstractmethod\n    def set_disabled(self, disabled: bool) -&gt; None:\n        \"\"\"Enable or disable tracing globally.\"\"\"\n\n    @abstractmethod\n    def time_iso(self) -&gt; str:\n        \"\"\"Return the current time in ISO 8601 format.\"\"\"\n\n    @abstractmethod\n    def gen_trace_id(self) -&gt; str:\n        \"\"\"Generate a new trace identifier.\"\"\"\n\n    @abstractmethod\n    def gen_span_id(self) -&gt; str:\n        \"\"\"Generate a new span identifier.\"\"\"\n\n    @abstractmethod\n    def gen_group_id(self) -&gt; str:\n        \"\"\"Generate a new group identifier.\"\"\"\n\n    @abstractmethod\n    def create_trace(\n        self,\n        name: str,\n        trace_id: str | None = None,\n        group_id: str | None = None,\n        metadata: dict[str, Any] | None = None,\n        disabled: bool = False,\n    ) -&gt; Trace:\n        \"\"\"Create a new trace.\"\"\"\n\n    @abstractmethod\n    def create_span(\n        self,\n        span_data: TSpanData,\n        span_id: str | None = None,\n        parent: Trace | Span[Any] | None = None,\n        disabled: bool = False,\n    ) -&gt; Span[TSpanData]:\n        \"\"\"Create a new span.\"\"\"\n\n    @abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Clean up any resources used by the provider.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.register_processor","title":"register_processor  <code>abstractmethod</code>","text":"<pre><code>register_processor(processor: TracingProcessor) -&gt; None\n</code></pre> <p>Add a processor that will receive all traces and spans.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef register_processor(self, processor: TracingProcessor) -&gt; None:\n    \"\"\"Add a processor that will receive all traces and spans.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.set_processors","title":"set_processors  <code>abstractmethod</code>","text":"<pre><code>set_processors(processors: list[TracingProcessor]) -&gt; None\n</code></pre> <p>Replace the list of processors with <code>processors</code>.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef set_processors(self, processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"Replace the list of processors with ``processors``.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.get_current_trace","title":"get_current_trace  <code>abstractmethod</code>","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Return the currently active trace, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef get_current_trace(self) -&gt; Trace | None:\n    \"\"\"Return the currently active trace, if any.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.get_current_span","title":"get_current_span  <code>abstractmethod</code>","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Return the currently active span, if any.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef get_current_span(self) -&gt; Span[Any] | None:\n    \"\"\"Return the currently active span, if any.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.set_disabled","title":"set_disabled  <code>abstractmethod</code>","text":"<pre><code>set_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Enable or disable tracing globally.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef set_disabled(self, disabled: bool) -&gt; None:\n    \"\"\"Enable or disable tracing globally.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.time_iso","title":"time_iso  <code>abstractmethod</code>","text":"<pre><code>time_iso() -&gt; str\n</code></pre> <p>Return the current time in ISO 8601 format.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef time_iso(self) -&gt; str:\n    \"\"\"Return the current time in ISO 8601 format.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.gen_trace_id","title":"gen_trace_id  <code>abstractmethod</code>","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_trace_id(self) -&gt; str:\n    \"\"\"Generate a new trace identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.gen_span_id","title":"gen_span_id  <code>abstractmethod</code>","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_span_id(self) -&gt; str:\n    \"\"\"Generate a new span identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.gen_group_id","title":"gen_group_id  <code>abstractmethod</code>","text":"<pre><code>gen_group_id() -&gt; str\n</code></pre> <p>Generate a new group identifier.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef gen_group_id(self) -&gt; str:\n    \"\"\"Generate a new group identifier.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.create_trace","title":"create_trace  <code>abstractmethod</code>","text":"<pre><code>create_trace(\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n) -&gt; Trace\n</code></pre> <p>Create a new trace.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef create_trace(\n    self,\n    name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n) -&gt; Trace:\n    \"\"\"Create a new trace.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.create_span","title":"create_span  <code>abstractmethod</code>","text":"<pre><code>create_span(\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]\n</code></pre> <p>Create a new span.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef create_span(\n    self,\n    span_data: TSpanData,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TSpanData]:\n    \"\"\"Create a new span.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TraceProvider.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Clean up any resources used by the provider.</p> Source code in <code>src/agents/tracing/provider.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Clean up any resources used by the provider.\"\"\"\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.AgentSpanData","title":"AgentSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an Agent Span in the trace. Includes name, handoffs, tools, and output type.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class AgentSpanData(SpanData):\n    \"\"\"\n    Represents an Agent Span in the trace.\n    Includes name, handoffs, tools, and output type.\n    \"\"\"\n\n    __slots__ = (\"name\", \"handoffs\", \"tools\", \"output_type\")\n\n    def __init__(\n        self,\n        name: str,\n        handoffs: list[str] | None = None,\n        tools: list[str] | None = None,\n        output_type: str | None = None,\n    ):\n        self.name = name\n        self.handoffs: list[str] | None = handoffs\n        self.tools: list[str] | None = tools\n        self.output_type: str | None = output_type\n\n    @property\n    def type(self) -&gt; str:\n        return \"agent\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"handoffs\": self.handoffs,\n            \"tools\": self.tools,\n            \"output_type\": self.output_type,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.CustomSpanData","title":"CustomSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Custom Span in the trace. Includes name and data property bag.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class CustomSpanData(SpanData):\n    \"\"\"\n    Represents a Custom Span in the trace.\n    Includes name and data property bag.\n    \"\"\"\n\n    __slots__ = (\"name\", \"data\")\n\n    def __init__(self, name: str, data: dict[str, Any]):\n        self.name = name\n        self.data = data\n\n    @property\n    def type(self) -&gt; str:\n        return \"custom\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"data\": self.data,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.FunctionSpanData","title":"FunctionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Function Span in the trace. Includes input, output and MCP data (if applicable).</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class FunctionSpanData(SpanData):\n    \"\"\"\n    Represents a Function Span in the trace.\n    Includes input, output and MCP data (if applicable).\n    \"\"\"\n\n    __slots__ = (\"name\", \"input\", \"output\", \"mcp_data\")\n\n    def __init__(\n        self,\n        name: str,\n        input: str | None,\n        output: Any | None,\n        mcp_data: dict[str, Any] | None = None,\n    ):\n        self.name = name\n        self.input = input\n        self.output = output\n        self.mcp_data = mcp_data\n\n    @property\n    def type(self) -&gt; str:\n        return \"function\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"input\": self.input,\n            \"output\": str(self.output) if self.output else None,\n            \"mcp_data\": self.mcp_data,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.GenerationSpanData","title":"GenerationSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Generation Span in the trace. Includes input, output, model, model configuration, and usage.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GenerationSpanData(SpanData):\n    \"\"\"\n    Represents a Generation Span in the trace.\n    Includes input, output, model, model configuration, and usage.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n        \"usage\",\n    )\n\n    def __init__(\n        self,\n        input: Sequence[Mapping[str, Any]] | None = None,\n        output: Sequence[Mapping[str, Any]] | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        usage: dict[str, Any] | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n        self.usage = usage\n\n    @property\n    def type(self) -&gt; str:\n        return \"generation\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"usage\": self.usage,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.GuardrailSpanData","title":"GuardrailSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Guardrail Span in the trace. Includes name and triggered status.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GuardrailSpanData(SpanData):\n    \"\"\"\n    Represents a Guardrail Span in the trace.\n    Includes name and triggered status.\n    \"\"\"\n\n    __slots__ = (\"name\", \"triggered\")\n\n    def __init__(self, name: str, triggered: bool = False):\n        self.name = name\n        self.triggered = triggered\n\n    @property\n    def type(self) -&gt; str:\n        return \"guardrail\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"triggered\": self.triggered,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.HandoffSpanData","title":"HandoffSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Handoff Span in the trace. Includes source and destination agents.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class HandoffSpanData(SpanData):\n    \"\"\"\n    Represents a Handoff Span in the trace.\n    Includes source and destination agents.\n    \"\"\"\n\n    __slots__ = (\"from_agent\", \"to_agent\")\n\n    def __init__(self, from_agent: str | None, to_agent: str | None):\n        self.from_agent = from_agent\n        self.to_agent = to_agent\n\n    @property\n    def type(self) -&gt; str:\n        return \"handoff\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"from_agent\": self.from_agent,\n            \"to_agent\": self.to_agent,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.MCPListToolsSpanData","title":"MCPListToolsSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an MCP List Tools Span in the trace. Includes server and result.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class MCPListToolsSpanData(SpanData):\n    \"\"\"\n    Represents an MCP List Tools Span in the trace.\n    Includes server and result.\n    \"\"\"\n\n    __slots__ = (\n        \"server\",\n        \"result\",\n    )\n\n    def __init__(self, server: str | None = None, result: list[str] | None = None):\n        self.server = server\n        self.result = result\n\n    @property\n    def type(self) -&gt; str:\n        return \"mcp_tools\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"server\": self.server,\n            \"result\": self.result,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.ResponseSpanData","title":"ResponseSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Response Span in the trace. Includes response and input.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class ResponseSpanData(SpanData):\n    \"\"\"\n    Represents a Response Span in the trace.\n    Includes response and input.\n    \"\"\"\n\n    __slots__ = (\"response\", \"input\")\n\n    def __init__(\n        self,\n        response: Response | None = None,\n        input: str | list[ResponseInputItemParam] | None = None,\n    ) -&gt; None:\n        self.response = response\n        # This is not used by the OpenAI trace processors, but is useful for other tracing\n        # processor implementations\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"response\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"response_id\": self.response.id if self.response else None,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpanData","title":"SpanData","text":"<p>               Bases: <code>ABC</code></p> <p>Represents span data in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpanData(abc.ABC):\n    \"\"\"\n    Represents span data in the trace.\n    \"\"\"\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any]:\n        \"\"\"Export the span data as a dictionary.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def type(self) -&gt; str:\n        \"\"\"Return the type of the span.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpanData.type","title":"type  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>Return the type of the span.</p>"},{"location":"ref/tracing/#agents.tracing.SpanData.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any]\n</code></pre> <p>Export the span data as a dictionary.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any]:\n    \"\"\"Export the span data as a dictionary.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpeechGroupSpanData","title":"SpeechGroupSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Group Span in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechGroupSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Group Span in the trace.\n    \"\"\"\n\n    __slots__ = \"input\"\n\n    def __init__(\n        self,\n        input: str | None = None,\n    ):\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech_group\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.SpeechSpanData","title":"SpeechSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Span in the trace. Includes input, output, model, model configuration, and first content timestamp.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Span in the trace.\n    Includes input, output, model, model configuration, and first content timestamp.\n    \"\"\"\n\n    __slots__ = (\"input\", \"output\", \"model\", \"model_config\", \"first_content_at\")\n\n    def __init__(\n        self,\n        input: str | None = None,\n        output: str | None = None,\n        output_format: str | None = \"pcm\",\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        first_content_at: str | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.output_format = output_format\n        self.model = model\n        self.model_config = model_config\n        self.first_content_at = first_content_at\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": {\n                \"data\": self.output or \"\",\n                \"format\": self.output_format,\n            },\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"first_content_at\": self.first_content_at,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.TranscriptionSpanData","title":"TranscriptionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Transcription Span in the trace. Includes input, output, model, and model configuration.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class TranscriptionSpanData(SpanData):\n    \"\"\"\n    Represents a Transcription Span in the trace.\n    Includes input, output, model, and model configuration.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n    )\n\n    def __init__(\n        self,\n        input: str | None = None,\n        input_format: str | None = \"pcm\",\n        output: str | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n    ):\n        self.input = input\n        self.input_format = input_format\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n\n    @property\n    def type(self) -&gt; str:\n        return \"transcription\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": {\n                \"data\": self.input or \"\",\n                \"format\": self.input_format,\n            },\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n        }\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Span","title":"Span","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TSpanData]</code></p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class Span(abc.ABC, Generic[TSpanData]):\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_id(self) -&gt; str:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_data(self) -&gt; TSpanData:\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"\n        Start the span.\n\n        Args:\n            mark_as_current: If true, the span will be marked as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False) -&gt; None:\n        \"\"\"\n        Finish the span.\n\n        Args:\n            reset_current: If true, the span will be reset as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Span[TSpanData]:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def parent_id(self) -&gt; str | None:\n        pass\n\n    @abc.abstractmethod\n    def set_error(self, error: SpanError) -&gt; None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def error(self) -&gt; SpanError | None:\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def started_at(self) -&gt; str | None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def ended_at(self) -&gt; str | None:\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Span.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the span.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, the span will be marked as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"\n    Start the span.\n\n    Args:\n        mark_as_current: If true, the span will be marked as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Span.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False) -&gt; None\n</code></pre> <p>Finish the span.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, the span will be reset as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False) -&gt; None:\n    \"\"\"\n    Finish the span.\n\n    Args:\n        reset_current: If true, the span will be reset as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace","title":"Trace","text":"<p>A trace is the root level object that tracing creates. It represents a logical \"workflow\".</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class Trace:\n    \"\"\"\n    A trace is the root level object that tracing creates. It represents a logical \"workflow\".\n    \"\"\"\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Trace:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"\n        Start the trace.\n\n        Args:\n            mark_as_current: If true, the trace will be marked as the current trace.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False):\n        \"\"\"\n        Finish the trace.\n\n        Args:\n            reset_current: If true, the trace will be reset as the current trace.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        \"\"\"\n        The trace ID.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"\n        The name of the workflow being traced.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Export the trace as a dictionary.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.trace_id","title":"trace_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>The trace ID.</p>"},{"location":"ref/tracing/#agents.tracing.Trace.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the workflow being traced.</p>"},{"location":"ref/tracing/#agents.tracing.Trace.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the trace.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, the trace will be marked as the current trace.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"\n    Start the trace.\n\n    Args:\n        mark_as_current: If true, the trace will be marked as the current trace.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False)\n</code></pre> <p>Finish the trace.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, the trace will be reset as the current trace.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False):\n    \"\"\"\n    Finish the trace.\n\n    Args:\n        reset_current: If true, the trace will be reset as the current trace.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.Trace.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any] | None\n</code></pre> <p>Export the trace as a dictionary.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Export the trace as a dictionary.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.agent_span","title":"agent_span","text":"<pre><code>agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]\n</code></pre> <p>Create a new agent span. The span will not be started automatically, you should either do <code>with agent_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the agent.</p> required <code>handoffs</code> <code>list[str] | None</code> <p>Optional list of agent names to which this agent could hand off control.</p> <code>None</code> <code>tools</code> <code>list[str] | None</code> <p>Optional list of tool names available to this agent.</p> <code>None</code> <code>output_type</code> <code>str | None</code> <p>Optional name of the output type produced by the agent.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[AgentSpanData]</code> <p>The newly created agent span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]:\n    \"\"\"Create a new agent span. The span will not be started automatically, you should either do\n    `with agent_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the agent.\n        handoffs: Optional list of agent names to which this agent could hand off control.\n        tools: Optional list of tool names available to this agent.\n        output_type: Optional name of the output type produced by the agent.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created agent span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=AgentSpanData(name=name, handoffs=handoffs, tools=tools, output_type=output_type),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.custom_span","title":"custom_span","text":"<pre><code>custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]\n</code></pre> <p>Create a new custom span, to which you can add your own metadata. The span will not be started automatically, you should either do <code>with custom_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom span.</p> required <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary structured data to associate with the span.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[CustomSpanData]</code> <p>The newly created custom span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]:\n    \"\"\"Create a new custom span, to which you can add your own metadata. The span will not be\n    started automatically, you should either do `with custom_span() ...` or call\n    `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the custom span.\n        data: Arbitrary structured data to associate with the span.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created custom span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=CustomSpanData(name=name, data=data or {}),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.function_span","title":"function_span","text":"<pre><code>function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]\n</code></pre> <p>Create a new function span. The span will not be started automatically, you should either do <code>with function_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function.</p> required <code>input</code> <code>str | None</code> <p>The input to the function.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The output of the function.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[FunctionSpanData]</code> <p>The newly created function span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]:\n    \"\"\"Create a new function span. The span will not be started automatically, you should either do\n    `with function_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the function.\n        input: The input to the function.\n        output: The output of the function.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created function span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=FunctionSpanData(name=name, input=input, output=output),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.generation_span","title":"generation_span","text":"<pre><code>generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]\n</code></pre> <p>Create a new generation span. The span will not be started automatically, you should either do <code>with generation_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>This span captures the details of a model generation, including the input message sequence, any generated outputs, the model name and configuration, and usage data. If you only need to capture a model response identifier, use <code>response_span()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of input messages sent to the model.</p> <code>None</code> <code>output</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of output messages received from the model.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>The model identifier used for the generation.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>usage</code> <code>dict[str, Any] | None</code> <p>A dictionary of usage information (input tokens, output tokens, etc.).</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[GenerationSpanData]</code> <p>The newly created generation span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]:\n    \"\"\"Create a new generation span. The span will not be started automatically, you should either\n    do `with generation_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    This span captures the details of a model generation, including the\n    input message sequence, any generated outputs, the model name and\n    configuration, and usage data. If you only need to capture a model\n    response identifier, use `response_span()` instead.\n\n    Args:\n        input: The sequence of input messages sent to the model.\n        output: The sequence of output messages received from the model.\n        model: The model identifier used for the generation.\n        model_config: The model configuration (hyperparameters) used.\n        usage: A dictionary of usage information (input tokens, output tokens, etc.).\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created generation span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GenerationSpanData(\n            input=input,\n            output=output,\n            model=model,\n            model_config=model_config,\n            usage=usage,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.get_current_span","title":"get_current_span","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Returns the currently active span, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_span() -&gt; Span[Any] | None:\n    \"\"\"Returns the currently active span, if present.\"\"\"\n    return get_trace_provider().get_current_span()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.get_current_trace","title":"get_current_trace","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Returns the currently active trace, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_trace() -&gt; Trace | None:\n    \"\"\"Returns the currently active trace, if present.\"\"\"\n    return get_trace_provider().get_current_trace()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.guardrail_span","title":"guardrail_span","text":"<pre><code>guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]\n</code></pre> <p>Create a new guardrail span. The span will not be started automatically, you should either do <code>with guardrail_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the guardrail.</p> required <code>triggered</code> <code>bool</code> <p>Whether the guardrail was triggered.</p> <code>False</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]:\n    \"\"\"Create a new guardrail span. The span will not be started automatically, you should either\n    do `with guardrail_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the guardrail.\n        triggered: Whether the guardrail was triggered.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GuardrailSpanData(name=name, triggered=triggered),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.handoff_span","title":"handoff_span","text":"<pre><code>handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]\n</code></pre> <p>Create a new handoff span. The span will not be started automatically, you should either do <code>with handoff_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>from_agent</code> <code>str | None</code> <p>The name of the agent that is handing off.</p> <code>None</code> <code>to_agent</code> <code>str | None</code> <p>The name of the agent that is receiving the handoff.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[HandoffSpanData]</code> <p>The newly created handoff span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]:\n    \"\"\"Create a new handoff span. The span will not be started automatically, you should either do\n    `with handoff_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        from_agent: The name of the agent that is handing off.\n        to_agent: The name of the agent that is receiving the handoff.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created handoff span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=HandoffSpanData(from_agent=from_agent, to_agent=to_agent),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.mcp_tools_span","title":"mcp_tools_span","text":"<pre><code>mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]\n</code></pre> <p>Create a new MCP list tools span. The span will not be started automatically, you should either do <code>with mcp_tools_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str | None</code> <p>The name of the MCP server.</p> <code>None</code> <code>result</code> <code>list[str] | None</code> <p>The result of the MCP list tools call.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]:\n    \"\"\"Create a new MCP list tools span. The span will not be started automatically, you should\n    either do `with mcp_tools_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        server: The name of the MCP server.\n        result: The result of the MCP list tools call.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=MCPListToolsSpanData(server=server, result=result),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.response_span","title":"response_span","text":"<pre><code>response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]\n</code></pre> <p>Create a new response span. The span will not be started automatically, you should either do <code>with response_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response | None</code> <p>The OpenAI Response object.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]:\n    \"\"\"Create a new response span. The span will not be started automatically, you should either do\n    `with response_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        response: The OpenAI Response object.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=ResponseSpanData(response=response),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.speech_group_span","title":"speech_group_span","text":"<pre><code>speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]\n</code></pre> <p>Create a new speech group span. The span will not be started automatically, you should either do <code>with speech_group_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str | None</code> <p>The input text used for the speech request.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]:\n    \"\"\"Create a new speech group span. The span will not be started automatically, you should\n    either do `with speech_group_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        input: The input text used for the speech request.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechGroupSpanData(input=input),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.speech_span","title":"speech_span","text":"<pre><code>speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]\n</code></pre> <p>Create a new speech span. The span will not be started automatically, you should either do <code>with speech_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the text-to-speech.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The text input of the text-to-speech.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.</p> <code>None</code> <code>output_format</code> <code>str | None</code> <p>The format of the audio output (defaults to \"pcm\").</p> <code>'pcm'</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>first_content_at</code> <code>str | None</code> <p>The time of the first byte of the audio output.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]:\n    \"\"\"Create a new speech span. The span will not be started automatically, you should either do\n    `with speech_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the text-to-speech.\n        input: The text input of the text-to-speech.\n        output: The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\n        output_format: The format of the audio output (defaults to \"pcm\").\n        model_config: The model configuration (hyperparameters) used.\n        first_content_at: The time of the first byte of the audio output.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechSpanData(\n            model=model,\n            input=input,\n            output=output,\n            output_format=output_format,\n            model_config=model_config,\n            first_content_at=first_content_at,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.trace","title":"trace","text":"<pre><code>trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n) -&gt; Trace\n</code></pre> <p>Create a new trace. The trace will not be started automatically; you should either use it as a context manager (<code>with trace(...):</code>) or call <code>trace.start()</code> + <code>trace.finish()</code> manually.</p> <p>In addition to the workflow name and optional grouping identifier, you can provide an arbitrary metadata dictionary to attach additional user-defined information to the trace.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_name</code> <code>str</code> <p>The name of the logical app or workflow. For example, you might provide \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.</p> required <code>trace_id</code> <code>str | None</code> <p>The ID of the trace. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_trace_id()</code> to generate a trace ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>group_id</code> <code>str | None</code> <p>Optional grouping identifier to link multiple traces from the same conversation or process. For instance, you might use a chat thread ID.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of additional metadata to attach to the trace.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Trace but the Trace will not be recorded. This will not be checked if there's an existing trace and <code>even_if_trace_running</code> is True.</p> <code>False</code> <p>Returns:</p> Type Description <code>Trace</code> <p>The newly created trace object.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n) -&gt; Trace:\n    \"\"\"\n    Create a new trace. The trace will not be started automatically; you should either use\n    it as a context manager (`with trace(...):`) or call `trace.start()` + `trace.finish()`\n    manually.\n\n    In addition to the workflow name and optional grouping identifier, you can provide\n    an arbitrary metadata dictionary to attach additional user-defined information to\n    the trace.\n\n    Args:\n        workflow_name: The name of the logical app or workflow. For example, you might provide\n            \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\n        trace_id: The ID of the trace. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_trace_id()` to generate a trace ID, to guarantee that IDs are\n            correctly formatted.\n        group_id: Optional grouping identifier to link multiple traces from the same conversation\n            or process. For instance, you might use a chat thread ID.\n        metadata: Optional dictionary of additional metadata to attach to the trace.\n        disabled: If True, we will return a Trace but the Trace will not be recorded. This will\n            not be checked if there's an existing trace and `even_if_trace_running` is True.\n\n    Returns:\n        The newly created trace object.\n    \"\"\"\n    current_trace = get_trace_provider().get_current_trace()\n    if current_trace:\n        logger.warning(\n            \"Trace already exists. Creating a new trace, but this is probably a mistake.\"\n        )\n\n    return get_trace_provider().create_trace(\n        name=workflow_name,\n        trace_id=trace_id,\n        group_id=group_id,\n        metadata=metadata,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.transcription_span","title":"transcription_span","text":"<pre><code>transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]\n</code></pre> <p>Create a new transcription span. The span will not be started automatically, you should either do <code>with transcription_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the speech-to-text.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The audio input of the speech-to-text transcription, as a base64 encoded string of audio bytes.</p> <code>None</code> <code>input_format</code> <code>str | None</code> <p>The format of the audio input (defaults to \"pcm\").</p> <code>'pcm'</code> <code>output</code> <code>str | None</code> <p>The output of the speech-to-text transcription.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[TranscriptionSpanData]</code> <p>The newly created speech-to-text span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]:\n    \"\"\"Create a new transcription span. The span will not be started automatically, you should\n    either do `with transcription_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the speech-to-text.\n        input: The audio input of the speech-to-text transcription, as a base64 encoded string of\n            audio bytes.\n        input_format: The format of the audio input (defaults to \"pcm\").\n        output: The output of the speech-to-text transcription.\n        model_config: The model configuration (hyperparameters) used.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created speech-to-text span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=TranscriptionSpanData(\n            input=input,\n            input_format=input_format,\n            output=output,\n            model=model,\n            model_config=model_config,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.get_trace_provider","title":"get_trace_provider","text":"<pre><code>get_trace_provider() -&gt; TraceProvider\n</code></pre> <p>Get the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def get_trace_provider() -&gt; TraceProvider:\n    \"\"\"Get the global trace provider used by tracing utilities.\"\"\"\n    if GLOBAL_TRACE_PROVIDER is None:\n        raise RuntimeError(\"Trace provider not set\")\n    return GLOBAL_TRACE_PROVIDER\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_trace_provider","title":"set_trace_provider","text":"<pre><code>set_trace_provider(provider: TraceProvider) -&gt; None\n</code></pre> <p>Set the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def set_trace_provider(provider: TraceProvider) -&gt; None:\n    \"\"\"Set the global trace provider used by tracing utilities.\"\"\"\n    global GLOBAL_TRACE_PROVIDER\n    GLOBAL_TRACE_PROVIDER = provider\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.gen_span_id","title":"gen_span_id","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_span_id() -&gt; str:\n    \"\"\"Generate a new span ID.\"\"\"\n    return get_trace_provider().gen_span_id()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.gen_trace_id","title":"gen_trace_id","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_trace_id() -&gt; str:\n    \"\"\"Generate a new trace ID.\"\"\"\n    return get_trace_provider().gen_trace_id()\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.add_trace_processor","title":"add_trace_processor","text":"<pre><code>add_trace_processor(\n    span_processor: TracingProcessor,\n) -&gt; None\n</code></pre> <p>Adds a new trace processor. This processor will receive all traces/spans.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def add_trace_processor(span_processor: TracingProcessor) -&gt; None:\n    \"\"\"\n    Adds a new trace processor. This processor will receive all traces/spans.\n    \"\"\"\n    get_trace_provider().register_processor(span_processor)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_trace_processors","title":"set_trace_processors","text":"<pre><code>set_trace_processors(\n    processors: list[TracingProcessor],\n) -&gt; None\n</code></pre> <p>Set the list of trace processors. This will replace the current list of processors.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_trace_processors(processors: list[TracingProcessor]) -&gt; None:\n    \"\"\"\n    Set the list of trace processors. This will replace the current list of processors.\n    \"\"\"\n    get_trace_provider().set_processors(processors)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_tracing_disabled","title":"set_tracing_disabled","text":"<pre><code>set_tracing_disabled(disabled: bool) -&gt; None\n</code></pre> <p>Set whether tracing is globally disabled.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_disabled(disabled: bool) -&gt; None:\n    \"\"\"\n    Set whether tracing is globally disabled.\n    \"\"\"\n    get_trace_provider().set_disabled(disabled)\n</code></pre>"},{"location":"ref/tracing/#agents.tracing.set_tracing_export_api_key","title":"set_tracing_export_api_key","text":"<pre><code>set_tracing_export_api_key(api_key: str) -&gt; None\n</code></pre> <p>Set the OpenAI API key for the backend exporter.</p> Source code in <code>src/agents/tracing/__init__.py</code> <pre><code>def set_tracing_export_api_key(api_key: str) -&gt; None:\n    \"\"\"\n    Set the OpenAI API key for the backend exporter.\n    \"\"\"\n    default_exporter().set_api_key(api_key)\n</code></pre>"},{"location":"ref/tracing/create/","title":"<code>Creating traces/spans</code>","text":""},{"location":"ref/tracing/create/#agents.tracing.create.trace","title":"trace","text":"<pre><code>trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n) -&gt; Trace\n</code></pre> <p>Create a new trace. The trace will not be started automatically; you should either use it as a context manager (<code>with trace(...):</code>) or call <code>trace.start()</code> + <code>trace.finish()</code> manually.</p> <p>In addition to the workflow name and optional grouping identifier, you can provide an arbitrary metadata dictionary to attach additional user-defined information to the trace.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_name</code> <code>str</code> <p>The name of the logical app or workflow. For example, you might provide \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.</p> required <code>trace_id</code> <code>str | None</code> <p>The ID of the trace. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_trace_id()</code> to generate a trace ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>group_id</code> <code>str | None</code> <p>Optional grouping identifier to link multiple traces from the same conversation or process. For instance, you might use a chat thread ID.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of additional metadata to attach to the trace.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Trace but the Trace will not be recorded. This will not be checked if there's an existing trace and <code>even_if_trace_running</code> is True.</p> <code>False</code> <p>Returns:</p> Type Description <code>Trace</code> <p>The newly created trace object.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def trace(\n    workflow_name: str,\n    trace_id: str | None = None,\n    group_id: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    disabled: bool = False,\n) -&gt; Trace:\n    \"\"\"\n    Create a new trace. The trace will not be started automatically; you should either use\n    it as a context manager (`with trace(...):`) or call `trace.start()` + `trace.finish()`\n    manually.\n\n    In addition to the workflow name and optional grouping identifier, you can provide\n    an arbitrary metadata dictionary to attach additional user-defined information to\n    the trace.\n\n    Args:\n        workflow_name: The name of the logical app or workflow. For example, you might provide\n            \"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\n        trace_id: The ID of the trace. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_trace_id()` to generate a trace ID, to guarantee that IDs are\n            correctly formatted.\n        group_id: Optional grouping identifier to link multiple traces from the same conversation\n            or process. For instance, you might use a chat thread ID.\n        metadata: Optional dictionary of additional metadata to attach to the trace.\n        disabled: If True, we will return a Trace but the Trace will not be recorded. This will\n            not be checked if there's an existing trace and `even_if_trace_running` is True.\n\n    Returns:\n        The newly created trace object.\n    \"\"\"\n    current_trace = get_trace_provider().get_current_trace()\n    if current_trace:\n        logger.warning(\n            \"Trace already exists. Creating a new trace, but this is probably a mistake.\"\n        )\n\n    return get_trace_provider().create_trace(\n        name=workflow_name,\n        trace_id=trace_id,\n        group_id=group_id,\n        metadata=metadata,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.get_current_trace","title":"get_current_trace","text":"<pre><code>get_current_trace() -&gt; Trace | None\n</code></pre> <p>Returns the currently active trace, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_trace() -&gt; Trace | None:\n    \"\"\"Returns the currently active trace, if present.\"\"\"\n    return get_trace_provider().get_current_trace()\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.get_current_span","title":"get_current_span","text":"<pre><code>get_current_span() -&gt; Span[Any] | None\n</code></pre> <p>Returns the currently active span, if present.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def get_current_span() -&gt; Span[Any] | None:\n    \"\"\"Returns the currently active span, if present.\"\"\"\n    return get_trace_provider().get_current_span()\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.agent_span","title":"agent_span","text":"<pre><code>agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]\n</code></pre> <p>Create a new agent span. The span will not be started automatically, you should either do <code>with agent_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the agent.</p> required <code>handoffs</code> <code>list[str] | None</code> <p>Optional list of agent names to which this agent could hand off control.</p> <code>None</code> <code>tools</code> <code>list[str] | None</code> <p>Optional list of tool names available to this agent.</p> <code>None</code> <code>output_type</code> <code>str | None</code> <p>Optional name of the output type produced by the agent.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[AgentSpanData]</code> <p>The newly created agent span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def agent_span(\n    name: str,\n    handoffs: list[str] | None = None,\n    tools: list[str] | None = None,\n    output_type: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[AgentSpanData]:\n    \"\"\"Create a new agent span. The span will not be started automatically, you should either do\n    `with agent_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the agent.\n        handoffs: Optional list of agent names to which this agent could hand off control.\n        tools: Optional list of tool names available to this agent.\n        output_type: Optional name of the output type produced by the agent.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created agent span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=AgentSpanData(name=name, handoffs=handoffs, tools=tools, output_type=output_type),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.function_span","title":"function_span","text":"<pre><code>function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]\n</code></pre> <p>Create a new function span. The span will not be started automatically, you should either do <code>with function_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function.</p> required <code>input</code> <code>str | None</code> <p>The input to the function.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The output of the function.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[FunctionSpanData]</code> <p>The newly created function span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def function_span(\n    name: str,\n    input: str | None = None,\n    output: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[FunctionSpanData]:\n    \"\"\"Create a new function span. The span will not be started automatically, you should either do\n    `with function_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the function.\n        input: The input to the function.\n        output: The output of the function.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created function span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=FunctionSpanData(name=name, input=input, output=output),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.generation_span","title":"generation_span","text":"<pre><code>generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]\n</code></pre> <p>Create a new generation span. The span will not be started automatically, you should either do <code>with generation_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>This span captures the details of a model generation, including the input message sequence, any generated outputs, the model name and configuration, and usage data. If you only need to capture a model response identifier, use <code>response_span()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of input messages sent to the model.</p> <code>None</code> <code>output</code> <code>Sequence[Mapping[str, Any]] | None</code> <p>The sequence of output messages received from the model.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>The model identifier used for the generation.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>usage</code> <code>dict[str, Any] | None</code> <p>A dictionary of usage information (input tokens, output tokens, etc.).</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[GenerationSpanData]</code> <p>The newly created generation span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def generation_span(\n    input: Sequence[Mapping[str, Any]] | None = None,\n    output: Sequence[Mapping[str, Any]] | None = None,\n    model: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    usage: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GenerationSpanData]:\n    \"\"\"Create a new generation span. The span will not be started automatically, you should either\n    do `with generation_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    This span captures the details of a model generation, including the\n    input message sequence, any generated outputs, the model name and\n    configuration, and usage data. If you only need to capture a model\n    response identifier, use `response_span()` instead.\n\n    Args:\n        input: The sequence of input messages sent to the model.\n        output: The sequence of output messages received from the model.\n        model: The model identifier used for the generation.\n        model_config: The model configuration (hyperparameters) used.\n        usage: A dictionary of usage information (input tokens, output tokens, etc.).\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created generation span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GenerationSpanData(\n            input=input,\n            output=output,\n            model=model,\n            model_config=model_config,\n            usage=usage,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.response_span","title":"response_span","text":"<pre><code>response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]\n</code></pre> <p>Create a new response span. The span will not be started automatically, you should either do <code>with response_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response | None</code> <p>The OpenAI Response object.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def response_span(\n    response: Response | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[ResponseSpanData]:\n    \"\"\"Create a new response span. The span will not be started automatically, you should either do\n    `with response_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        response: The OpenAI Response object.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=ResponseSpanData(response=response),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.handoff_span","title":"handoff_span","text":"<pre><code>handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]\n</code></pre> <p>Create a new handoff span. The span will not be started automatically, you should either do <code>with handoff_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>from_agent</code> <code>str | None</code> <p>The name of the agent that is handing off.</p> <code>None</code> <code>to_agent</code> <code>str | None</code> <p>The name of the agent that is receiving the handoff.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[HandoffSpanData]</code> <p>The newly created handoff span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def handoff_span(\n    from_agent: str | None = None,\n    to_agent: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[HandoffSpanData]:\n    \"\"\"Create a new handoff span. The span will not be started automatically, you should either do\n    `with handoff_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        from_agent: The name of the agent that is handing off.\n        to_agent: The name of the agent that is receiving the handoff.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created handoff span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=HandoffSpanData(from_agent=from_agent, to_agent=to_agent),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.custom_span","title":"custom_span","text":"<pre><code>custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]\n</code></pre> <p>Create a new custom span, to which you can add your own metadata. The span will not be started automatically, you should either do <code>with custom_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom span.</p> required <code>data</code> <code>dict[str, Any] | None</code> <p>Arbitrary structured data to associate with the span.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[CustomSpanData]</code> <p>The newly created custom span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def custom_span(\n    name: str,\n    data: dict[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[CustomSpanData]:\n    \"\"\"Create a new custom span, to which you can add your own metadata. The span will not be\n    started automatically, you should either do `with custom_span() ...` or call\n    `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the custom span.\n        data: Arbitrary structured data to associate with the span.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created custom span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=CustomSpanData(name=name, data=data or {}),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.guardrail_span","title":"guardrail_span","text":"<pre><code>guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]\n</code></pre> <p>Create a new guardrail span. The span will not be started automatically, you should either do <code>with guardrail_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the guardrail.</p> required <code>triggered</code> <code>bool</code> <p>Whether the guardrail was triggered.</p> <code>False</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def guardrail_span(\n    name: str,\n    triggered: bool = False,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[GuardrailSpanData]:\n    \"\"\"Create a new guardrail span. The span will not be started automatically, you should either\n    do `with guardrail_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        name: The name of the guardrail.\n        triggered: Whether the guardrail was triggered.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=GuardrailSpanData(name=name, triggered=triggered),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.transcription_span","title":"transcription_span","text":"<pre><code>transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]\n</code></pre> <p>Create a new transcription span. The span will not be started automatically, you should either do <code>with transcription_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the speech-to-text.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The audio input of the speech-to-text transcription, as a base64 encoded string of audio bytes.</p> <code>None</code> <code>input_format</code> <code>str | None</code> <p>The format of the audio input (defaults to \"pcm\").</p> <code>'pcm'</code> <code>output</code> <code>str | None</code> <p>The output of the speech-to-text transcription.</p> <code>None</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Span[TranscriptionSpanData]</code> <p>The newly created speech-to-text span.</p> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def transcription_span(\n    model: str | None = None,\n    input: str | None = None,\n    input_format: str | None = \"pcm\",\n    output: str | None = None,\n    model_config: Mapping[str, Any] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[TranscriptionSpanData]:\n    \"\"\"Create a new transcription span. The span will not be started automatically, you should\n    either do `with transcription_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the speech-to-text.\n        input: The audio input of the speech-to-text transcription, as a base64 encoded string of\n            audio bytes.\n        input_format: The format of the audio input (defaults to \"pcm\").\n        output: The output of the speech-to-text transcription.\n        model_config: The model configuration (hyperparameters) used.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n\n    Returns:\n        The newly created speech-to-text span.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=TranscriptionSpanData(\n            input=input,\n            input_format=input_format,\n            output=output,\n            model=model,\n            model_config=model_config,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.speech_span","title":"speech_span","text":"<pre><code>speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]\n</code></pre> <p>Create a new speech span. The span will not be started automatically, you should either do <code>with speech_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The name of the model used for the text-to-speech.</p> <code>None</code> <code>input</code> <code>str | None</code> <p>The text input of the text-to-speech.</p> <code>None</code> <code>output</code> <code>str | None</code> <p>The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.</p> <code>None</code> <code>output_format</code> <code>str | None</code> <p>The format of the audio output (defaults to \"pcm\").</p> <code>'pcm'</code> <code>model_config</code> <code>Mapping[str, Any] | None</code> <p>The model configuration (hyperparameters) used.</p> <code>None</code> <code>first_content_at</code> <code>str | None</code> <p>The time of the first byte of the audio output.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_span(\n    model: str | None = None,\n    input: str | None = None,\n    output: str | None = None,\n    output_format: str | None = \"pcm\",\n    model_config: Mapping[str, Any] | None = None,\n    first_content_at: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechSpanData]:\n    \"\"\"Create a new speech span. The span will not be started automatically, you should either do\n    `with speech_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        model: The name of the model used for the text-to-speech.\n        input: The text input of the text-to-speech.\n        output: The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\n        output_format: The format of the audio output (defaults to \"pcm\").\n        model_config: The model configuration (hyperparameters) used.\n        first_content_at: The time of the first byte of the audio output.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechSpanData(\n            model=model,\n            input=input,\n            output=output,\n            output_format=output_format,\n            model_config=model_config,\n            first_content_at=first_content_at,\n        ),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.speech_group_span","title":"speech_group_span","text":"<pre><code>speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]\n</code></pre> <p>Create a new speech group span. The span will not be started automatically, you should either do <code>with speech_group_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str | None</code> <p>The input text used for the speech request.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def speech_group_span(\n    input: str | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[SpeechGroupSpanData]:\n    \"\"\"Create a new speech group span. The span will not be started automatically, you should\n    either do `with speech_group_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        input: The input text used for the speech request.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=SpeechGroupSpanData(input=input),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/create/#agents.tracing.create.mcp_tools_span","title":"mcp_tools_span","text":"<pre><code>mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]\n</code></pre> <p>Create a new MCP list tools span. The span will not be started automatically, you should either do <code>with mcp_tools_span() ...</code> or call <code>span.start()</code> + <code>span.finish()</code> manually.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str | None</code> <p>The name of the MCP server.</p> <code>None</code> <code>result</code> <code>list[str] | None</code> <p>The result of the MCP list tools call.</p> <code>None</code> <code>span_id</code> <code>str | None</code> <p>The ID of the span. Optional. If not provided, we will generate an ID. We recommend using <code>util.gen_span_id()</code> to generate a span ID, to guarantee that IDs are correctly formatted.</p> <code>None</code> <code>parent</code> <code>Trace | Span[Any] | None</code> <p>The parent span or trace. If not provided, we will automatically use the current trace/span as the parent.</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>If True, we will return a Span but the Span will not be recorded.</p> <code>False</code> Source code in <code>src/agents/tracing/create.py</code> <pre><code>def mcp_tools_span(\n    server: str | None = None,\n    result: list[str] | None = None,\n    span_id: str | None = None,\n    parent: Trace | Span[Any] | None = None,\n    disabled: bool = False,\n) -&gt; Span[MCPListToolsSpanData]:\n    \"\"\"Create a new MCP list tools span. The span will not be started automatically, you should\n    either do `with mcp_tools_span() ...` or call `span.start()` + `span.finish()` manually.\n\n    Args:\n        server: The name of the MCP server.\n        result: The result of the MCP list tools call.\n        span_id: The ID of the span. Optional. If not provided, we will generate an ID. We\n            recommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\n            correctly formatted.\n        parent: The parent span or trace. If not provided, we will automatically use the current\n            trace/span as the parent.\n        disabled: If True, we will return a Span but the Span will not be recorded.\n    \"\"\"\n    return get_trace_provider().create_span(\n        span_data=MCPListToolsSpanData(server=server, result=result),\n        span_id=span_id,\n        parent=parent,\n        disabled=disabled,\n    )\n</code></pre>"},{"location":"ref/tracing/processor_interface/","title":"<code>Processor interface</code>","text":""},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor","title":"TracingProcessor","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for processing spans.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>class TracingProcessor(abc.ABC):\n    \"\"\"Interface for processing spans.\"\"\"\n\n    @abc.abstractmethod\n    def on_trace_start(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a trace is started.\n\n        Args:\n            trace: The trace that started.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_trace_end(self, trace: \"Trace\") -&gt; None:\n        \"\"\"Called when a trace is finished.\n\n        Args:\n            trace: The trace that finished.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_start(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a span is started.\n\n        Args:\n            span: The span that started.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def on_span_end(self, span: \"Span[Any]\") -&gt; None:\n        \"\"\"Called when a span is finished. Should not block or raise exceptions.\n\n        Args:\n            span: The span that finished.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def shutdown(self) -&gt; None:\n        \"\"\"Called when the application stops.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def force_flush(self) -&gt; None:\n        \"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_trace_start","title":"on_trace_start  <code>abstractmethod</code>","text":"<pre><code>on_trace_start(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace is started.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace that started.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_start(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a trace is started.\n\n    Args:\n        trace: The trace that started.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_trace_end","title":"on_trace_end  <code>abstractmethod</code>","text":"<pre><code>on_trace_end(trace: Trace) -&gt; None\n</code></pre> <p>Called when a trace is finished.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace that finished.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_trace_end(self, trace: \"Trace\") -&gt; None:\n    \"\"\"Called when a trace is finished.\n\n    Args:\n        trace: The trace that finished.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_span_start","title":"on_span_start  <code>abstractmethod</code>","text":"<pre><code>on_span_start(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span is started.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The span that started.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_start(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a span is started.\n\n    Args:\n        span: The span that started.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.on_span_end","title":"on_span_end  <code>abstractmethod</code>","text":"<pre><code>on_span_end(span: Span[Any]) -&gt; None\n</code></pre> <p>Called when a span is finished. Should not block or raise exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span[Any]</code> <p>The span that finished.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef on_span_end(self, span: \"Span[Any]\") -&gt; None:\n    \"\"\"Called when a span is finished. Should not block or raise exceptions.\n\n    Args:\n        span: The span that finished.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.shutdown","title":"shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Called when the application stops.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"Called when the application stops.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingProcessor.force_flush","title":"force_flush  <code>abstractmethod</code>","text":"<pre><code>force_flush() -&gt; None\n</code></pre> <p>Forces an immediate flush of all queued spans/traces.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef force_flush(self) -&gt; None:\n    \"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingExporter","title":"TracingExporter","text":"<p>               Bases: <code>ABC</code></p> <p>Exports traces and spans. For example, could log them or send them to a backend.</p> Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>class TracingExporter(abc.ABC):\n    \"\"\"Exports traces and spans. For example, could log them or send them to a backend.\"\"\"\n\n    @abc.abstractmethod\n    def export(self, items: list[\"Trace | Span[Any]\"]) -&gt; None:\n        \"\"\"Exports a list of traces and spans.\n\n        Args:\n            items: The items to export.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/processor_interface/#agents.tracing.processor_interface.TracingExporter.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export(items: list[Trace | Span[Any]]) -&gt; None\n</code></pre> <p>Exports a list of traces and spans.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[Trace | Span[Any]]</code> <p>The items to export.</p> required Source code in <code>src/agents/tracing/processor_interface.py</code> <pre><code>@abc.abstractmethod\ndef export(self, items: list[\"Trace | Span[Any]\"]) -&gt; None:\n    \"\"\"Exports a list of traces and spans.\n\n    Args:\n        items: The items to export.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/processors/","title":"<code>Processors</code>","text":""},{"location":"ref/tracing/processors/#agents.tracing.processors.ConsoleSpanExporter","title":"ConsoleSpanExporter","text":"<p>               Bases: <code>TracingExporter</code></p> <p>Prints the traces and spans to the console.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>class ConsoleSpanExporter(TracingExporter):\n    \"\"\"Prints the traces and spans to the console.\"\"\"\n\n    def export(self, items: list[Trace | Span[Any]]) -&gt; None:\n        for item in items:\n            if isinstance(item, Trace):\n                print(f\"[Exporter] Export trace_id={item.trace_id}, name={item.name}, \")\n            else:\n                print(f\"[Exporter] Export span: {item.export()}\")\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter","title":"BackendSpanExporter","text":"<p>               Bases: <code>TracingExporter</code></p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>class BackendSpanExporter(TracingExporter):\n    def __init__(\n        self,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        endpoint: str = \"https://api.openai.com/v1/traces/ingest\",\n        max_retries: int = 3,\n        base_delay: float = 1.0,\n        max_delay: float = 30.0,\n    ):\n        \"\"\"\n        Args:\n            api_key: The API key for the \"Authorization\" header. Defaults to\n                `os.environ[\"OPENAI_API_KEY\"]` if not provided.\n            organization: The OpenAI organization to use. Defaults to\n                `os.environ[\"OPENAI_ORG_ID\"]` if not provided.\n            project: The OpenAI project to use. Defaults to\n                `os.environ[\"OPENAI_PROJECT_ID\"]` if not provided.\n            endpoint: The HTTP endpoint to which traces/spans are posted.\n            max_retries: Maximum number of retries upon failures.\n            base_delay: Base delay (in seconds) for the first backoff.\n            max_delay: Maximum delay (in seconds) for backoff growth.\n        \"\"\"\n        self._api_key = api_key\n        self._organization = organization\n        self._project = project\n        self.endpoint = endpoint\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n\n        # Keep a client open for connection pooling across multiple export calls\n        self._client = httpx.Client(timeout=httpx.Timeout(timeout=60, connect=5.0))\n\n    def set_api_key(self, api_key: str):\n        \"\"\"Set the OpenAI API key for the exporter.\n\n        Args:\n            api_key: The OpenAI API key to use. This is the same key used by the OpenAI Python\n                client.\n        \"\"\"\n        # We're specifically setting the underlying cached property as well\n        self._api_key = api_key\n        self.api_key = api_key\n\n    @cached_property\n    def api_key(self):\n        return self._api_key or os.environ.get(\"OPENAI_API_KEY\")\n\n    @cached_property\n    def organization(self):\n        return self._organization or os.environ.get(\"OPENAI_ORG_ID\")\n\n    @cached_property\n    def project(self):\n        return self._project or os.environ.get(\"OPENAI_PROJECT_ID\")\n\n    def export(self, items: list[Trace | Span[Any]]) -&gt; None:\n        if not items:\n            return\n\n        if not self.api_key:\n            logger.warning(\"OPENAI_API_KEY is not set, skipping trace export\")\n            return\n\n        data = [item.export() for item in items if item.export()]\n        payload = {\"data\": data}\n\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"OpenAI-Beta\": \"traces=v1\",\n        }\n\n        if self.organization:\n            headers[\"OpenAI-Organization\"] = self.organization\n\n        if self.project:\n            headers[\"OpenAI-Project\"] = self.project\n\n        # Exponential backoff loop\n        attempt = 0\n        delay = self.base_delay\n        while True:\n            attempt += 1\n            try:\n                response = self._client.post(url=self.endpoint, headers=headers, json=payload)\n\n                # If the response is successful, break out of the loop\n                if response.status_code &lt; 300:\n                    logger.debug(f\"Exported {len(items)} items\")\n                    return\n\n                # If the response is a client error (4xx), we wont retry\n                if 400 &lt;= response.status_code &lt; 500:\n                    logger.error(\n                        f\"[non-fatal] Tracing client error {response.status_code}: {response.text}\"\n                    )\n                    return\n\n                # For 5xx or other unexpected codes, treat it as transient and retry\n                logger.warning(\n                    f\"[non-fatal] Tracing: server error {response.status_code}, retrying.\"\n                )\n            except httpx.RequestError as exc:\n                # Network or other I/O error, we'll retry\n                logger.warning(f\"[non-fatal] Tracing: request failed: {exc}\")\n\n            # If we reach here, we need to retry or give up\n            if attempt &gt;= self.max_retries:\n                logger.error(\"[non-fatal] Tracing: max retries reached, giving up on this batch.\")\n                return\n\n            # Exponential backoff + jitter\n            sleep_time = delay + random.uniform(0, 0.1 * delay)  # 10% jitter\n            time.sleep(sleep_time)\n            delay = min(delay * 2, self.max_delay)\n\n    def close(self):\n        \"\"\"Close the underlying HTTP client.\"\"\"\n        self._client.close()\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key: str | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n    endpoint: str = \"https://api.openai.com/v1/traces/ingest\",\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 30.0,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key for the \"Authorization\" header. Defaults to <code>os.environ[\"OPENAI_API_KEY\"]</code> if not provided.</p> <code>None</code> <code>organization</code> <code>str | None</code> <p>The OpenAI organization to use. Defaults to <code>os.environ[\"OPENAI_ORG_ID\"]</code> if not provided.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>The OpenAI project to use. Defaults to <code>os.environ[\"OPENAI_PROJECT_ID\"]</code> if not provided.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The HTTP endpoint to which traces/spans are posted.</p> <code>'https://api.openai.com/v1/traces/ingest'</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries upon failures.</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Base delay (in seconds) for the first backoff.</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay (in seconds) for backoff growth.</p> <code>30.0</code> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def __init__(\n    self,\n    api_key: str | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n    endpoint: str = \"https://api.openai.com/v1/traces/ingest\",\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 30.0,\n):\n    \"\"\"\n    Args:\n        api_key: The API key for the \"Authorization\" header. Defaults to\n            `os.environ[\"OPENAI_API_KEY\"]` if not provided.\n        organization: The OpenAI organization to use. Defaults to\n            `os.environ[\"OPENAI_ORG_ID\"]` if not provided.\n        project: The OpenAI project to use. Defaults to\n            `os.environ[\"OPENAI_PROJECT_ID\"]` if not provided.\n        endpoint: The HTTP endpoint to which traces/spans are posted.\n        max_retries: Maximum number of retries upon failures.\n        base_delay: Base delay (in seconds) for the first backoff.\n        max_delay: Maximum delay (in seconds) for backoff growth.\n    \"\"\"\n    self._api_key = api_key\n    self._organization = organization\n    self._project = project\n    self.endpoint = endpoint\n    self.max_retries = max_retries\n    self.base_delay = base_delay\n    self.max_delay = max_delay\n\n    # Keep a client open for connection pooling across multiple export calls\n    self._client = httpx.Client(timeout=httpx.Timeout(timeout=60, connect=5.0))\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter.set_api_key","title":"set_api_key","text":"<pre><code>set_api_key(api_key: str)\n</code></pre> <p>Set the OpenAI API key for the exporter.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The OpenAI API key to use. This is the same key used by the OpenAI Python client.</p> required Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def set_api_key(self, api_key: str):\n    \"\"\"Set the OpenAI API key for the exporter.\n\n    Args:\n        api_key: The OpenAI API key to use. This is the same key used by the OpenAI Python\n            client.\n    \"\"\"\n    # We're specifically setting the underlying cached property as well\n    self._api_key = api_key\n    self.api_key = api_key\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the underlying HTTP client.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def close(self):\n    \"\"\"Close the underlying HTTP client.\"\"\"\n    self._client.close()\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor","title":"BatchTraceProcessor","text":"<p>               Bases: <code>TracingProcessor</code></p> <p>Some implementation notes: 1. Using Queue, which is thread-safe. 2. Using a background thread to export spans, to minimize any performance issues. 3. Spans are stored in memory until they are exported.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>class BatchTraceProcessor(TracingProcessor):\n    \"\"\"Some implementation notes:\n    1. Using Queue, which is thread-safe.\n    2. Using a background thread to export spans, to minimize any performance issues.\n    3. Spans are stored in memory until they are exported.\n    \"\"\"\n\n    def __init__(\n        self,\n        exporter: TracingExporter,\n        max_queue_size: int = 8192,\n        max_batch_size: int = 128,\n        schedule_delay: float = 5.0,\n        export_trigger_ratio: float = 0.7,\n    ):\n        \"\"\"\n        Args:\n            exporter: The exporter to use.\n            max_queue_size: The maximum number of spans to store in the queue. After this, we will\n                start dropping spans.\n            max_batch_size: The maximum number of spans to export in a single batch.\n            schedule_delay: The delay between checks for new spans to export.\n            export_trigger_ratio: The ratio of the queue size at which we will trigger an export.\n        \"\"\"\n        self._exporter = exporter\n        self._queue: queue.Queue[Trace | Span[Any]] = queue.Queue(maxsize=max_queue_size)\n        self._max_queue_size = max_queue_size\n        self._max_batch_size = max_batch_size\n        self._schedule_delay = schedule_delay\n        self._shutdown_event = threading.Event()\n\n        # The queue size threshold at which we export immediately.\n        self._export_trigger_size = int(max_queue_size * export_trigger_ratio)\n\n        # Track when we next *must* perform a scheduled export\n        self._next_export_time = time.time() + self._schedule_delay\n\n        # We lazily start the background worker thread the first time a span/trace is queued.\n        self._worker_thread: threading.Thread | None = None\n        self._thread_start_lock = threading.Lock()\n\n    def _ensure_thread_started(self) -&gt; None:\n        # Fast path without holding the lock\n        if self._worker_thread and self._worker_thread.is_alive():\n            return\n\n        # Double-checked locking to avoid starting multiple threads\n        with self._thread_start_lock:\n            if self._worker_thread and self._worker_thread.is_alive():\n                return\n\n            self._worker_thread = threading.Thread(target=self._run, daemon=True)\n            self._worker_thread.start()\n\n    def on_trace_start(self, trace: Trace) -&gt; None:\n        # Ensure the background worker is running before we enqueue anything.\n        self._ensure_thread_started()\n\n        try:\n            self._queue.put_nowait(trace)\n        except queue.Full:\n            logger.warning(\"Queue is full, dropping trace.\")\n\n    def on_trace_end(self, trace: Trace) -&gt; None:\n        # We send traces via on_trace_start, so we don't need to do anything here.\n        pass\n\n    def on_span_start(self, span: Span[Any]) -&gt; None:\n        # We send spans via on_span_end, so we don't need to do anything here.\n        pass\n\n    def on_span_end(self, span: Span[Any]) -&gt; None:\n        # Ensure the background worker is running before we enqueue anything.\n        self._ensure_thread_started()\n\n        try:\n            self._queue.put_nowait(span)\n        except queue.Full:\n            logger.warning(\"Queue is full, dropping span.\")\n\n    def shutdown(self, timeout: float | None = None):\n        \"\"\"\n        Called when the application stops. We signal our thread to stop, then join it.\n        \"\"\"\n        self._shutdown_event.set()\n\n        # Only join if we ever started the background thread; otherwise flush synchronously.\n        if self._worker_thread and self._worker_thread.is_alive():\n            self._worker_thread.join(timeout=timeout)\n        else:\n            # No background thread: process any remaining items synchronously.\n            self._export_batches(force=True)\n\n    def force_flush(self):\n        \"\"\"\n        Forces an immediate flush of all queued spans.\n        \"\"\"\n        self._export_batches(force=True)\n\n    def _run(self):\n        while not self._shutdown_event.is_set():\n            current_time = time.time()\n            queue_size = self._queue.qsize()\n\n            # If it's time for a scheduled flush or queue is above the trigger threshold\n            if current_time &gt;= self._next_export_time or queue_size &gt;= self._export_trigger_size:\n                self._export_batches(force=False)\n                # Reset the next scheduled flush time\n                self._next_export_time = time.time() + self._schedule_delay\n            else:\n                # Sleep a short interval so we don't busy-wait.\n                time.sleep(0.2)\n\n        # Final drain after shutdown\n        self._export_batches(force=True)\n\n    def _export_batches(self, force: bool = False):\n        \"\"\"Drains the queue and exports in batches. If force=True, export everything.\n        Otherwise, export up to `max_batch_size` repeatedly until the queue is empty or below a\n        certain threshold.\n        \"\"\"\n        while True:\n            items_to_export: list[Span[Any] | Trace] = []\n\n            # Gather a batch of spans up to max_batch_size\n            while not self._queue.empty() and (\n                force or len(items_to_export) &lt; self._max_batch_size\n            ):\n                try:\n                    items_to_export.append(self._queue.get_nowait())\n                except queue.Empty:\n                    # Another thread might have emptied the queue between checks\n                    break\n\n            # If we collected nothing, we're done\n            if not items_to_export:\n                break\n\n            # Export the batch\n            self._exporter.export(items_to_export)\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor.__init__","title":"__init__","text":"<pre><code>__init__(\n    exporter: TracingExporter,\n    max_queue_size: int = 8192,\n    max_batch_size: int = 128,\n    schedule_delay: float = 5.0,\n    export_trigger_ratio: float = 0.7,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>exporter</code> <code>TracingExporter</code> <p>The exporter to use.</p> required <code>max_queue_size</code> <code>int</code> <p>The maximum number of spans to store in the queue. After this, we will start dropping spans.</p> <code>8192</code> <code>max_batch_size</code> <code>int</code> <p>The maximum number of spans to export in a single batch.</p> <code>128</code> <code>schedule_delay</code> <code>float</code> <p>The delay between checks for new spans to export.</p> <code>5.0</code> <code>export_trigger_ratio</code> <code>float</code> <p>The ratio of the queue size at which we will trigger an export.</p> <code>0.7</code> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def __init__(\n    self,\n    exporter: TracingExporter,\n    max_queue_size: int = 8192,\n    max_batch_size: int = 128,\n    schedule_delay: float = 5.0,\n    export_trigger_ratio: float = 0.7,\n):\n    \"\"\"\n    Args:\n        exporter: The exporter to use.\n        max_queue_size: The maximum number of spans to store in the queue. After this, we will\n            start dropping spans.\n        max_batch_size: The maximum number of spans to export in a single batch.\n        schedule_delay: The delay between checks for new spans to export.\n        export_trigger_ratio: The ratio of the queue size at which we will trigger an export.\n    \"\"\"\n    self._exporter = exporter\n    self._queue: queue.Queue[Trace | Span[Any]] = queue.Queue(maxsize=max_queue_size)\n    self._max_queue_size = max_queue_size\n    self._max_batch_size = max_batch_size\n    self._schedule_delay = schedule_delay\n    self._shutdown_event = threading.Event()\n\n    # The queue size threshold at which we export immediately.\n    self._export_trigger_size = int(max_queue_size * export_trigger_ratio)\n\n    # Track when we next *must* perform a scheduled export\n    self._next_export_time = time.time() + self._schedule_delay\n\n    # We lazily start the background worker thread the first time a span/trace is queued.\n    self._worker_thread: threading.Thread | None = None\n    self._thread_start_lock = threading.Lock()\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor.shutdown","title":"shutdown","text":"<pre><code>shutdown(timeout: float | None = None)\n</code></pre> <p>Called when the application stops. We signal our thread to stop, then join it.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def shutdown(self, timeout: float | None = None):\n    \"\"\"\n    Called when the application stops. We signal our thread to stop, then join it.\n    \"\"\"\n    self._shutdown_event.set()\n\n    # Only join if we ever started the background thread; otherwise flush synchronously.\n    if self._worker_thread and self._worker_thread.is_alive():\n        self._worker_thread.join(timeout=timeout)\n    else:\n        # No background thread: process any remaining items synchronously.\n        self._export_batches(force=True)\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor.force_flush","title":"force_flush","text":"<pre><code>force_flush()\n</code></pre> <p>Forces an immediate flush of all queued spans.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def force_flush(self):\n    \"\"\"\n    Forces an immediate flush of all queued spans.\n    \"\"\"\n    self._export_batches(force=True)\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.default_exporter","title":"default_exporter","text":"<pre><code>default_exporter() -&gt; BackendSpanExporter\n</code></pre> <p>The default exporter, which exports traces and spans to the backend in batches.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def default_exporter() -&gt; BackendSpanExporter:\n    \"\"\"The default exporter, which exports traces and spans to the backend in batches.\"\"\"\n    return _global_exporter\n</code></pre>"},{"location":"ref/tracing/processors/#agents.tracing.processors.default_processor","title":"default_processor","text":"<pre><code>default_processor() -&gt; BatchTraceProcessor\n</code></pre> <p>The default processor, which exports traces and spans to the backend in batches.</p> Source code in <code>src/agents/tracing/processors.py</code> <pre><code>def default_processor() -&gt; BatchTraceProcessor:\n    \"\"\"The default processor, which exports traces and spans to the backend in batches.\"\"\"\n    return _global_processor\n</code></pre>"},{"location":"ref/tracing/scope/","title":"<code>Scope</code>","text":""},{"location":"ref/tracing/scope/#agents.tracing.scope.Scope","title":"Scope","text":"<p>Manages the current span and trace in the context.</p> Source code in <code>src/agents/tracing/scope.py</code> <pre><code>class Scope:\n    \"\"\"\n    Manages the current span and trace in the context.\n    \"\"\"\n\n    @classmethod\n    def get_current_span(cls) -&gt; \"Span[Any] | None\":\n        return _current_span.get()\n\n    @classmethod\n    def set_current_span(cls, span: \"Span[Any] | None\") -&gt; \"contextvars.Token[Span[Any] | None]\":\n        return _current_span.set(span)\n\n    @classmethod\n    def reset_current_span(cls, token: \"contextvars.Token[Span[Any] | None]\") -&gt; None:\n        _current_span.reset(token)\n\n    @classmethod\n    def get_current_trace(cls) -&gt; \"Trace | None\":\n        return _current_trace.get()\n\n    @classmethod\n    def set_current_trace(cls, trace: \"Trace | None\") -&gt; \"contextvars.Token[Trace | None]\":\n        logger.debug(f\"Setting current trace: {trace.trace_id if trace else None}\")\n        return _current_trace.set(trace)\n\n    @classmethod\n    def reset_current_trace(cls, token: \"contextvars.Token[Trace | None]\") -&gt; None:\n        logger.debug(\"Resetting current trace\")\n        _current_trace.reset(token)\n</code></pre>"},{"location":"ref/tracing/setup/","title":"<code>Setup</code>","text":""},{"location":"ref/tracing/setup/#agents.tracing.setup.set_trace_provider","title":"set_trace_provider","text":"<pre><code>set_trace_provider(provider: TraceProvider) -&gt; None\n</code></pre> <p>Set the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def set_trace_provider(provider: TraceProvider) -&gt; None:\n    \"\"\"Set the global trace provider used by tracing utilities.\"\"\"\n    global GLOBAL_TRACE_PROVIDER\n    GLOBAL_TRACE_PROVIDER = provider\n</code></pre>"},{"location":"ref/tracing/setup/#agents.tracing.setup.get_trace_provider","title":"get_trace_provider","text":"<pre><code>get_trace_provider() -&gt; TraceProvider\n</code></pre> <p>Get the global trace provider used by tracing utilities.</p> Source code in <code>src/agents/tracing/setup.py</code> <pre><code>def get_trace_provider() -&gt; TraceProvider:\n    \"\"\"Get the global trace provider used by tracing utilities.\"\"\"\n    if GLOBAL_TRACE_PROVIDER is None:\n        raise RuntimeError(\"Trace provider not set\")\n    return GLOBAL_TRACE_PROVIDER\n</code></pre>"},{"location":"ref/tracing/span_data/","title":"<code>Span data</code>","text":""},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpanData","title":"SpanData","text":"<p>               Bases: <code>ABC</code></p> <p>Represents span data in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpanData(abc.ABC):\n    \"\"\"\n    Represents span data in the trace.\n    \"\"\"\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any]:\n        \"\"\"Export the span data as a dictionary.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def type(self) -&gt; str:\n        \"\"\"Return the type of the span.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpanData.type","title":"type  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>Return the type of the span.</p>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpanData.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any]\n</code></pre> <p>Export the span data as a dictionary.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any]:\n    \"\"\"Export the span data as a dictionary.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.AgentSpanData","title":"AgentSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an Agent Span in the trace. Includes name, handoffs, tools, and output type.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class AgentSpanData(SpanData):\n    \"\"\"\n    Represents an Agent Span in the trace.\n    Includes name, handoffs, tools, and output type.\n    \"\"\"\n\n    __slots__ = (\"name\", \"handoffs\", \"tools\", \"output_type\")\n\n    def __init__(\n        self,\n        name: str,\n        handoffs: list[str] | None = None,\n        tools: list[str] | None = None,\n        output_type: str | None = None,\n    ):\n        self.name = name\n        self.handoffs: list[str] | None = handoffs\n        self.tools: list[str] | None = tools\n        self.output_type: str | None = output_type\n\n    @property\n    def type(self) -&gt; str:\n        return \"agent\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"handoffs\": self.handoffs,\n            \"tools\": self.tools,\n            \"output_type\": self.output_type,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.FunctionSpanData","title":"FunctionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Function Span in the trace. Includes input, output and MCP data (if applicable).</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class FunctionSpanData(SpanData):\n    \"\"\"\n    Represents a Function Span in the trace.\n    Includes input, output and MCP data (if applicable).\n    \"\"\"\n\n    __slots__ = (\"name\", \"input\", \"output\", \"mcp_data\")\n\n    def __init__(\n        self,\n        name: str,\n        input: str | None,\n        output: Any | None,\n        mcp_data: dict[str, Any] | None = None,\n    ):\n        self.name = name\n        self.input = input\n        self.output = output\n        self.mcp_data = mcp_data\n\n    @property\n    def type(self) -&gt; str:\n        return \"function\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"input\": self.input,\n            \"output\": str(self.output) if self.output else None,\n            \"mcp_data\": self.mcp_data,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.GenerationSpanData","title":"GenerationSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Generation Span in the trace. Includes input, output, model, model configuration, and usage.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GenerationSpanData(SpanData):\n    \"\"\"\n    Represents a Generation Span in the trace.\n    Includes input, output, model, model configuration, and usage.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n        \"usage\",\n    )\n\n    def __init__(\n        self,\n        input: Sequence[Mapping[str, Any]] | None = None,\n        output: Sequence[Mapping[str, Any]] | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        usage: dict[str, Any] | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n        self.usage = usage\n\n    @property\n    def type(self) -&gt; str:\n        return \"generation\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"usage\": self.usage,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.ResponseSpanData","title":"ResponseSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Response Span in the trace. Includes response and input.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class ResponseSpanData(SpanData):\n    \"\"\"\n    Represents a Response Span in the trace.\n    Includes response and input.\n    \"\"\"\n\n    __slots__ = (\"response\", \"input\")\n\n    def __init__(\n        self,\n        response: Response | None = None,\n        input: str | list[ResponseInputItemParam] | None = None,\n    ) -&gt; None:\n        self.response = response\n        # This is not used by the OpenAI trace processors, but is useful for other tracing\n        # processor implementations\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"response\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"response_id\": self.response.id if self.response else None,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.HandoffSpanData","title":"HandoffSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Handoff Span in the trace. Includes source and destination agents.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class HandoffSpanData(SpanData):\n    \"\"\"\n    Represents a Handoff Span in the trace.\n    Includes source and destination agents.\n    \"\"\"\n\n    __slots__ = (\"from_agent\", \"to_agent\")\n\n    def __init__(self, from_agent: str | None, to_agent: str | None):\n        self.from_agent = from_agent\n        self.to_agent = to_agent\n\n    @property\n    def type(self) -&gt; str:\n        return \"handoff\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"from_agent\": self.from_agent,\n            \"to_agent\": self.to_agent,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.CustomSpanData","title":"CustomSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Custom Span in the trace. Includes name and data property bag.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class CustomSpanData(SpanData):\n    \"\"\"\n    Represents a Custom Span in the trace.\n    Includes name and data property bag.\n    \"\"\"\n\n    __slots__ = (\"name\", \"data\")\n\n    def __init__(self, name: str, data: dict[str, Any]):\n        self.name = name\n        self.data = data\n\n    @property\n    def type(self) -&gt; str:\n        return \"custom\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"data\": self.data,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.GuardrailSpanData","title":"GuardrailSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Guardrail Span in the trace. Includes name and triggered status.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class GuardrailSpanData(SpanData):\n    \"\"\"\n    Represents a Guardrail Span in the trace.\n    Includes name and triggered status.\n    \"\"\"\n\n    __slots__ = (\"name\", \"triggered\")\n\n    def __init__(self, name: str, triggered: bool = False):\n        self.name = name\n        self.triggered = triggered\n\n    @property\n    def type(self) -&gt; str:\n        return \"guardrail\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"name\": self.name,\n            \"triggered\": self.triggered,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.TranscriptionSpanData","title":"TranscriptionSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Transcription Span in the trace. Includes input, output, model, and model configuration.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class TranscriptionSpanData(SpanData):\n    \"\"\"\n    Represents a Transcription Span in the trace.\n    Includes input, output, model, and model configuration.\n    \"\"\"\n\n    __slots__ = (\n        \"input\",\n        \"output\",\n        \"model\",\n        \"model_config\",\n    )\n\n    def __init__(\n        self,\n        input: str | None = None,\n        input_format: str | None = \"pcm\",\n        output: str | None = None,\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n    ):\n        self.input = input\n        self.input_format = input_format\n        self.output = output\n        self.model = model\n        self.model_config = model_config\n\n    @property\n    def type(self) -&gt; str:\n        return \"transcription\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": {\n                \"data\": self.input or \"\",\n                \"format\": self.input_format,\n            },\n            \"output\": self.output,\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpeechSpanData","title":"SpeechSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Span in the trace. Includes input, output, model, model configuration, and first content timestamp.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Span in the trace.\n    Includes input, output, model, model configuration, and first content timestamp.\n    \"\"\"\n\n    __slots__ = (\"input\", \"output\", \"model\", \"model_config\", \"first_content_at\")\n\n    def __init__(\n        self,\n        input: str | None = None,\n        output: str | None = None,\n        output_format: str | None = \"pcm\",\n        model: str | None = None,\n        model_config: Mapping[str, Any] | None = None,\n        first_content_at: str | None = None,\n    ):\n        self.input = input\n        self.output = output\n        self.output_format = output_format\n        self.model = model\n        self.model_config = model_config\n        self.first_content_at = first_content_at\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n            \"output\": {\n                \"data\": self.output or \"\",\n                \"format\": self.output_format,\n            },\n            \"model\": self.model,\n            \"model_config\": self.model_config,\n            \"first_content_at\": self.first_content_at,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.SpeechGroupSpanData","title":"SpeechGroupSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents a Speech Group Span in the trace.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class SpeechGroupSpanData(SpanData):\n    \"\"\"\n    Represents a Speech Group Span in the trace.\n    \"\"\"\n\n    __slots__ = \"input\"\n\n    def __init__(\n        self,\n        input: str | None = None,\n    ):\n        self.input = input\n\n    @property\n    def type(self) -&gt; str:\n        return \"speech_group\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"input\": self.input,\n        }\n</code></pre>"},{"location":"ref/tracing/span_data/#agents.tracing.span_data.MCPListToolsSpanData","title":"MCPListToolsSpanData","text":"<p>               Bases: <code>SpanData</code></p> <p>Represents an MCP List Tools Span in the trace. Includes server and result.</p> Source code in <code>src/agents/tracing/span_data.py</code> <pre><code>class MCPListToolsSpanData(SpanData):\n    \"\"\"\n    Represents an MCP List Tools Span in the trace.\n    Includes server and result.\n    \"\"\"\n\n    __slots__ = (\n        \"server\",\n        \"result\",\n    )\n\n    def __init__(self, server: str | None = None, result: list[str] | None = None):\n        self.server = server\n        self.result = result\n\n    @property\n    def type(self) -&gt; str:\n        return \"mcp_tools\"\n\n    def export(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": self.type,\n            \"server\": self.server,\n            \"result\": self.result,\n        }\n</code></pre>"},{"location":"ref/tracing/spans/","title":"<code>Spans</code>","text":""},{"location":"ref/tracing/spans/#agents.tracing.spans.Span","title":"Span","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TSpanData]</code></p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class Span(abc.ABC, Generic[TSpanData]):\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_id(self) -&gt; str:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def span_data(self) -&gt; TSpanData:\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"\n        Start the span.\n\n        Args:\n            mark_as_current: If true, the span will be marked as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False) -&gt; None:\n        \"\"\"\n        Finish the span.\n\n        Args:\n            reset_current: If true, the span will be reset as the current span.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Span[TSpanData]:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @property\n    @abc.abstractmethod\n    def parent_id(self) -&gt; str | None:\n        pass\n\n    @abc.abstractmethod\n    def set_error(self, error: SpanError) -&gt; None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def error(self) -&gt; SpanError | None:\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def started_at(self) -&gt; str | None:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def ended_at(self) -&gt; str | None:\n        pass\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the span.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, the span will be marked as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"\n    Start the span.\n\n    Args:\n        mark_as_current: If true, the span will be marked as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.Span.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False) -&gt; None\n</code></pre> <p>Finish the span.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, the span will be reset as the current span.</p> <code>False</code> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False) -&gt; None:\n    \"\"\"\n    Finish the span.\n\n    Args:\n        reset_current: If true, the span will be reset as the current span.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.NoOpSpan","title":"NoOpSpan","text":"<p>               Bases: <code>Span[TSpanData]</code></p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class NoOpSpan(Span[TSpanData]):\n    __slots__ = (\"_span_data\", \"_prev_span_token\")\n\n    def __init__(self, span_data: TSpanData):\n        self._span_data = span_data\n        self._prev_span_token: contextvars.Token[Span[TSpanData] | None] | None = None\n\n    @property\n    def trace_id(self) -&gt; str:\n        return \"no-op\"\n\n    @property\n    def span_id(self) -&gt; str:\n        return \"no-op\"\n\n    @property\n    def span_data(self) -&gt; TSpanData:\n        return self._span_data\n\n    @property\n    def parent_id(self) -&gt; str | None:\n        return None\n\n    def start(self, mark_as_current: bool = False):\n        if mark_as_current:\n            self._prev_span_token = Scope.set_current_span(self)\n\n    def finish(self, reset_current: bool = False) -&gt; None:\n        if reset_current and self._prev_span_token is not None:\n            Scope.reset_current_span(self._prev_span_token)\n            self._prev_span_token = None\n\n    def __enter__(self) -&gt; Span[TSpanData]:\n        self.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        reset_current = True\n        if exc_type is GeneratorExit:\n            logger.debug(\"GeneratorExit, skipping span reset\")\n            reset_current = False\n\n        self.finish(reset_current=reset_current)\n\n    def set_error(self, error: SpanError) -&gt; None:\n        pass\n\n    @property\n    def error(self) -&gt; SpanError | None:\n        return None\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return None\n\n    @property\n    def started_at(self) -&gt; str | None:\n        return None\n\n    @property\n    def ended_at(self) -&gt; str | None:\n        return None\n</code></pre>"},{"location":"ref/tracing/spans/#agents.tracing.spans.SpanImpl","title":"SpanImpl","text":"<p>               Bases: <code>Span[TSpanData]</code></p> Source code in <code>src/agents/tracing/spans.py</code> <pre><code>class SpanImpl(Span[TSpanData]):\n    __slots__ = (\n        \"_trace_id\",\n        \"_span_id\",\n        \"_parent_id\",\n        \"_started_at\",\n        \"_ended_at\",\n        \"_error\",\n        \"_prev_span_token\",\n        \"_processor\",\n        \"_span_data\",\n    )\n\n    def __init__(\n        self,\n        trace_id: str,\n        span_id: str | None,\n        parent_id: str | None,\n        processor: TracingProcessor,\n        span_data: TSpanData,\n    ):\n        self._trace_id = trace_id\n        self._span_id = span_id or util.gen_span_id()\n        self._parent_id = parent_id\n        self._started_at: str | None = None\n        self._ended_at: str | None = None\n        self._processor = processor\n        self._error: SpanError | None = None\n        self._prev_span_token: contextvars.Token[Span[TSpanData] | None] | None = None\n        self._span_data = span_data\n\n    @property\n    def trace_id(self) -&gt; str:\n        return self._trace_id\n\n    @property\n    def span_id(self) -&gt; str:\n        return self._span_id\n\n    @property\n    def span_data(self) -&gt; TSpanData:\n        return self._span_data\n\n    @property\n    def parent_id(self) -&gt; str | None:\n        return self._parent_id\n\n    def start(self, mark_as_current: bool = False):\n        if self.started_at is not None:\n            logger.warning(\"Span already started\")\n            return\n\n        self._started_at = util.time_iso()\n        self._processor.on_span_start(self)\n        if mark_as_current:\n            self._prev_span_token = Scope.set_current_span(self)\n\n    def finish(self, reset_current: bool = False) -&gt; None:\n        if self.ended_at is not None:\n            logger.warning(\"Span already finished\")\n            return\n\n        self._ended_at = util.time_iso()\n        self._processor.on_span_end(self)\n        if reset_current and self._prev_span_token is not None:\n            Scope.reset_current_span(self._prev_span_token)\n            self._prev_span_token = None\n\n    def __enter__(self) -&gt; Span[TSpanData]:\n        self.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        reset_current = True\n        if exc_type is GeneratorExit:\n            logger.debug(\"GeneratorExit, skipping span reset\")\n            reset_current = False\n\n        self.finish(reset_current=reset_current)\n\n    def set_error(self, error: SpanError) -&gt; None:\n        self._error = error\n\n    @property\n    def error(self) -&gt; SpanError | None:\n        return self._error\n\n    @property\n    def started_at(self) -&gt; str | None:\n        return self._started_at\n\n    @property\n    def ended_at(self) -&gt; str | None:\n        return self._ended_at\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return {\n            \"object\": \"trace.span\",\n            \"id\": self.span_id,\n            \"trace_id\": self.trace_id,\n            \"parent_id\": self._parent_id,\n            \"started_at\": self._started_at,\n            \"ended_at\": self._ended_at,\n            \"span_data\": self.span_data.export(),\n            \"error\": self._error,\n        }\n</code></pre>"},{"location":"ref/tracing/traces/","title":"<code>Traces</code>","text":""},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace","title":"Trace","text":"<p>A trace is the root level object that tracing creates. It represents a logical \"workflow\".</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class Trace:\n    \"\"\"\n    A trace is the root level object that tracing creates. It represents a logical \"workflow\".\n    \"\"\"\n\n    @abc.abstractmethod\n    def __enter__(self) -&gt; Trace:\n        pass\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @abc.abstractmethod\n    def start(self, mark_as_current: bool = False):\n        \"\"\"\n        Start the trace.\n\n        Args:\n            mark_as_current: If true, the trace will be marked as the current trace.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def finish(self, reset_current: bool = False):\n        \"\"\"\n        Finish the trace.\n\n        Args:\n            reset_current: If true, the trace will be reset as the current trace.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def trace_id(self) -&gt; str:\n        \"\"\"\n        The trace ID.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"\n        The name of the workflow being traced.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def export(self) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Export the trace as a dictionary.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.trace_id","title":"trace_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>trace_id: str\n</code></pre> <p>The trace ID.</p>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the workflow being traced.</p>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(mark_as_current: bool = False)\n</code></pre> <p>Start the trace.</p> <p>Parameters:</p> Name Type Description Default <code>mark_as_current</code> <code>bool</code> <p>If true, the trace will be marked as the current trace.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef start(self, mark_as_current: bool = False):\n    \"\"\"\n    Start the trace.\n\n    Args:\n        mark_as_current: If true, the trace will be marked as the current trace.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(reset_current: bool = False)\n</code></pre> <p>Finish the trace.</p> <p>Parameters:</p> Name Type Description Default <code>reset_current</code> <code>bool</code> <p>If true, the trace will be reset as the current trace.</p> <code>False</code> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef finish(self, reset_current: bool = False):\n    \"\"\"\n    Finish the trace.\n\n    Args:\n        reset_current: If true, the trace will be reset as the current trace.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.Trace.export","title":"export  <code>abstractmethod</code>","text":"<pre><code>export() -&gt; dict[str, Any] | None\n</code></pre> <p>Export the trace as a dictionary.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>@abc.abstractmethod\ndef export(self) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Export the trace as a dictionary.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.NoOpTrace","title":"NoOpTrace","text":"<p>               Bases: <code>Trace</code></p> <p>A no-op trace that will not be recorded.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class NoOpTrace(Trace):\n    \"\"\"\n    A no-op trace that will not be recorded.\n    \"\"\"\n\n    def __init__(self):\n        self._started = False\n        self._prev_context_token: contextvars.Token[Trace | None] | None = None\n\n    def __enter__(self) -&gt; Trace:\n        if self._started:\n            if not self._prev_context_token:\n                logger.error(\"Trace already started but no context token set\")\n            return self\n\n        self._started = True\n        self.start(mark_as_current=True)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.finish(reset_current=True)\n\n    def start(self, mark_as_current: bool = False):\n        if mark_as_current:\n            self._prev_context_token = Scope.set_current_trace(self)\n\n    def finish(self, reset_current: bool = False):\n        if reset_current and self._prev_context_token is not None:\n            Scope.reset_current_trace(self._prev_context_token)\n            self._prev_context_token = None\n\n    @property\n    def trace_id(self) -&gt; str:\n        return \"no-op\"\n\n    @property\n    def name(self) -&gt; str:\n        return \"no-op\"\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return None\n</code></pre>"},{"location":"ref/tracing/traces/#agents.tracing.traces.TraceImpl","title":"TraceImpl","text":"<p>               Bases: <code>Trace</code></p> <p>A trace that will be recorded by the tracing library.</p> Source code in <code>src/agents/tracing/traces.py</code> <pre><code>class TraceImpl(Trace):\n    \"\"\"\n    A trace that will be recorded by the tracing library.\n    \"\"\"\n\n    __slots__ = (\n        \"_name\",\n        \"_trace_id\",\n        \"group_id\",\n        \"metadata\",\n        \"_prev_context_token\",\n        \"_processor\",\n        \"_started\",\n    )\n\n    def __init__(\n        self,\n        name: str,\n        trace_id: str | None,\n        group_id: str | None,\n        metadata: dict[str, Any] | None,\n        processor: TracingProcessor,\n    ):\n        self._name = name\n        self._trace_id = trace_id or util.gen_trace_id()\n        self.group_id = group_id\n        self.metadata = metadata\n        self._prev_context_token: contextvars.Token[Trace | None] | None = None\n        self._processor = processor\n        self._started = False\n\n    @property\n    def trace_id(self) -&gt; str:\n        return self._trace_id\n\n    @property\n    def name(self) -&gt; str:\n        return self._name\n\n    def start(self, mark_as_current: bool = False):\n        if self._started:\n            return\n\n        self._started = True\n        self._processor.on_trace_start(self)\n\n        if mark_as_current:\n            self._prev_context_token = Scope.set_current_trace(self)\n\n    def finish(self, reset_current: bool = False):\n        if not self._started:\n            return\n\n        self._processor.on_trace_end(self)\n\n        if reset_current and self._prev_context_token is not None:\n            Scope.reset_current_trace(self._prev_context_token)\n            self._prev_context_token = None\n\n    def __enter__(self) -&gt; Trace:\n        if self._started:\n            if not self._prev_context_token:\n                logger.error(\"Trace already started but no context token set\")\n            return self\n\n        self.start(mark_as_current=True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.finish(reset_current=exc_type is not GeneratorExit)\n\n    def export(self) -&gt; dict[str, Any] | None:\n        return {\n            \"object\": \"trace\",\n            \"id\": self.trace_id,\n            \"workflow_name\": self.name,\n            \"group_id\": self.group_id,\n            \"metadata\": self.metadata,\n        }\n</code></pre>"},{"location":"ref/tracing/util/","title":"<code>Util</code>","text":""},{"location":"ref/tracing/util/#agents.tracing.util.time_iso","title":"time_iso","text":"<pre><code>time_iso() -&gt; str\n</code></pre> <p>Return the current time in ISO 8601 format.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def time_iso() -&gt; str:\n    \"\"\"Return the current time in ISO 8601 format.\"\"\"\n    return get_trace_provider().time_iso()\n</code></pre>"},{"location":"ref/tracing/util/#agents.tracing.util.gen_trace_id","title":"gen_trace_id","text":"<pre><code>gen_trace_id() -&gt; str\n</code></pre> <p>Generate a new trace ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_trace_id() -&gt; str:\n    \"\"\"Generate a new trace ID.\"\"\"\n    return get_trace_provider().gen_trace_id()\n</code></pre>"},{"location":"ref/tracing/util/#agents.tracing.util.gen_span_id","title":"gen_span_id","text":"<pre><code>gen_span_id() -&gt; str\n</code></pre> <p>Generate a new span ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_span_id() -&gt; str:\n    \"\"\"Generate a new span ID.\"\"\"\n    return get_trace_provider().gen_span_id()\n</code></pre>"},{"location":"ref/tracing/util/#agents.tracing.util.gen_group_id","title":"gen_group_id","text":"<pre><code>gen_group_id() -&gt; str\n</code></pre> <p>Generate a new group ID.</p> Source code in <code>src/agents/tracing/util.py</code> <pre><code>def gen_group_id() -&gt; str:\n    \"\"\"Generate a new group ID.\"\"\"\n    return get_trace_provider().gen_group_id()\n</code></pre>"},{"location":"ref/voice/events/","title":"<code>Events</code>","text":""},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEvent","title":"VoiceStreamEvent  <code>module-attribute</code>","text":"<pre><code>VoiceStreamEvent: TypeAlias = Union[\n    VoiceStreamEventAudio,\n    VoiceStreamEventLifecycle,\n    VoiceStreamEventError,\n]\n</code></pre> <p>An event from the <code>VoicePipeline</code>, streamed via <code>StreamedAudioResult.stream()</code>.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventAudio","title":"VoiceStreamEventAudio  <code>dataclass</code>","text":"<p>Streaming event from the VoicePipeline</p> Source code in <code>src/agents/voice/events.py</code> <pre><code>@dataclass\nclass VoiceStreamEventAudio:\n    \"\"\"Streaming event from the VoicePipeline\"\"\"\n\n    data: npt.NDArray[np.int16 | np.float32] | None\n    \"\"\"The audio data.\"\"\"\n\n    type: Literal[\"voice_stream_event_audio\"] = \"voice_stream_event_audio\"\n    \"\"\"The type of event.\"\"\"\n</code></pre>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventAudio.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: NDArray[int16 | float32] | None\n</code></pre> <p>The audio data.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventAudio.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"voice_stream_event_audio\"] = (\n    \"voice_stream_event_audio\"\n)\n</code></pre> <p>The type of event.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventLifecycle","title":"VoiceStreamEventLifecycle  <code>dataclass</code>","text":"<p>Streaming event from the VoicePipeline</p> Source code in <code>src/agents/voice/events.py</code> <pre><code>@dataclass\nclass VoiceStreamEventLifecycle:\n    \"\"\"Streaming event from the VoicePipeline\"\"\"\n\n    event: Literal[\"turn_started\", \"turn_ended\", \"session_ended\"]\n    \"\"\"The event that occurred.\"\"\"\n\n    type: Literal[\"voice_stream_event_lifecycle\"] = \"voice_stream_event_lifecycle\"\n    \"\"\"The type of event.\"\"\"\n</code></pre>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventLifecycle.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: Literal[\n    \"turn_started\", \"turn_ended\", \"session_ended\"\n]\n</code></pre> <p>The event that occurred.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventLifecycle.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"voice_stream_event_lifecycle\"] = (\n    \"voice_stream_event_lifecycle\"\n)\n</code></pre> <p>The type of event.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventError","title":"VoiceStreamEventError  <code>dataclass</code>","text":"<p>Streaming event from the VoicePipeline</p> Source code in <code>src/agents/voice/events.py</code> <pre><code>@dataclass\nclass VoiceStreamEventError:\n    \"\"\"Streaming event from the VoicePipeline\"\"\"\n\n    error: Exception\n    \"\"\"The error that occurred.\"\"\"\n\n    type: Literal[\"voice_stream_event_error\"] = \"voice_stream_event_error\"\n    \"\"\"The type of event.\"\"\"\n</code></pre>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventError.error","title":"error  <code>instance-attribute</code>","text":"<pre><code>error: Exception\n</code></pre> <p>The error that occurred.</p>"},{"location":"ref/voice/events/#agents.voice.events.VoiceStreamEventError.type","title":"type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>type: Literal[\"voice_stream_event_error\"] = (\n    \"voice_stream_event_error\"\n)\n</code></pre> <p>The type of event.</p>"},{"location":"ref/voice/exceptions/","title":"<code>Exceptions</code>","text":""},{"location":"ref/voice/exceptions/#agents.voice.exceptions.STTWebsocketConnectionError","title":"STTWebsocketConnectionError","text":"<p>               Bases: <code>AgentsException</code></p> <p>Exception raised when the STT websocket connection fails.</p> Source code in <code>src/agents/voice/exceptions.py</code> <pre><code>class STTWebsocketConnectionError(AgentsException):\n    \"\"\"Exception raised when the STT websocket connection fails.\"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n</code></pre>"},{"location":"ref/voice/input/","title":"<code>Input</code>","text":""},{"location":"ref/voice/input/#agents.voice.input.AudioInput","title":"AudioInput  <code>dataclass</code>","text":"<p>Static audio to be used as input for the VoicePipeline.</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>@dataclass\nclass AudioInput:\n    \"\"\"Static audio to be used as input for the VoicePipeline.\"\"\"\n\n    buffer: npt.NDArray[np.int16 | np.float32]\n    \"\"\"\n    A buffer containing the audio data for the agent. Must be a numpy array of int16 or float32.\n    \"\"\"\n\n    frame_rate: int = DEFAULT_SAMPLE_RATE\n    \"\"\"The sample rate of the audio data. Defaults to 24000.\"\"\"\n\n    sample_width: int = 2\n    \"\"\"The sample width of the audio data. Defaults to 2.\"\"\"\n\n    channels: int = 1\n    \"\"\"The number of channels in the audio data. Defaults to 1.\"\"\"\n\n    def to_audio_file(self) -&gt; tuple[str, io.BytesIO, str]:\n        \"\"\"Returns a tuple of (filename, bytes, content_type)\"\"\"\n        return _buffer_to_audio_file(self.buffer, self.frame_rate, self.sample_width, self.channels)\n\n    def to_base64(self) -&gt; str:\n        \"\"\"Returns the audio data as a base64 encoded string.\"\"\"\n        if self.buffer.dtype == np.float32:\n            # convert to int16\n            self.buffer = np.clip(self.buffer, -1.0, 1.0)\n            self.buffer = (self.buffer * 32767).astype(np.int16)\n        elif self.buffer.dtype != np.int16:\n            raise UserError(\"Buffer must be a numpy array of int16 or float32\")\n\n        return base64.b64encode(self.buffer.tobytes()).decode(\"utf-8\")\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.buffer","title":"buffer  <code>instance-attribute</code>","text":"<pre><code>buffer: NDArray[int16 | float32]\n</code></pre> <p>A buffer containing the audio data for the agent. Must be a numpy array of int16 or float32.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.frame_rate","title":"frame_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frame_rate: int = DEFAULT_SAMPLE_RATE\n</code></pre> <p>The sample rate of the audio data. Defaults to 24000.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.sample_width","title":"sample_width  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_width: int = 2\n</code></pre> <p>The sample width of the audio data. Defaults to 2.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.channels","title":"channels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>channels: int = 1\n</code></pre> <p>The number of channels in the audio data. Defaults to 1.</p>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.to_audio_file","title":"to_audio_file","text":"<pre><code>to_audio_file() -&gt; tuple[str, BytesIO, str]\n</code></pre> <p>Returns a tuple of (filename, bytes, content_type)</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>def to_audio_file(self) -&gt; tuple[str, io.BytesIO, str]:\n    \"\"\"Returns a tuple of (filename, bytes, content_type)\"\"\"\n    return _buffer_to_audio_file(self.buffer, self.frame_rate, self.sample_width, self.channels)\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.AudioInput.to_base64","title":"to_base64","text":"<pre><code>to_base64() -&gt; str\n</code></pre> <p>Returns the audio data as a base64 encoded string.</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>def to_base64(self) -&gt; str:\n    \"\"\"Returns the audio data as a base64 encoded string.\"\"\"\n    if self.buffer.dtype == np.float32:\n        # convert to int16\n        self.buffer = np.clip(self.buffer, -1.0, 1.0)\n        self.buffer = (self.buffer * 32767).astype(np.int16)\n    elif self.buffer.dtype != np.int16:\n        raise UserError(\"Buffer must be a numpy array of int16 or float32\")\n\n    return base64.b64encode(self.buffer.tobytes()).decode(\"utf-8\")\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.StreamedAudioInput","title":"StreamedAudioInput","text":"<p>Audio input represented as a stream of audio data. You can pass this to the <code>VoicePipeline</code> and then push audio data into the queue using the <code>add_audio</code> method.</p> Source code in <code>src/agents/voice/input.py</code> <pre><code>class StreamedAudioInput:\n    \"\"\"Audio input represented as a stream of audio data. You can pass this to the `VoicePipeline`\n    and then push audio data into the queue using the `add_audio` method.\n    \"\"\"\n\n    def __init__(self):\n        self.queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32]] = asyncio.Queue()\n\n    async def add_audio(self, audio: npt.NDArray[np.int16 | np.float32]):\n        \"\"\"Adds more audio data to the stream.\n\n        Args:\n            audio: The audio data to add. Must be a numpy array of int16 or float32.\n        \"\"\"\n        await self.queue.put(audio)\n</code></pre>"},{"location":"ref/voice/input/#agents.voice.input.StreamedAudioInput.add_audio","title":"add_audio  <code>async</code>","text":"<pre><code>add_audio(audio: NDArray[int16 | float32])\n</code></pre> <p>Adds more audio data to the stream.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>NDArray[int16 | float32]</code> <p>The audio data to add. Must be a numpy array of int16 or float32.</p> required Source code in <code>src/agents/voice/input.py</code> <pre><code>async def add_audio(self, audio: npt.NDArray[np.int16 | np.float32]):\n    \"\"\"Adds more audio data to the stream.\n\n    Args:\n        audio: The audio data to add. Must be a numpy array of int16 or float32.\n    \"\"\"\n    await self.queue.put(audio)\n</code></pre>"},{"location":"ref/voice/model/","title":"<code>Model</code>","text":""},{"location":"ref/voice/model/#agents.voice.model.TTSVoice","title":"TTSVoice  <code>module-attribute</code>","text":"<pre><code>TTSVoice = Literal[\n    \"alloy\",\n    \"ash\",\n    \"coral\",\n    \"echo\",\n    \"fable\",\n    \"onyx\",\n    \"nova\",\n    \"sage\",\n    \"shimmer\",\n]\n</code></pre> <p>Exportable type for the TTSModelSettings voice enum</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings","title":"TTSModelSettings  <code>dataclass</code>","text":"<p>Settings for a TTS model.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@dataclass\nclass TTSModelSettings:\n    \"\"\"Settings for a TTS model.\"\"\"\n\n    voice: TTSVoice | None = None\n    \"\"\"\n    The voice to use for the TTS model. If not provided, the default voice for the respective model\n    will be used.\n    \"\"\"\n\n    buffer_size: int = 120\n    \"\"\"The minimal size of the chunks of audio data that are being streamed out.\"\"\"\n\n    dtype: npt.DTypeLike = np.int16\n    \"\"\"The data type for the audio data to be returned in.\"\"\"\n\n    transform_data: (\n        Callable[[npt.NDArray[np.int16 | np.float32]], npt.NDArray[np.int16 | np.float32]] | None\n    ) = None\n    \"\"\"\n    A function to transform the data from the TTS model. This is useful if you want the resulting\n    audio stream to have the data in a specific shape already.\n    \"\"\"\n\n    instructions: str = (\n        \"You will receive partial sentences. Do not complete the sentence just read out the text.\"\n    )\n    \"\"\"\n    The instructions to use for the TTS model. This is useful if you want to control the tone of the\n    audio output.\n    \"\"\"\n\n    text_splitter: Callable[[str], tuple[str, str]] = get_sentence_based_splitter()\n    \"\"\"\n    A function to split the text into chunks. This is useful if you want to split the text into\n    chunks before sending it to the TTS model rather than waiting for the whole text to be\n    processed.\n    \"\"\"\n\n    speed: float | None = None\n    \"\"\"The speed with which the TTS model will read the text. Between 0.25 and 4.0.\"\"\"\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.voice","title":"voice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>voice: TTSVoice | None = None\n</code></pre> <p>The voice to use for the TTS model. If not provided, the default voice for the respective model will be used.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.buffer_size","title":"buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer_size: int = 120\n</code></pre> <p>The minimal size of the chunks of audio data that are being streamed out.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.dtype","title":"dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dtype: DTypeLike = int16\n</code></pre> <p>The data type for the audio data to be returned in.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.transform_data","title":"transform_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transform_data: (\n    Callable[\n        [NDArray[int16 | float32]], NDArray[int16 | float32]\n    ]\n    | None\n) = None\n</code></pre> <p>A function to transform the data from the TTS model. This is useful if you want the resulting audio stream to have the data in a specific shape already.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.instructions","title":"instructions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instructions: str = \"You will receive partial sentences. Do not complete the sentence just read out the text.\"\n</code></pre> <p>The instructions to use for the TTS model. This is useful if you want to control the tone of the audio output.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.text_splitter","title":"text_splitter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_splitter: Callable[[str], tuple[str, str]] = (\n    get_sentence_based_splitter()\n)\n</code></pre> <p>A function to split the text into chunks. This is useful if you want to split the text into chunks before sending it to the TTS model rather than waiting for the whole text to be processed.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModelSettings.speed","title":"speed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>speed: float | None = None\n</code></pre> <p>The speed with which the TTS model will read the text. Between 0.25 and 4.0.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModel","title":"TTSModel","text":"<p>               Bases: <code>ABC</code></p> <p>A text-to-speech model that can convert text into audio output.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class TTSModel(abc.ABC):\n    \"\"\"A text-to-speech model that can convert text into audio output.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"The name of the TTS model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n        \"\"\"Given a text string, produces a stream of audio bytes, in PCM format.\n\n        Args:\n            text: The text to convert to audio.\n\n        Returns:\n            An async iterator of audio bytes, in PCM format.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.TTSModel.model_name","title":"model_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>The name of the TTS model.</p>"},{"location":"ref/voice/model/#agents.voice.model.TTSModel.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(\n    text: str, settings: TTSModelSettings\n) -&gt; AsyncIterator[bytes]\n</code></pre> <p>Given a text string, produces a stream of audio bytes, in PCM format.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to convert to audio.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[bytes]</code> <p>An async iterator of audio bytes, in PCM format.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n    \"\"\"Given a text string, produces a stream of audio bytes, in PCM format.\n\n    Args:\n        text: The text to convert to audio.\n\n    Returns:\n        An async iterator of audio bytes, in PCM format.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.StreamedTranscriptionSession","title":"StreamedTranscriptionSession","text":"<p>               Bases: <code>ABC</code></p> <p>A streamed transcription of audio input.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class StreamedTranscriptionSession(abc.ABC):\n    \"\"\"A streamed transcription of audio input.\"\"\"\n\n    @abc.abstractmethod\n    def transcribe_turns(self) -&gt; AsyncIterator[str]:\n        \"\"\"Yields a stream of text transcriptions. Each transcription is a turn in the conversation.\n\n        This method is expected to return only after `close()` is called.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def close(self) -&gt; None:\n        \"\"\"Closes the session.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.StreamedTranscriptionSession.transcribe_turns","title":"transcribe_turns  <code>abstractmethod</code>","text":"<pre><code>transcribe_turns() -&gt; AsyncIterator[str]\n</code></pre> <p>Yields a stream of text transcriptions. Each transcription is a turn in the conversation.</p> <p>This method is expected to return only after <code>close()</code> is called.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef transcribe_turns(self) -&gt; AsyncIterator[str]:\n    \"\"\"Yields a stream of text transcriptions. Each transcription is a turn in the conversation.\n\n    This method is expected to return only after `close()` is called.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.StreamedTranscriptionSession.close","title":"close  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the session.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Closes the session.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings","title":"STTModelSettings  <code>dataclass</code>","text":"<p>Settings for a speech-to-text model.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@dataclass\nclass STTModelSettings:\n    \"\"\"Settings for a speech-to-text model.\"\"\"\n\n    prompt: str | None = None\n    \"\"\"Instructions for the model to follow.\"\"\"\n\n    language: str | None = None\n    \"\"\"The language of the audio input.\"\"\"\n\n    temperature: float | None = None\n    \"\"\"The temperature of the model.\"\"\"\n\n    turn_detection: dict[str, Any] | None = None\n    \"\"\"The turn detection settings for the model when using streamed audio input.\"\"\"\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.prompt","title":"prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt: str | None = None\n</code></pre> <p>Instructions for the model to follow.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.language","title":"language  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>language: str | None = None\n</code></pre> <p>The language of the audio input.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float | None = None\n</code></pre> <p>The temperature of the model.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModelSettings.turn_detection","title":"turn_detection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>turn_detection: dict[str, Any] | None = None\n</code></pre> <p>The turn detection settings for the model when using streamed audio input.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModel","title":"STTModel","text":"<p>               Bases: <code>ABC</code></p> <p>A speech-to-text model that can convert audio input into text.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class STTModel(abc.ABC):\n    \"\"\"A speech-to-text model that can convert audio input into text.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def model_name(self) -&gt; str:\n        \"\"\"The name of the STT model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def transcribe(\n        self,\n        input: AudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; str:\n        \"\"\"Given an audio input, produces a text transcription.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n            trace_include_sensitive_data: Whether to include sensitive data in traces.\n            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n        Returns:\n            The text transcription of the audio input.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def create_session(\n        self,\n        input: StreamedAudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; StreamedTranscriptionSession:\n        \"\"\"Creates a new transcription session, which you can push audio to, and receive a stream\n        of text transcriptions.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n            trace_include_sensitive_data: Whether to include sensitive data in traces.\n            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n        Returns:\n            A new transcription session.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModel.model_name","title":"model_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>The name of the STT model.</p>"},{"location":"ref/voice/model/#agents.voice.model.STTModel.transcribe","title":"transcribe  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>transcribe(\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str\n</code></pre> <p>Given an audio input, produces a text transcription.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>AudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <code>trace_include_sensitive_data</code> <code>bool</code> <p>Whether to include sensitive data in traces.</p> required <code>trace_include_sensitive_audio_data</code> <code>bool</code> <p>Whether to include sensitive audio data in traces.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text transcription of the audio input.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\nasync def transcribe(\n    self,\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str:\n    \"\"\"Given an audio input, produces a text transcription.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n        trace_include_sensitive_data: Whether to include sensitive data in traces.\n        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n    Returns:\n        The text transcription of the audio input.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.STTModel.create_session","title":"create_session  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>create_session(\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession\n</code></pre> <p>Creates a new transcription session, which you can push audio to, and receive a stream of text transcriptions.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StreamedAudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <code>trace_include_sensitive_data</code> <code>bool</code> <p>Whether to include sensitive data in traces.</p> required <code>trace_include_sensitive_audio_data</code> <code>bool</code> <p>Whether to include sensitive audio data in traces.</p> required <p>Returns:</p> Type Description <code>StreamedTranscriptionSession</code> <p>A new transcription session.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\nasync def create_session(\n    self,\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession:\n    \"\"\"Creates a new transcription session, which you can push audio to, and receive a stream\n    of text transcriptions.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n        trace_include_sensitive_data: Whether to include sensitive data in traces.\n        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n    Returns:\n        A new transcription session.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.VoiceModelProvider","title":"VoiceModelProvider","text":"<p>               Bases: <code>ABC</code></p> <p>The base interface for a voice model provider.</p> <p>A model provider is responsible for creating speech-to-text and text-to-speech models, given a name.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>class VoiceModelProvider(abc.ABC):\n    \"\"\"The base interface for a voice model provider.\n\n    A model provider is responsible for creating speech-to-text and text-to-speech models, given a\n    name.\n    \"\"\"\n\n    @abc.abstractmethod\n    def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n        \"\"\"Get a speech-to-text model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The speech-to-text model.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n        \"\"\"Get a text-to-speech model by name.\"\"\"\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.VoiceModelProvider.get_stt_model","title":"get_stt_model  <code>abstractmethod</code>","text":"<pre><code>get_stt_model(model_name: str | None) -&gt; STTModel\n</code></pre> <p>Get a speech-to-text model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>STTModel</code> <p>The speech-to-text model.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef get_stt_model(self, model_name: str | None) -&gt; STTModel:\n    \"\"\"Get a speech-to-text model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The speech-to-text model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/model/#agents.voice.model.VoiceModelProvider.get_tts_model","title":"get_tts_model  <code>abstractmethod</code>","text":"<pre><code>get_tts_model(model_name: str | None) -&gt; TTSModel\n</code></pre> <p>Get a text-to-speech model by name.</p> Source code in <code>src/agents/voice/model.py</code> <pre><code>@abc.abstractmethod\ndef get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n    \"\"\"Get a text-to-speech model by name.\"\"\"\n</code></pre>"},{"location":"ref/voice/pipeline/","title":"<code>Pipeline</code>","text":""},{"location":"ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline","title":"VoicePipeline","text":"<p>An opinionated voice agent pipeline. It works in three steps: 1. Transcribe audio input into text. 2. Run the provided <code>workflow</code>, which produces a sequence of text responses. 3. Convert the text responses into streaming audio output.</p> Source code in <code>src/agents/voice/pipeline.py</code> <pre><code>class VoicePipeline:\n    \"\"\"An opinionated voice agent pipeline. It works in three steps:\n    1. Transcribe audio input into text.\n    2. Run the provided `workflow`, which produces a sequence of text responses.\n    3. Convert the text responses into streaming audio output.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        workflow: VoiceWorkflowBase,\n        stt_model: STTModel | str | None = None,\n        tts_model: TTSModel | str | None = None,\n        config: VoicePipelineConfig | None = None,\n    ):\n        \"\"\"Create a new voice pipeline.\n\n        Args:\n            workflow: The workflow to run. See `VoiceWorkflowBase`.\n            stt_model: The speech-to-text model to use. If not provided, a default OpenAI\n                model will be used.\n            tts_model: The text-to-speech model to use. If not provided, a default OpenAI\n                model will be used.\n            config: The pipeline configuration. If not provided, a default configuration will be\n                used.\n        \"\"\"\n        self.workflow = workflow\n        self.stt_model = stt_model if isinstance(stt_model, STTModel) else None\n        self.tts_model = tts_model if isinstance(tts_model, TTSModel) else None\n        self._stt_model_name = stt_model if isinstance(stt_model, str) else None\n        self._tts_model_name = tts_model if isinstance(tts_model, str) else None\n        self.config = config or VoicePipelineConfig()\n\n    async def run(self, audio_input: AudioInput | StreamedAudioInput) -&gt; StreamedAudioResult:\n        \"\"\"Run the voice pipeline.\n\n        Args:\n            audio_input: The audio input to process. This can either be an `AudioInput` instance,\n                which is a single static buffer, or a `StreamedAudioInput` instance, which is a\n                stream of audio data that you can append to.\n\n        Returns:\n            A `StreamedAudioResult` instance. You can use this object to stream audio events and\n            play them out.\n        \"\"\"\n        if isinstance(audio_input, AudioInput):\n            return await self._run_single_turn(audio_input)\n        elif isinstance(audio_input, StreamedAudioInput):\n            return await self._run_multi_turn(audio_input)\n        else:\n            raise UserError(f\"Unsupported audio input type: {type(audio_input)}\")\n\n    def _get_tts_model(self) -&gt; TTSModel:\n        if not self.tts_model:\n            self.tts_model = self.config.model_provider.get_tts_model(self._tts_model_name)\n        return self.tts_model\n\n    def _get_stt_model(self) -&gt; STTModel:\n        if not self.stt_model:\n            self.stt_model = self.config.model_provider.get_stt_model(self._stt_model_name)\n        return self.stt_model\n\n    async def _process_audio_input(self, audio_input: AudioInput) -&gt; str:\n        model = self._get_stt_model()\n        return await model.transcribe(\n            audio_input,\n            self.config.stt_settings,\n            self.config.trace_include_sensitive_data,\n            self.config.trace_include_sensitive_audio_data,\n        )\n\n    async def _run_single_turn(self, audio_input: AudioInput) -&gt; StreamedAudioResult:\n        # Since this is single turn, we can use the TraceCtxManager to manage starting/ending the\n        # trace\n        with TraceCtxManager(\n            workflow_name=self.config.workflow_name or \"Voice Agent\",\n            trace_id=None,  # Automatically generated\n            group_id=self.config.group_id,\n            metadata=self.config.trace_metadata,\n            disabled=self.config.tracing_disabled,\n        ):\n            input_text = await self._process_audio_input(audio_input)\n\n            output = StreamedAudioResult(\n                self._get_tts_model(), self.config.tts_settings, self.config\n            )\n\n            async def stream_events():\n                try:\n                    async for text_event in self.workflow.run(input_text):\n                        await output._add_text(text_event)\n                    await output._turn_done()\n                    await output._done()\n                except Exception as e:\n                    logger.error(f\"Error processing single turn: {e}\")\n                    await output._add_error(e)\n                    raise e\n\n            output._set_task(asyncio.create_task(stream_events()))\n            return output\n\n    async def _run_multi_turn(self, audio_input: StreamedAudioInput) -&gt; StreamedAudioResult:\n        with TraceCtxManager(\n            workflow_name=self.config.workflow_name or \"Voice Agent\",\n            trace_id=None,\n            group_id=self.config.group_id,\n            metadata=self.config.trace_metadata,\n            disabled=self.config.tracing_disabled,\n        ):\n            output = StreamedAudioResult(\n                self._get_tts_model(), self.config.tts_settings, self.config\n            )\n\n            try:\n                async for intro_text in self.workflow.on_start():\n                    await output._add_text(intro_text)\n            except Exception as e:\n                logger.warning(f\"on_start() failed: {e}\")\n\n            transcription_session = await self._get_stt_model().create_session(\n                audio_input,\n                self.config.stt_settings,\n                self.config.trace_include_sensitive_data,\n                self.config.trace_include_sensitive_audio_data,\n            )\n\n            async def process_turns():\n                try:\n                    async for input_text in transcription_session.transcribe_turns():\n                        result = self.workflow.run(input_text)\n                        async for text_event in result:\n                            await output._add_text(text_event)\n                        await output._turn_done()\n                except Exception as e:\n                    logger.error(f\"Error processing turns: {e}\")\n                    await output._add_error(e)\n                    raise e\n                finally:\n                    await transcription_session.close()\n                    await output._done()\n\n            output._set_task(asyncio.create_task(process_turns()))\n            return output\n</code></pre>"},{"location":"ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    workflow: VoiceWorkflowBase,\n    stt_model: STTModel | str | None = None,\n    tts_model: TTSModel | str | None = None,\n    config: VoicePipelineConfig | None = None,\n)\n</code></pre> <p>Create a new voice pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>VoiceWorkflowBase</code> <p>The workflow to run. See <code>VoiceWorkflowBase</code>.</p> required <code>stt_model</code> <code>STTModel | str | None</code> <p>The speech-to-text model to use. If not provided, a default OpenAI model will be used.</p> <code>None</code> <code>tts_model</code> <code>TTSModel | str | None</code> <p>The text-to-speech model to use. If not provided, a default OpenAI model will be used.</p> <code>None</code> <code>config</code> <code>VoicePipelineConfig | None</code> <p>The pipeline configuration. If not provided, a default configuration will be used.</p> <code>None</code> Source code in <code>src/agents/voice/pipeline.py</code> <pre><code>def __init__(\n    self,\n    *,\n    workflow: VoiceWorkflowBase,\n    stt_model: STTModel | str | None = None,\n    tts_model: TTSModel | str | None = None,\n    config: VoicePipelineConfig | None = None,\n):\n    \"\"\"Create a new voice pipeline.\n\n    Args:\n        workflow: The workflow to run. See `VoiceWorkflowBase`.\n        stt_model: The speech-to-text model to use. If not provided, a default OpenAI\n            model will be used.\n        tts_model: The text-to-speech model to use. If not provided, a default OpenAI\n            model will be used.\n        config: The pipeline configuration. If not provided, a default configuration will be\n            used.\n    \"\"\"\n    self.workflow = workflow\n    self.stt_model = stt_model if isinstance(stt_model, STTModel) else None\n    self.tts_model = tts_model if isinstance(tts_model, TTSModel) else None\n    self._stt_model_name = stt_model if isinstance(stt_model, str) else None\n    self._tts_model_name = tts_model if isinstance(tts_model, str) else None\n    self.config = config or VoicePipelineConfig()\n</code></pre>"},{"location":"ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    audio_input: AudioInput | StreamedAudioInput,\n) -&gt; StreamedAudioResult\n</code></pre> <p>Run the voice pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>audio_input</code> <code>AudioInput | StreamedAudioInput</code> <p>The audio input to process. This can either be an <code>AudioInput</code> instance, which is a single static buffer, or a <code>StreamedAudioInput</code> instance, which is a stream of audio data that you can append to.</p> required <p>Returns:</p> Type Description <code>StreamedAudioResult</code> <p>A <code>StreamedAudioResult</code> instance. You can use this object to stream audio events and</p> <code>StreamedAudioResult</code> <p>play them out.</p> Source code in <code>src/agents/voice/pipeline.py</code> <pre><code>async def run(self, audio_input: AudioInput | StreamedAudioInput) -&gt; StreamedAudioResult:\n    \"\"\"Run the voice pipeline.\n\n    Args:\n        audio_input: The audio input to process. This can either be an `AudioInput` instance,\n            which is a single static buffer, or a `StreamedAudioInput` instance, which is a\n            stream of audio data that you can append to.\n\n    Returns:\n        A `StreamedAudioResult` instance. You can use this object to stream audio events and\n        play them out.\n    \"\"\"\n    if isinstance(audio_input, AudioInput):\n        return await self._run_single_turn(audio_input)\n    elif isinstance(audio_input, StreamedAudioInput):\n        return await self._run_multi_turn(audio_input)\n    else:\n        raise UserError(f\"Unsupported audio input type: {type(audio_input)}\")\n</code></pre>"},{"location":"ref/voice/pipeline_config/","title":"<code>Pipeline Config</code>","text":""},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig","title":"VoicePipelineConfig  <code>dataclass</code>","text":"<p>Configuration for a <code>VoicePipeline</code>.</p> Source code in <code>src/agents/voice/pipeline_config.py</code> <pre><code>@dataclass\nclass VoicePipelineConfig:\n    \"\"\"Configuration for a `VoicePipeline`.\"\"\"\n\n    model_provider: VoiceModelProvider = field(default_factory=OpenAIVoiceModelProvider)\n    \"\"\"The voice model provider to use for the pipeline. Defaults to OpenAI.\"\"\"\n\n    tracing_disabled: bool = False\n    \"\"\"Whether to disable tracing of the pipeline. Defaults to `False`.\"\"\"\n\n    trace_include_sensitive_data: bool = True\n    \"\"\"Whether to include sensitive data in traces. Defaults to `True`. This is specifically for the\n      voice pipeline, and not for anything that goes on inside your Workflow.\"\"\"\n\n    trace_include_sensitive_audio_data: bool = True\n    \"\"\"Whether to include audio data in traces. Defaults to `True`.\"\"\"\n\n    workflow_name: str = \"Voice Agent\"\n    \"\"\"The name of the workflow to use for tracing. Defaults to `Voice Agent`.\"\"\"\n\n    group_id: str = field(default_factory=gen_group_id)\n    \"\"\"\n    A grouping identifier to use for tracing, to link multiple traces from the same conversation\n    or process. If not provided, we will create a random group ID.\n    \"\"\"\n\n    trace_metadata: dict[str, Any] | None = None\n    \"\"\"\n    An optional dictionary of additional metadata to include with the trace.\n    \"\"\"\n\n    stt_settings: STTModelSettings = field(default_factory=STTModelSettings)\n    \"\"\"The settings to use for the STT model.\"\"\"\n\n    tts_settings: TTSModelSettings = field(default_factory=TTSModelSettings)\n    \"\"\"The settings to use for the TTS model.\"\"\"\n</code></pre>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.model_provider","title":"model_provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_provider: VoiceModelProvider = field(\n    default_factory=OpenAIVoiceModelProvider\n)\n</code></pre> <p>The voice model provider to use for the pipeline. Defaults to OpenAI.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.tracing_disabled","title":"tracing_disabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tracing_disabled: bool = False\n</code></pre> <p>Whether to disable tracing of the pipeline. Defaults to <code>False</code>.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_data","title":"trace_include_sensitive_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_data: bool = True\n</code></pre> <p>Whether to include sensitive data in traces. Defaults to <code>True</code>. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data","title":"trace_include_sensitive_audio_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_include_sensitive_audio_data: bool = True\n</code></pre> <p>Whether to include audio data in traces. Defaults to <code>True</code>.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.workflow_name","title":"workflow_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>workflow_name: str = 'Voice Agent'\n</code></pre> <p>The name of the workflow to use for tracing. Defaults to <code>Voice Agent</code>.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.group_id","title":"group_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_id: str = field(default_factory=gen_group_id)\n</code></pre> <p>A grouping identifier to use for tracing, to link multiple traces from the same conversation or process. If not provided, we will create a random group ID.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_metadata","title":"trace_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_metadata: dict[str, Any] | None = None\n</code></pre> <p>An optional dictionary of additional metadata to include with the trace.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.stt_settings","title":"stt_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stt_settings: STTModelSettings = field(\n    default_factory=STTModelSettings\n)\n</code></pre> <p>The settings to use for the STT model.</p>"},{"location":"ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.tts_settings","title":"tts_settings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tts_settings: TTSModelSettings = field(\n    default_factory=TTSModelSettings\n)\n</code></pre> <p>The settings to use for the TTS model.</p>"},{"location":"ref/voice/result/","title":"<code>Result</code>","text":""},{"location":"ref/voice/result/#agents.voice.result.StreamedAudioResult","title":"StreamedAudioResult","text":"<p>The output of a <code>VoicePipeline</code>. Streams events and audio data as they're generated.</p> Source code in <code>src/agents/voice/result.py</code> <pre><code>class StreamedAudioResult:\n    \"\"\"The output of a `VoicePipeline`. Streams events and audio data as they're generated.\"\"\"\n\n    def __init__(\n        self,\n        tts_model: TTSModel,\n        tts_settings: TTSModelSettings,\n        voice_pipeline_config: VoicePipelineConfig,\n    ):\n        \"\"\"Create a new `StreamedAudioResult` instance.\n\n        Args:\n            tts_model: The TTS model to use.\n            tts_settings: The TTS settings to use.\n            voice_pipeline_config: The voice pipeline config to use.\n        \"\"\"\n        self.tts_model = tts_model\n        self.tts_settings = tts_settings\n        self.total_output_text = \"\"\n        self.instructions = tts_settings.instructions\n        self.text_generation_task: asyncio.Task[Any] | None = None\n\n        self._voice_pipeline_config = voice_pipeline_config\n        self._text_buffer = \"\"\n        self._turn_text_buffer = \"\"\n        self._queue: asyncio.Queue[VoiceStreamEvent] = asyncio.Queue()\n        self._tasks: list[asyncio.Task[Any]] = []\n        self._ordered_tasks: list[\n            asyncio.Queue[VoiceStreamEvent | None]\n        ] = []  # New: list to hold local queues for each text segment\n        self._dispatcher_task: asyncio.Task[Any] | None = (\n            None  # Task to dispatch audio chunks in order\n        )\n\n        self._done_processing = False\n        self._buffer_size = tts_settings.buffer_size\n        self._started_processing_turn = False\n        self._first_byte_received = False\n        self._generation_start_time: str | None = None\n        self._completed_session = False\n        self._stored_exception: BaseException | None = None\n        self._tracing_span: Span[SpeechGroupSpanData] | None = None\n\n    async def _start_turn(self):\n        if self._started_processing_turn:\n            return\n\n        self._tracing_span = speech_group_span()\n        self._tracing_span.start()\n        self._started_processing_turn = True\n        self._first_byte_received = False\n        self._generation_start_time = time_iso()\n        await self._queue.put(VoiceStreamEventLifecycle(event=\"turn_started\"))\n\n    def _set_task(self, task: asyncio.Task[Any]):\n        self.text_generation_task = task\n\n    async def _add_error(self, error: Exception):\n        await self._queue.put(VoiceStreamEventError(error))\n\n    def _transform_audio_buffer(\n        self, buffer: list[bytes], output_dtype: npt.DTypeLike\n    ) -&gt; npt.NDArray[np.int16 | np.float32]:\n        np_array = np.frombuffer(b\"\".join(buffer), dtype=np.int16)\n\n        if output_dtype == np.int16:\n            return np_array\n        elif output_dtype == np.float32:\n            return (np_array.astype(np.float32) / 32767.0).reshape(-1, 1)\n        else:\n            raise UserError(\"Invalid output dtype\")\n\n    async def _stream_audio(\n        self,\n        text: str,\n        local_queue: asyncio.Queue[VoiceStreamEvent | None],\n        finish_turn: bool = False,\n    ):\n        with speech_span(\n            model=self.tts_model.model_name,\n            input=text if self._voice_pipeline_config.trace_include_sensitive_data else \"\",\n            model_config={\n                \"voice\": self.tts_settings.voice,\n                \"instructions\": self.instructions,\n                \"speed\": self.tts_settings.speed,\n            },\n            output_format=\"pcm\",\n            parent=self._tracing_span,\n        ) as tts_span:\n            try:\n                first_byte_received = False\n                buffer: list[bytes] = []\n                full_audio_data: list[bytes] = []\n\n                async for chunk in self.tts_model.run(text, self.tts_settings):\n                    if not first_byte_received:\n                        first_byte_received = True\n                        tts_span.span_data.first_content_at = time_iso()\n\n                    if chunk:\n                        buffer.append(chunk)\n                        full_audio_data.append(chunk)\n                        if len(buffer) &gt;= self._buffer_size:\n                            audio_np = self._transform_audio_buffer(buffer, self.tts_settings.dtype)\n                            if self.tts_settings.transform_data:\n                                audio_np = self.tts_settings.transform_data(audio_np)\n                            await local_queue.put(\n                                VoiceStreamEventAudio(data=audio_np)\n                            )  # Use local queue\n                            buffer = []\n                if buffer:\n                    audio_np = self._transform_audio_buffer(buffer, self.tts_settings.dtype)\n                    if self.tts_settings.transform_data:\n                        audio_np = self.tts_settings.transform_data(audio_np)\n                    await local_queue.put(VoiceStreamEventAudio(data=audio_np))  # Use local queue\n\n                if self._voice_pipeline_config.trace_include_sensitive_audio_data:\n                    tts_span.span_data.output = _audio_to_base64(full_audio_data)\n                else:\n                    tts_span.span_data.output = \"\"\n\n                if finish_turn:\n                    await local_queue.put(VoiceStreamEventLifecycle(event=\"turn_ended\"))\n                else:\n                    await local_queue.put(None)  # Signal completion for this segment\n            except Exception as e:\n                tts_span.set_error(\n                    {\n                        \"message\": str(e),\n                        \"data\": {\n                            \"text\": text\n                            if self._voice_pipeline_config.trace_include_sensitive_data\n                            else \"\",\n                        },\n                    }\n                )\n                logger.error(f\"Error streaming audio: {e}\")\n\n                # Signal completion for whole session because of error\n                await local_queue.put(VoiceStreamEventLifecycle(event=\"session_ended\"))\n                raise e\n\n    async def _add_text(self, text: str):\n        await self._start_turn()\n\n        self._text_buffer += text\n        self.total_output_text += text\n        self._turn_text_buffer += text\n\n        combined_sentences, self._text_buffer = self.tts_settings.text_splitter(self._text_buffer)\n\n        if len(combined_sentences) &gt;= 20:\n            local_queue: asyncio.Queue[VoiceStreamEvent | None] = asyncio.Queue()\n            self._ordered_tasks.append(local_queue)\n            self._tasks.append(\n                asyncio.create_task(self._stream_audio(combined_sentences, local_queue))\n            )\n            if self._dispatcher_task is None:\n                self._dispatcher_task = asyncio.create_task(self._dispatch_audio())\n\n    async def _turn_done(self):\n        if self._text_buffer:\n            local_queue: asyncio.Queue[VoiceStreamEvent | None] = asyncio.Queue()\n            self._ordered_tasks.append(local_queue)  # Append the local queue for the final segment\n            self._tasks.append(\n                asyncio.create_task(\n                    self._stream_audio(self._text_buffer, local_queue, finish_turn=True)\n                )\n            )\n            self._text_buffer = \"\"\n        self._done_processing = True\n        if self._dispatcher_task is None:\n            self._dispatcher_task = asyncio.create_task(self._dispatch_audio())\n        await asyncio.gather(*self._tasks)\n\n    def _finish_turn(self):\n        if self._tracing_span:\n            if self._voice_pipeline_config.trace_include_sensitive_data:\n                self._tracing_span.span_data.input = self._turn_text_buffer\n            else:\n                self._tracing_span.span_data.input = \"\"\n\n            self._tracing_span.finish()\n            self._tracing_span = None\n        self._turn_text_buffer = \"\"\n        self._started_processing_turn = False\n\n    async def _done(self):\n        self._completed_session = True\n        await self._wait_for_completion()\n\n    async def _dispatch_audio(self):\n        # Dispatch audio chunks from each segment in the order they were added\n        while True:\n            if len(self._ordered_tasks) == 0:\n                if self._completed_session:\n                    break\n                await asyncio.sleep(0)\n                continue\n            local_queue = self._ordered_tasks.pop(0)\n            while True:\n                chunk = await local_queue.get()\n                if chunk is None:\n                    break\n                await self._queue.put(chunk)\n                if isinstance(chunk, VoiceStreamEventLifecycle):\n                    local_queue.task_done()\n                    if chunk.event == \"turn_ended\":\n                        self._finish_turn()\n                        break\n        await self._queue.put(VoiceStreamEventLifecycle(event=\"session_ended\"))\n\n    async def _wait_for_completion(self):\n        tasks: list[asyncio.Task[Any]] = self._tasks\n        if self._dispatcher_task is not None:\n            tasks.append(self._dispatcher_task)\n        await asyncio.gather(*tasks)\n\n    def _cleanup_tasks(self):\n        self._finish_turn()\n\n        for task in self._tasks:\n            if not task.done():\n                task.cancel()\n\n        if self._dispatcher_task and not self._dispatcher_task.done():\n            self._dispatcher_task.cancel()\n\n        if self.text_generation_task and not self.text_generation_task.done():\n            self.text_generation_task.cancel()\n\n    def _check_errors(self):\n        for task in self._tasks:\n            if task.done():\n                if task.exception():\n                    self._stored_exception = task.exception()\n                    break\n\n    async def stream(self) -&gt; AsyncIterator[VoiceStreamEvent]:\n        \"\"\"Stream the events and audio data as they're generated.\"\"\"\n        while True:\n            try:\n                event = await self._queue.get()\n            except asyncio.CancelledError:\n                break\n            if isinstance(event, VoiceStreamEventError):\n                self._stored_exception = event.error\n                logger.error(f\"Error processing output: {event.error}\")\n                break\n            if event is None:\n                break\n            yield event\n            if event.type == \"voice_stream_event_lifecycle\" and event.event == \"session_ended\":\n                break\n\n        self._check_errors()\n        self._cleanup_tasks()\n\n        if self._stored_exception:\n            raise self._stored_exception\n</code></pre>"},{"location":"ref/voice/result/#agents.voice.result.StreamedAudioResult.__init__","title":"__init__","text":"<pre><code>__init__(\n    tts_model: TTSModel,\n    tts_settings: TTSModelSettings,\n    voice_pipeline_config: VoicePipelineConfig,\n)\n</code></pre> <p>Create a new <code>StreamedAudioResult</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>tts_model</code> <code>TTSModel</code> <p>The TTS model to use.</p> required <code>tts_settings</code> <code>TTSModelSettings</code> <p>The TTS settings to use.</p> required <code>voice_pipeline_config</code> <code>VoicePipelineConfig</code> <p>The voice pipeline config to use.</p> required Source code in <code>src/agents/voice/result.py</code> <pre><code>def __init__(\n    self,\n    tts_model: TTSModel,\n    tts_settings: TTSModelSettings,\n    voice_pipeline_config: VoicePipelineConfig,\n):\n    \"\"\"Create a new `StreamedAudioResult` instance.\n\n    Args:\n        tts_model: The TTS model to use.\n        tts_settings: The TTS settings to use.\n        voice_pipeline_config: The voice pipeline config to use.\n    \"\"\"\n    self.tts_model = tts_model\n    self.tts_settings = tts_settings\n    self.total_output_text = \"\"\n    self.instructions = tts_settings.instructions\n    self.text_generation_task: asyncio.Task[Any] | None = None\n\n    self._voice_pipeline_config = voice_pipeline_config\n    self._text_buffer = \"\"\n    self._turn_text_buffer = \"\"\n    self._queue: asyncio.Queue[VoiceStreamEvent] = asyncio.Queue()\n    self._tasks: list[asyncio.Task[Any]] = []\n    self._ordered_tasks: list[\n        asyncio.Queue[VoiceStreamEvent | None]\n    ] = []  # New: list to hold local queues for each text segment\n    self._dispatcher_task: asyncio.Task[Any] | None = (\n        None  # Task to dispatch audio chunks in order\n    )\n\n    self._done_processing = False\n    self._buffer_size = tts_settings.buffer_size\n    self._started_processing_turn = False\n    self._first_byte_received = False\n    self._generation_start_time: str | None = None\n    self._completed_session = False\n    self._stored_exception: BaseException | None = None\n    self._tracing_span: Span[SpeechGroupSpanData] | None = None\n</code></pre>"},{"location":"ref/voice/result/#agents.voice.result.StreamedAudioResult.stream","title":"stream  <code>async</code>","text":"<pre><code>stream() -&gt; AsyncIterator[VoiceStreamEvent]\n</code></pre> <p>Stream the events and audio data as they're generated.</p> Source code in <code>src/agents/voice/result.py</code> <pre><code>async def stream(self) -&gt; AsyncIterator[VoiceStreamEvent]:\n    \"\"\"Stream the events and audio data as they're generated.\"\"\"\n    while True:\n        try:\n            event = await self._queue.get()\n        except asyncio.CancelledError:\n            break\n        if isinstance(event, VoiceStreamEventError):\n            self._stored_exception = event.error\n            logger.error(f\"Error processing output: {event.error}\")\n            break\n        if event is None:\n            break\n        yield event\n        if event.type == \"voice_stream_event_lifecycle\" and event.event == \"session_ended\":\n            break\n\n    self._check_errors()\n    self._cleanup_tasks()\n\n    if self._stored_exception:\n        raise self._stored_exception\n</code></pre>"},{"location":"ref/voice/utils/","title":"<code>Utils</code>","text":""},{"location":"ref/voice/utils/#agents.voice.utils.get_sentence_based_splitter","title":"get_sentence_based_splitter","text":"<pre><code>get_sentence_based_splitter(\n    min_sentence_length: int = 20,\n) -&gt; Callable[[str], tuple[str, str]]\n</code></pre> <p>Returns a function that splits text into chunks based on sentence boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>min_sentence_length</code> <code>int</code> <p>The minimum length of a sentence to be included in a chunk.</p> <code>20</code> <p>Returns:</p> Type Description <code>Callable[[str], tuple[str, str]]</code> <p>A function that splits text into chunks based on sentence boundaries.</p> Source code in <code>src/agents/voice/utils.py</code> <pre><code>def get_sentence_based_splitter(\n    min_sentence_length: int = 20,\n) -&gt; Callable[[str], tuple[str, str]]:\n    \"\"\"Returns a function that splits text into chunks based on sentence boundaries.\n\n    Args:\n        min_sentence_length: The minimum length of a sentence to be included in a chunk.\n\n    Returns:\n        A function that splits text into chunks based on sentence boundaries.\n    \"\"\"\n\n    def sentence_based_text_splitter(text_buffer: str) -&gt; tuple[str, str]:\n        \"\"\"\n        A function to split the text into chunks. This is useful if you want to split the text into\n        chunks before sending it to the TTS model rather than waiting for the whole text to be\n        processed.\n\n        Args:\n            text_buffer: The text to split.\n\n        Returns:\n            A tuple of the text to process and the remaining text buffer.\n        \"\"\"\n        sentences = re.split(r\"(?&lt;=[.!?])\\s+\", text_buffer.strip())\n        if len(sentences) &gt;= 1:\n            combined_sentences = \" \".join(sentences[:-1])\n            if len(combined_sentences) &gt;= min_sentence_length:\n                remaining_text_buffer = sentences[-1]\n                return combined_sentences, remaining_text_buffer\n        return \"\", text_buffer\n\n    return sentence_based_text_splitter\n</code></pre>"},{"location":"ref/voice/workflow/","title":"<code>Workflow</code>","text":""},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowBase","title":"VoiceWorkflowBase","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for a voice workflow. You must implement the <code>run</code> method. A \"workflow\" is any code you want, that receives a transcription and yields text that will be turned into speech by a text-to-speech model. In most cases, you'll create <code>Agent</code>s and use <code>Runner.run_streamed()</code> to run them, returning some or all of the text events from the stream. You can use the <code>VoiceWorkflowHelper</code> class to help with extracting text events from the stream. If you have a simple workflow that has a single starting agent and no custom logic, you can use <code>SingleAgentVoiceWorkflow</code> directly.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class VoiceWorkflowBase(abc.ABC):\n    \"\"\"\n    A base class for a voice workflow. You must implement the `run` method. A \"workflow\" is any\n    code you want, that receives a transcription and yields text that will be turned into speech\n    by a text-to-speech model.\n    In most cases, you'll create `Agent`s and use `Runner.run_streamed()` to run them, returning\n    some or all of the text events from the stream. You can use the `VoiceWorkflowHelper` class to\n    help with extracting text events from the stream.\n    If you have a simple workflow that has a single starting agent and no custom logic, you can\n    use `SingleAgentVoiceWorkflow` directly.\n    \"\"\"\n\n    @abc.abstractmethod\n    def run(self, transcription: str) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Run the voice workflow. You will receive an input transcription, and must yield text that\n        will be spoken to the user. You can run whatever logic you want here. In most cases, the\n        final logic will involve calling `Runner.run_streamed()` and yielding any text events from\n        the stream.\n        \"\"\"\n        pass\n\n    async def on_start(self) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Optional method that runs before any user input is received. Can be used\n        to deliver a greeting or instruction via TTS. Defaults to doing nothing.\n        \"\"\"\n        return\n        yield\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowBase.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(transcription: str) -&gt; AsyncIterator[str]\n</code></pre> <p>Run the voice workflow. You will receive an input transcription, and must yield text that will be spoken to the user. You can run whatever logic you want here. In most cases, the final logic will involve calling <code>Runner.run_streamed()</code> and yielding any text events from the stream.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>@abc.abstractmethod\ndef run(self, transcription: str) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Run the voice workflow. You will receive an input transcription, and must yield text that\n    will be spoken to the user. You can run whatever logic you want here. In most cases, the\n    final logic will involve calling `Runner.run_streamed()` and yielding any text events from\n    the stream.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowBase.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start() -&gt; AsyncIterator[str]\n</code></pre> <p>Optional method that runs before any user input is received. Can be used to deliver a greeting or instruction via TTS. Defaults to doing nothing.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>async def on_start(self) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Optional method that runs before any user input is received. Can be used\n    to deliver a greeting or instruction via TTS. Defaults to doing nothing.\n    \"\"\"\n    return\n    yield\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowHelper","title":"VoiceWorkflowHelper","text":"Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class VoiceWorkflowHelper:\n    @classmethod\n    async def stream_text_from(cls, result: RunResultStreaming) -&gt; AsyncIterator[str]:\n        \"\"\"Wraps a `RunResultStreaming` object and yields text events from the stream.\"\"\"\n        async for event in result.stream_events():\n            if (\n                event.type == \"raw_response_event\"\n                and event.data.type == \"response.output_text.delta\"\n            ):\n                yield event.data.delta\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.VoiceWorkflowHelper.stream_text_from","title":"stream_text_from  <code>async</code> <code>classmethod</code>","text":"<pre><code>stream_text_from(\n    result: RunResultStreaming,\n) -&gt; AsyncIterator[str]\n</code></pre> <p>Wraps a <code>RunResultStreaming</code> object and yields text events from the stream.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>@classmethod\nasync def stream_text_from(cls, result: RunResultStreaming) -&gt; AsyncIterator[str]:\n    \"\"\"Wraps a `RunResultStreaming` object and yields text events from the stream.\"\"\"\n    async for event in result.stream_events():\n        if (\n            event.type == \"raw_response_event\"\n            and event.data.type == \"response.output_text.delta\"\n        ):\n            yield event.data.delta\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentWorkflowCallbacks","title":"SingleAgentWorkflowCallbacks","text":"Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class SingleAgentWorkflowCallbacks:\n    def on_run(self, workflow: SingleAgentVoiceWorkflow, transcription: str) -&gt; None:\n        \"\"\"Called when the workflow is run.\"\"\"\n        pass\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentWorkflowCallbacks.on_run","title":"on_run","text":"<pre><code>on_run(\n    workflow: SingleAgentVoiceWorkflow, transcription: str\n) -&gt; None\n</code></pre> <p>Called when the workflow is run.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>def on_run(self, workflow: SingleAgentVoiceWorkflow, transcription: str) -&gt; None:\n    \"\"\"Called when the workflow is run.\"\"\"\n    pass\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow","title":"SingleAgentVoiceWorkflow","text":"<p>               Bases: <code>VoiceWorkflowBase</code></p> <p>A simple voice workflow that runs a single agent. Each transcription and result is added to the input history. For more complex workflows (e.g. multiple Runner calls, custom message history, custom logic, custom configs), subclass <code>VoiceWorkflowBase</code> and implement your own logic.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>class SingleAgentVoiceWorkflow(VoiceWorkflowBase):\n    \"\"\"A simple voice workflow that runs a single agent. Each transcription and result is added to\n    the input history.\n    For more complex workflows (e.g. multiple Runner calls, custom message history, custom logic,\n    custom configs), subclass `VoiceWorkflowBase` and implement your own logic.\n    \"\"\"\n\n    def __init__(self, agent: Agent[Any], callbacks: SingleAgentWorkflowCallbacks | None = None):\n        \"\"\"Create a new single agent voice workflow.\n\n        Args:\n            agent: The agent to run.\n            callbacks: Optional callbacks to call during the workflow.\n        \"\"\"\n        self._input_history: list[TResponseInputItem] = []\n        self._current_agent = agent\n        self._callbacks = callbacks\n\n    async def run(self, transcription: str) -&gt; AsyncIterator[str]:\n        if self._callbacks:\n            self._callbacks.on_run(self, transcription)\n\n        # Add the transcription to the input history\n        self._input_history.append(\n            {\n                \"role\": \"user\",\n                \"content\": transcription,\n            }\n        )\n\n        # Run the agent\n        result = Runner.run_streamed(self._current_agent, self._input_history)\n\n        # Stream the text from the result\n        async for chunk in VoiceWorkflowHelper.stream_text_from(result):\n            yield chunk\n\n        # Update the input history and current agent\n        self._input_history = result.to_input_list()\n        self._current_agent = result.last_agent\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow.__init__","title":"__init__","text":"<pre><code>__init__(\n    agent: Agent[Any],\n    callbacks: SingleAgentWorkflowCallbacks | None = None,\n)\n</code></pre> <p>Create a new single agent voice workflow.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[Any]</code> <p>The agent to run.</p> required <code>callbacks</code> <code>SingleAgentWorkflowCallbacks | None</code> <p>Optional callbacks to call during the workflow.</p> <code>None</code> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>def __init__(self, agent: Agent[Any], callbacks: SingleAgentWorkflowCallbacks | None = None):\n    \"\"\"Create a new single agent voice workflow.\n\n    Args:\n        agent: The agent to run.\n        callbacks: Optional callbacks to call during the workflow.\n    \"\"\"\n    self._input_history: list[TResponseInputItem] = []\n    self._current_agent = agent\n    self._callbacks = callbacks\n</code></pre>"},{"location":"ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow.on_start","title":"on_start  <code>async</code>","text":"<pre><code>on_start() -&gt; AsyncIterator[str]\n</code></pre> <p>Optional method that runs before any user input is received. Can be used to deliver a greeting or instruction via TTS. Defaults to doing nothing.</p> Source code in <code>src/agents/voice/workflow.py</code> <pre><code>async def on_start(self) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Optional method that runs before any user input is received. Can be used\n    to deliver a greeting or instruction via TTS. Defaults to doing nothing.\n    \"\"\"\n    return\n    yield\n</code></pre>"},{"location":"ref/voice/models/openai_provider/","title":"<code>OpenAIVoiceModelProvider</code>","text":""},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider","title":"OpenAIVoiceModelProvider","text":"<p>               Bases: <code>VoiceModelProvider</code></p> <p>A voice model provider that uses OpenAI models.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>class OpenAIVoiceModelProvider(VoiceModelProvider):\n    \"\"\"A voice model provider that uses OpenAI models.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n    ) -&gt; None:\n        \"\"\"Create a new OpenAI voice model provider.\n\n        Args:\n            api_key: The API key to use for the OpenAI client. If not provided, we will use the\n                default API key.\n            base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n                default base URL.\n            openai_client: An optional OpenAI client to use. If not provided, we will create a new\n                OpenAI client using the api_key and base_url.\n            organization: The organization to use for the OpenAI client.\n            project: The project to use for the OpenAI client.\n        \"\"\"\n        if openai_client is not None:\n            assert api_key is None and base_url is None, (\n                \"Don't provide api_key or base_url if you provide openai_client\"\n            )\n            self._client: AsyncOpenAI | None = openai_client\n        else:\n            self._client = None\n            self._stored_api_key = api_key\n            self._stored_base_url = base_url\n            self._stored_organization = organization\n            self._stored_project = project\n\n    # We lazy load the client in case you never actually use OpenAIProvider(). Otherwise\n    # AsyncOpenAI() raises an error if you don't have an API key set.\n    def _get_client(self) -&gt; AsyncOpenAI:\n        if self._client is None:\n            self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(\n                api_key=self._stored_api_key or _openai_shared.get_default_openai_key(),\n                base_url=self._stored_base_url,\n                organization=self._stored_organization,\n                project=self._stored_project,\n                http_client=shared_http_client(),\n            )\n\n        return self._client\n\n    def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n        \"\"\"Get a speech-to-text model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The speech-to-text model.\n        \"\"\"\n        return OpenAISTTModel(model_name or DEFAULT_STT_MODEL, self._get_client())\n\n    def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n        \"\"\"Get a text-to-speech model by name.\n\n        Args:\n            model_name: The name of the model to get.\n\n        Returns:\n            The text-to-speech model.\n        \"\"\"\n        return OpenAITTSModel(model_name or DEFAULT_TTS_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n) -&gt; None\n</code></pre> <p>Create a new OpenAI voice model provider.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key to use for the OpenAI client. If not provided, we will use the default API key.</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>The base URL to use for the OpenAI client. If not provided, we will use the default base URL.</p> <code>None</code> <code>openai_client</code> <code>AsyncOpenAI | None</code> <p>An optional OpenAI client to use. If not provided, we will create a new OpenAI client using the api_key and base_url.</p> <code>None</code> <code>organization</code> <code>str | None</code> <p>The organization to use for the OpenAI client.</p> <code>None</code> <code>project</code> <code>str | None</code> <p>The project to use for the OpenAI client.</p> <code>None</code> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    organization: str | None = None,\n    project: str | None = None,\n) -&gt; None:\n    \"\"\"Create a new OpenAI voice model provider.\n\n    Args:\n        api_key: The API key to use for the OpenAI client. If not provided, we will use the\n            default API key.\n        base_url: The base URL to use for the OpenAI client. If not provided, we will use the\n            default base URL.\n        openai_client: An optional OpenAI client to use. If not provided, we will create a new\n            OpenAI client using the api_key and base_url.\n        organization: The organization to use for the OpenAI client.\n        project: The project to use for the OpenAI client.\n    \"\"\"\n    if openai_client is not None:\n        assert api_key is None and base_url is None, (\n            \"Don't provide api_key or base_url if you provide openai_client\"\n        )\n        self._client: AsyncOpenAI | None = openai_client\n    else:\n        self._client = None\n        self._stored_api_key = api_key\n        self._stored_base_url = base_url\n        self._stored_organization = organization\n        self._stored_project = project\n</code></pre>"},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.get_stt_model","title":"get_stt_model","text":"<pre><code>get_stt_model(model_name: str | None) -&gt; STTModel\n</code></pre> <p>Get a speech-to-text model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>STTModel</code> <p>The speech-to-text model.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def get_stt_model(self, model_name: str | None) -&gt; STTModel:\n    \"\"\"Get a speech-to-text model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The speech-to-text model.\n    \"\"\"\n    return OpenAISTTModel(model_name or DEFAULT_STT_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_provider/#agents.voice.models.openai_model_provider.OpenAIVoiceModelProvider.get_tts_model","title":"get_tts_model","text":"<pre><code>get_tts_model(model_name: str | None) -&gt; TTSModel\n</code></pre> <p>Get a text-to-speech model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str | None</code> <p>The name of the model to get.</p> required <p>Returns:</p> Type Description <code>TTSModel</code> <p>The text-to-speech model.</p> Source code in <code>src/agents/voice/models/openai_model_provider.py</code> <pre><code>def get_tts_model(self, model_name: str | None) -&gt; TTSModel:\n    \"\"\"Get a text-to-speech model by name.\n\n    Args:\n        model_name: The name of the model to get.\n\n    Returns:\n        The text-to-speech model.\n    \"\"\"\n    return OpenAITTSModel(model_name or DEFAULT_TTS_MODEL, self._get_client())\n</code></pre>"},{"location":"ref/voice/models/openai_stt/","title":"<code>OpenAI STT</code>","text":""},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTTranscriptionSession","title":"OpenAISTTTranscriptionSession","text":"<p>               Bases: <code>StreamedTranscriptionSession</code></p> <p>A transcription session for OpenAI's STT model.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>class OpenAISTTTranscriptionSession(StreamedTranscriptionSession):\n    \"\"\"A transcription session for OpenAI's STT model.\"\"\"\n\n    def __init__(\n        self,\n        input: StreamedAudioInput,\n        client: AsyncOpenAI,\n        model: str,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ):\n        self.connected: bool = False\n        self._client = client\n        self._model = model\n        self._settings = settings\n        self._turn_detection = settings.turn_detection or DEFAULT_TURN_DETECTION\n        self._trace_include_sensitive_data = trace_include_sensitive_data\n        self._trace_include_sensitive_audio_data = trace_include_sensitive_audio_data\n\n        self._input_queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32]] = input.queue\n        self._output_queue: asyncio.Queue[str | ErrorSentinel | SessionCompleteSentinel] = (\n            asyncio.Queue()\n        )\n        self._websocket: websockets.ClientConnection | None = None\n        self._event_queue: asyncio.Queue[dict[str, Any] | WebsocketDoneSentinel] = asyncio.Queue()\n        self._state_queue: asyncio.Queue[dict[str, Any]] = asyncio.Queue()\n        self._turn_audio_buffer: list[npt.NDArray[np.int16 | np.float32]] = []\n        self._tracing_span: Span[TranscriptionSpanData] | None = None\n\n        # tasks\n        self._listener_task: asyncio.Task[Any] | None = None\n        self._process_events_task: asyncio.Task[Any] | None = None\n        self._stream_audio_task: asyncio.Task[Any] | None = None\n        self._connection_task: asyncio.Task[Any] | None = None\n        self._stored_exception: Exception | None = None\n\n    def _start_turn(self) -&gt; None:\n        self._tracing_span = transcription_span(\n            model=self._model,\n            model_config={\n                \"temperature\": self._settings.temperature,\n                \"language\": self._settings.language,\n                \"prompt\": self._settings.prompt,\n                \"turn_detection\": self._turn_detection,\n            },\n        )\n        self._tracing_span.start()\n\n    def _end_turn(self, _transcript: str) -&gt; None:\n        if len(_transcript) &lt; 1:\n            return\n\n        if self._tracing_span:\n            if self._trace_include_sensitive_audio_data:\n                self._tracing_span.span_data.input = _audio_to_base64(self._turn_audio_buffer)\n\n            self._tracing_span.span_data.input_format = \"pcm\"\n\n            if self._trace_include_sensitive_data:\n                self._tracing_span.span_data.output = _transcript\n\n            self._tracing_span.finish()\n            self._turn_audio_buffer = []\n            self._tracing_span = None\n\n    async def _event_listener(self) -&gt; None:\n        assert self._websocket is not None, \"Websocket not initialized\"\n\n        async for message in self._websocket:\n            try:\n                event = json.loads(message)\n\n                if event.get(\"type\") == \"error\":\n                    raise STTWebsocketConnectionError(f\"Error event: {event.get('error')}\")\n\n                if event.get(\"type\") in [\n                    \"session.updated\",\n                    \"transcription_session.updated\",\n                    \"session.created\",\n                    \"transcription_session.created\",\n                ]:\n                    await self._state_queue.put(event)\n\n                await self._event_queue.put(event)\n            except Exception as e:\n                await self._output_queue.put(ErrorSentinel(e))\n                raise STTWebsocketConnectionError(\"Error parsing events\") from e\n        await self._event_queue.put(WebsocketDoneSentinel())\n\n    async def _configure_session(self) -&gt; None:\n        assert self._websocket is not None, \"Websocket not initialized\"\n        await self._websocket.send(\n            json.dumps(\n                {\n                    \"type\": \"transcription_session.update\",\n                    \"session\": {\n                        \"input_audio_format\": \"pcm16\",\n                        \"input_audio_transcription\": {\"model\": self._model},\n                        \"turn_detection\": self._turn_detection,\n                    },\n                }\n            )\n        )\n\n    async def _setup_connection(self, ws: websockets.ClientConnection) -&gt; None:\n        self._websocket = ws\n        self._listener_task = asyncio.create_task(self._event_listener())\n\n        try:\n            event = await _wait_for_event(\n                self._state_queue,\n                [\"session.created\", \"transcription_session.created\"],\n                SESSION_CREATION_TIMEOUT,\n            )\n        except TimeoutError as e:\n            wrapped_err = STTWebsocketConnectionError(\n                \"Timeout waiting for transcription_session.created event\"\n            )\n            await self._output_queue.put(ErrorSentinel(wrapped_err))\n            raise wrapped_err from e\n        except Exception as e:\n            await self._output_queue.put(ErrorSentinel(e))\n            raise e\n\n        await self._configure_session()\n\n        try:\n            event = await _wait_for_event(\n                self._state_queue,\n                [\"session.updated\", \"transcription_session.updated\"],\n                SESSION_UPDATE_TIMEOUT,\n            )\n            if _debug.DONT_LOG_MODEL_DATA:\n                logger.debug(\"Session updated\")\n            else:\n                logger.debug(f\"Session updated: {event}\")\n        except TimeoutError as e:\n            wrapped_err = STTWebsocketConnectionError(\n                \"Timeout waiting for transcription_session.updated event\"\n            )\n            await self._output_queue.put(ErrorSentinel(wrapped_err))\n            raise wrapped_err from e\n        except Exception as e:\n            await self._output_queue.put(ErrorSentinel(e))\n            raise\n\n    async def _handle_events(self) -&gt; None:\n        while True:\n            try:\n                event = await asyncio.wait_for(\n                    self._event_queue.get(), timeout=EVENT_INACTIVITY_TIMEOUT\n                )\n                if isinstance(event, WebsocketDoneSentinel):\n                    # processed all events and websocket is done\n                    break\n\n                event_type = event.get(\"type\", \"unknown\")\n                if event_type == \"input_audio_transcription_completed\":\n                    transcript = cast(str, event.get(\"transcript\", \"\"))\n                    if len(transcript) &gt; 0:\n                        self._end_turn(transcript)\n                        self._start_turn()\n                        await self._output_queue.put(transcript)\n                await asyncio.sleep(0)  # yield control\n            except asyncio.TimeoutError:\n                # No new events for a while. Assume the session is done.\n                break\n            except Exception as e:\n                await self._output_queue.put(ErrorSentinel(e))\n                raise e\n        await self._output_queue.put(SessionCompleteSentinel())\n\n    async def _stream_audio(\n        self, audio_queue: asyncio.Queue[npt.NDArray[np.int16 | np.float32]]\n    ) -&gt; None:\n        assert self._websocket is not None, \"Websocket not initialized\"\n        self._start_turn()\n        while True:\n            buffer = await audio_queue.get()\n            if buffer is None:\n                break\n\n            self._turn_audio_buffer.append(buffer)\n            try:\n                await self._websocket.send(\n                    json.dumps(\n                        {\n                            \"type\": \"input_audio_buffer.append\",\n                            \"audio\": base64.b64encode(buffer.tobytes()).decode(\"utf-8\"),\n                        }\n                    )\n                )\n            except websockets.ConnectionClosed:\n                break\n            except Exception as e:\n                await self._output_queue.put(ErrorSentinel(e))\n                raise e\n\n            await asyncio.sleep(0)  # yield control\n\n    async def _process_websocket_connection(self) -&gt; None:\n        try:\n            async with websockets.connect(\n                \"wss://api.openai.com/v1/realtime?intent=transcription\",\n                additional_headers={\n                    \"Authorization\": f\"Bearer {self._client.api_key}\",\n                    \"OpenAI-Beta\": \"realtime=v1\",\n                    \"OpenAI-Log-Session\": \"1\",\n                },\n            ) as ws:\n                await self._setup_connection(ws)\n                self._process_events_task = asyncio.create_task(self._handle_events())\n                self._stream_audio_task = asyncio.create_task(self._stream_audio(self._input_queue))\n                self.connected = True\n                if self._listener_task:\n                    await self._listener_task\n                else:\n                    logger.error(\"Listener task not initialized\")\n                    raise AgentsException(\"Listener task not initialized\")\n        except Exception as e:\n            await self._output_queue.put(ErrorSentinel(e))\n            raise e\n\n    def _check_errors(self) -&gt; None:\n        if self._connection_task and self._connection_task.done():\n            exc = self._connection_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n        if self._process_events_task and self._process_events_task.done():\n            exc = self._process_events_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n        if self._stream_audio_task and self._stream_audio_task.done():\n            exc = self._stream_audio_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n        if self._listener_task and self._listener_task.done():\n            exc = self._listener_task.exception()\n            if exc and isinstance(exc, Exception):\n                self._stored_exception = exc\n\n    def _cleanup_tasks(self) -&gt; None:\n        if self._listener_task and not self._listener_task.done():\n            self._listener_task.cancel()\n\n        if self._process_events_task and not self._process_events_task.done():\n            self._process_events_task.cancel()\n\n        if self._stream_audio_task and not self._stream_audio_task.done():\n            self._stream_audio_task.cancel()\n\n        if self._connection_task and not self._connection_task.done():\n            self._connection_task.cancel()\n\n    async def transcribe_turns(self) -&gt; AsyncIterator[str]:\n        self._connection_task = asyncio.create_task(self._process_websocket_connection())\n\n        while True:\n            try:\n                turn = await self._output_queue.get()\n            except asyncio.CancelledError:\n                break\n\n            if (\n                turn is None\n                or isinstance(turn, ErrorSentinel)\n                or isinstance(turn, SessionCompleteSentinel)\n            ):\n                self._output_queue.task_done()\n                break\n            yield turn\n            self._output_queue.task_done()\n\n        if self._tracing_span:\n            self._end_turn(\"\")\n\n        if self._websocket:\n            await self._websocket.close()\n\n        self._check_errors()\n        if self._stored_exception:\n            raise self._stored_exception\n\n    async def close(self) -&gt; None:\n        if self._websocket:\n            await self._websocket.close()\n\n        self._cleanup_tasks()\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel","title":"OpenAISTTModel","text":"<p>               Bases: <code>STTModel</code></p> <p>A speech-to-text model for OpenAI.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>class OpenAISTTModel(STTModel):\n    \"\"\"A speech-to-text model for OpenAI.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        openai_client: AsyncOpenAI,\n    ):\n        \"\"\"Create a new OpenAI speech-to-text model.\n\n        Args:\n            model: The name of the model to use.\n            openai_client: The OpenAI client to use.\n        \"\"\"\n        self.model = model\n        self._client = openai_client\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model\n\n    def _non_null_or_not_given(self, value: Any) -&gt; Any:\n        return value if value is not None else None  # NOT_GIVEN\n\n    async def transcribe(\n        self,\n        input: AudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; str:\n        \"\"\"Transcribe an audio input.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n\n        Returns:\n            The transcribed text.\n        \"\"\"\n        with transcription_span(\n            model=self.model,\n            input=input.to_base64() if trace_include_sensitive_audio_data else \"\",\n            input_format=\"pcm\",\n            model_config={\n                \"temperature\": self._non_null_or_not_given(settings.temperature),\n                \"language\": self._non_null_or_not_given(settings.language),\n                \"prompt\": self._non_null_or_not_given(settings.prompt),\n            },\n        ) as span:\n            try:\n                response = await self._client.audio.transcriptions.create(\n                    model=self.model,\n                    file=input.to_audio_file(),\n                    prompt=self._non_null_or_not_given(settings.prompt),\n                    language=self._non_null_or_not_given(settings.language),\n                    temperature=self._non_null_or_not_given(settings.temperature),\n                )\n                if trace_include_sensitive_data:\n                    span.span_data.output = response.text\n                return response.text\n            except Exception as e:\n                span.span_data.output = \"\"\n                span.set_error(SpanError(message=str(e), data={}))\n                raise e\n\n    async def create_session(\n        self,\n        input: StreamedAudioInput,\n        settings: STTModelSettings,\n        trace_include_sensitive_data: bool,\n        trace_include_sensitive_audio_data: bool,\n    ) -&gt; StreamedTranscriptionSession:\n        \"\"\"Create a new transcription session.\n\n        Args:\n            input: The audio input to transcribe.\n            settings: The settings to use for the transcription.\n            trace_include_sensitive_data: Whether to include sensitive data in traces.\n            trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n        Returns:\n            A new transcription session.\n        \"\"\"\n        return OpenAISTTTranscriptionSession(\n            input,\n            self._client,\n            self.model,\n            settings,\n            trace_include_sensitive_data,\n            trace_include_sensitive_audio_data,\n        )\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel.__init__","title":"__init__","text":"<pre><code>__init__(model: str, openai_client: AsyncOpenAI)\n</code></pre> <p>Create a new OpenAI speech-to-text model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model to use.</p> required <code>openai_client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use.</p> required Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    openai_client: AsyncOpenAI,\n):\n    \"\"\"Create a new OpenAI speech-to-text model.\n\n    Args:\n        model: The name of the model to use.\n        openai_client: The OpenAI client to use.\n    \"\"\"\n    self.model = model\n    self._client = openai_client\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel.transcribe","title":"transcribe  <code>async</code>","text":"<pre><code>transcribe(\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str\n</code></pre> <p>Transcribe an audio input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>AudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transcribed text.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>async def transcribe(\n    self,\n    input: AudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; str:\n    \"\"\"Transcribe an audio input.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n\n    Returns:\n        The transcribed text.\n    \"\"\"\n    with transcription_span(\n        model=self.model,\n        input=input.to_base64() if trace_include_sensitive_audio_data else \"\",\n        input_format=\"pcm\",\n        model_config={\n            \"temperature\": self._non_null_or_not_given(settings.temperature),\n            \"language\": self._non_null_or_not_given(settings.language),\n            \"prompt\": self._non_null_or_not_given(settings.prompt),\n        },\n    ) as span:\n        try:\n            response = await self._client.audio.transcriptions.create(\n                model=self.model,\n                file=input.to_audio_file(),\n                prompt=self._non_null_or_not_given(settings.prompt),\n                language=self._non_null_or_not_given(settings.language),\n                temperature=self._non_null_or_not_given(settings.temperature),\n            )\n            if trace_include_sensitive_data:\n                span.span_data.output = response.text\n            return response.text\n        except Exception as e:\n            span.span_data.output = \"\"\n            span.set_error(SpanError(message=str(e), data={}))\n            raise e\n</code></pre>"},{"location":"ref/voice/models/openai_stt/#agents.voice.models.openai_stt.OpenAISTTModel.create_session","title":"create_session  <code>async</code>","text":"<pre><code>create_session(\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession\n</code></pre> <p>Create a new transcription session.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>StreamedAudioInput</code> <p>The audio input to transcribe.</p> required <code>settings</code> <code>STTModelSettings</code> <p>The settings to use for the transcription.</p> required <code>trace_include_sensitive_data</code> <code>bool</code> <p>Whether to include sensitive data in traces.</p> required <code>trace_include_sensitive_audio_data</code> <code>bool</code> <p>Whether to include sensitive audio data in traces.</p> required <p>Returns:</p> Type Description <code>StreamedTranscriptionSession</code> <p>A new transcription session.</p> Source code in <code>src/agents/voice/models/openai_stt.py</code> <pre><code>async def create_session(\n    self,\n    input: StreamedAudioInput,\n    settings: STTModelSettings,\n    trace_include_sensitive_data: bool,\n    trace_include_sensitive_audio_data: bool,\n) -&gt; StreamedTranscriptionSession:\n    \"\"\"Create a new transcription session.\n\n    Args:\n        input: The audio input to transcribe.\n        settings: The settings to use for the transcription.\n        trace_include_sensitive_data: Whether to include sensitive data in traces.\n        trace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\n\n    Returns:\n        A new transcription session.\n    \"\"\"\n    return OpenAISTTTranscriptionSession(\n        input,\n        self._client,\n        self.model,\n        settings,\n        trace_include_sensitive_data,\n        trace_include_sensitive_audio_data,\n    )\n</code></pre>"},{"location":"ref/voice/models/openai_tts/","title":"<code>OpenAI TTS</code>","text":""},{"location":"ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel","title":"OpenAITTSModel","text":"<p>               Bases: <code>TTSModel</code></p> <p>A text-to-speech model for OpenAI.</p> Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>class OpenAITTSModel(TTSModel):\n    \"\"\"A text-to-speech model for OpenAI.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        openai_client: AsyncOpenAI,\n    ):\n        \"\"\"Create a new OpenAI text-to-speech model.\n\n        Args:\n            model: The name of the model to use.\n            openai_client: The OpenAI client to use.\n        \"\"\"\n        self.model = model\n        self._client = openai_client\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model\n\n    async def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n        \"\"\"Run the text-to-speech model.\n\n        Args:\n            text: The text to convert to speech.\n            settings: The settings to use for the text-to-speech model.\n\n        Returns:\n            An iterator of audio chunks.\n        \"\"\"\n        response = self._client.audio.speech.with_streaming_response.create(\n            model=self.model,\n            voice=settings.voice or DEFAULT_VOICE,\n            input=text,\n            response_format=\"pcm\",\n            extra_body={\n                \"instructions\": settings.instructions,\n            },\n        )\n\n        async with response as stream:\n            async for chunk in stream.iter_bytes(chunk_size=1024):\n                yield chunk\n</code></pre>"},{"location":"ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel.__init__","title":"__init__","text":"<pre><code>__init__(model: str, openai_client: AsyncOpenAI)\n</code></pre> <p>Create a new OpenAI text-to-speech model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model to use.</p> required <code>openai_client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use.</p> required Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    openai_client: AsyncOpenAI,\n):\n    \"\"\"Create a new OpenAI text-to-speech model.\n\n    Args:\n        model: The name of the model to use.\n        openai_client: The OpenAI client to use.\n    \"\"\"\n    self.model = model\n    self._client = openai_client\n</code></pre>"},{"location":"ref/voice/models/openai_tts/#agents.voice.models.openai_tts.OpenAITTSModel.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    text: str, settings: TTSModelSettings\n) -&gt; AsyncIterator[bytes]\n</code></pre> <p>Run the text-to-speech model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to convert to speech.</p> required <code>settings</code> <code>TTSModelSettings</code> <p>The settings to use for the text-to-speech model.</p> required <p>Returns:</p> Type Description <code>AsyncIterator[bytes]</code> <p>An iterator of audio chunks.</p> Source code in <code>src/agents/voice/models/openai_tts.py</code> <pre><code>async def run(self, text: str, settings: TTSModelSettings) -&gt; AsyncIterator[bytes]:\n    \"\"\"Run the text-to-speech model.\n\n    Args:\n        text: The text to convert to speech.\n        settings: The settings to use for the text-to-speech model.\n\n    Returns:\n        An iterator of audio chunks.\n    \"\"\"\n    response = self._client.audio.speech.with_streaming_response.create(\n        model=self.model,\n        voice=settings.voice or DEFAULT_VOICE,\n        input=text,\n        response_format=\"pcm\",\n        extra_body={\n            \"instructions\": settings.instructions,\n        },\n    )\n\n    async with response as stream:\n        async for chunk in stream.iter_bytes(chunk_size=1024):\n            yield chunk\n</code></pre>"},{"location":"voice/pipeline/","title":"Pipelines and workflows","text":"<p><code>VoicePipeline</code> is a class that makes it easy to turn your agentic workflows into a voice app. You pass in a workflow to run, and the pipeline takes care of transcribing input audio, detecting when the audio ends, calling your workflow at the right time, and turning the workflow output back into audio.</p> <pre><code>graph LR\n    %% Input\n    A[\"\ud83c\udfa4 Audio Input\"]\n\n    %% Voice Pipeline\n    subgraph Voice_Pipeline [Voice Pipeline]\n        direction TB\n        B[\"Transcribe (speech-to-text)\"]\n        C[\"Your Code\"]:::highlight\n        D[\"Text-to-speech\"]\n        B --&gt; C --&gt; D\n    end\n\n    %% Output\n    E[\"\ud83c\udfa7 Audio Output\"]\n\n    %% Flow\n    A --&gt; Voice_Pipeline\n    Voice_Pipeline --&gt; E\n\n    %% Custom styling\n    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;\n</code></pre>"},{"location":"voice/pipeline/#configuring-a-pipeline","title":"Configuring a pipeline","text":"<p>When you create a pipeline, you can set a few things:</p> <ol> <li>The <code>workflow</code>, which is the code that runs each time new audio is transcribed.</li> <li>The <code>speech-to-text</code> and <code>text-to-speech</code> models used</li> <li>The <code>config</code>, which lets you configure things like:<ul> <li>A model provider, which can map model names to models</li> <li>Tracing, including whether to disable tracing, whether audio files are uploaded, the workflow name, trace IDs etc.</li> <li>Settings on the TTS and STT models, like the prompt, language and data types used.</li> </ul> </li> </ol>"},{"location":"voice/pipeline/#running-a-pipeline","title":"Running a pipeline","text":"<p>You can run a pipeline via the <code>run()</code> method, which lets you pass in audio input in two forms:</p> <ol> <li><code>AudioInput</code> is used when you have a full audio transcript, and just want to produce a result for it. This is useful in cases where you don't need to detect when a speaker is done speaking; for example, when you have pre-recorded audio or in push-to-talk apps where it's clear when the user is done speaking.</li> <li><code>StreamedAudioInput</code> is used when you might need to detect when a user is done speaking. It allows you to push audio chunks as they are detected, and the voice pipeline will automatically run the agent workflow at the right time, via a process called \"activity detection\".</li> </ol>"},{"location":"voice/pipeline/#results","title":"Results","text":"<p>The result of a voice pipeline run is a <code>StreamedAudioResult</code>. This is an object that lets you stream events as they occur. There are a few kinds of <code>VoiceStreamEvent</code>, including:</p> <ol> <li><code>VoiceStreamEventAudio</code>, which contains a chunk of audio.</li> <li><code>VoiceStreamEventLifecycle</code>, which informs you of lifecycle events like a turn starting or ending.</li> <li><code>VoiceStreamEventError</code>, is an error event.</li> </ol> <pre><code>result = await pipeline.run(input)\n\nasync for event in result.stream():\n    if event.type == \"voice_stream_event_audio\":\n        # play audio\n    elif event.type == \"voice_stream_event_lifecycle\":\n        # lifecycle\n    elif event.type == \"voice_stream_event_error\"\n        # error\n    ...\n</code></pre>"},{"location":"voice/pipeline/#best-practices","title":"Best practices","text":""},{"location":"voice/pipeline/#interruptions","title":"Interruptions","text":"<p>The Agents SDK currently does not support any built-in interruptions support for <code>StreamedAudioInput</code>. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the <code>VoiceStreamEventLifecycle</code> events. <code>turn_started</code> will indicate that a new turn was transcribed and processing is beginning. <code>turn_ended</code> will trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.</p>"},{"location":"voice/quickstart/","title":"Quickstart","text":""},{"location":"voice/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you've followed the base quickstart instructions for the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK:</p> <pre><code>pip install 'openai-agents[voice]'\n</code></pre>"},{"location":"voice/quickstart/#concepts","title":"Concepts","text":"<p>The main concept to know about is a <code>VoicePipeline</code>, which is a 3 step process:</p> <ol> <li>Run a speech-to-text model to turn audio into text.</li> <li>Run your code, which is usually an agentic workflow, to produce a result.</li> <li>Run a text-to-speech model to turn the result text back into audio.</li> </ol> <pre><code>graph LR\n    %% Input\n    A[\"\ud83c\udfa4 Audio Input\"]\n\n    %% Voice Pipeline\n    subgraph Voice_Pipeline [Voice Pipeline]\n        direction TB\n        B[\"Transcribe (speech-to-text)\"]\n        C[\"Your Code\"]:::highlight\n        D[\"Text-to-speech\"]\n        B --&gt; C --&gt; D\n    end\n\n    %% Output\n    E[\"\ud83c\udfa7 Audio Output\"]\n\n    %% Flow\n    A --&gt; Voice_Pipeline\n    Voice_Pipeline --&gt; E\n\n    %% Custom styling\n    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;\n</code></pre>"},{"location":"voice/quickstart/#agents","title":"Agents","text":"<p>First, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool.</p> <pre><code>import asyncio\nimport random\n\nfrom agents import (\n    Agent,\n    function_tool,\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    print(f\"[debug] get_weather called with city: {city}\")\n    choices = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    return f\"The weather in {city} is {random.choice(choices)}.\"\n\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    handoff_description=\"A spanish speaking agent.\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. Speak in Spanish.\",\n    ),\n    model=\"gpt-4o-mini\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\",\n    ),\n    model=\"gpt-4o-mini\",\n    handoffs=[spanish_agent],\n    tools=[get_weather],\n)\n</code></pre>"},{"location":"voice/quickstart/#voice-pipeline","title":"Voice pipeline","text":"<p>We'll set up a simple voice pipeline, using <code>SingleAgentVoiceWorkflow</code> as the workflow.</p> <pre><code>from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline\npipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n</code></pre>"},{"location":"voice/quickstart/#run-the-pipeline","title":"Run the pipeline","text":"<pre><code>import numpy as np\nimport sounddevice as sd\nfrom agents.voice import AudioInput\n\n# For simplicity, we'll just create 3 seconds of silence\n# In reality, you'd get microphone data\nbuffer = np.zeros(24000 * 3, dtype=np.int16)\naudio_input = AudioInput(buffer=buffer)\n\nresult = await pipeline.run(audio_input)\n\n# Create an audio player using `sounddevice`\nplayer = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\nplayer.start()\n\n# Play the audio stream as it comes in\nasync for event in result.stream():\n    if event.type == \"voice_stream_event_audio\":\n        player.write(event.data)\n</code></pre>"},{"location":"voice/quickstart/#put-it-all-together","title":"Put it all together","text":"<pre><code>import asyncio\nimport random\n\nimport numpy as np\nimport sounddevice as sd\n\nfrom agents import (\n    Agent,\n    function_tool,\n    set_tracing_disabled,\n)\nfrom agents.voice import (\n    AudioInput,\n    SingleAgentVoiceWorkflow,\n    VoicePipeline,\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    print(f\"[debug] get_weather called with city: {city}\")\n    choices = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    return f\"The weather in {city} is {random.choice(choices)}.\"\n\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    handoff_description=\"A spanish speaking agent.\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. Speak in Spanish.\",\n    ),\n    model=\"gpt-4o-mini\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\",\n    ),\n    model=\"gpt-4o-mini\",\n    handoffs=[spanish_agent],\n    tools=[get_weather],\n)\n\n\nasync def main():\n    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n    buffer = np.zeros(24000 * 3, dtype=np.int16)\n    audio_input = AudioInput(buffer=buffer)\n\n    result = await pipeline.run(audio_input)\n\n    # Create an audio player using `sounddevice`\n    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n    player.start()\n\n    # Play the audio stream as it comes in\n    async for event in result.stream():\n        if event.type == \"voice_stream_event_audio\":\n            player.write(event.data)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>If you run this example, the agent will speak to you! Check out the example in examples/voice/static to see a demo where you can speak to the agent yourself.</p>"},{"location":"voice/tracing/","title":"Tracing","text":"<p>Just like the way agents are traced, voice pipelines are also automatically traced.</p> <p>You can read the tracing doc above for basic tracing information, but you can additionally configure tracing of a pipeline via <code>VoicePipelineConfig</code>.</p> <p>Key tracing related fields are:</p> <ul> <li><code>tracing_disabled</code>: controls whether tracing is disabled. By default, tracing is enabled.</li> <li><code>trace_include_sensitive_data</code>: controls whether traces include potentially sensitive data, like audio transcripts. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.</li> <li><code>trace_include_sensitive_audio_data</code>: controls whether traces include audio data.</li> <li><code>workflow_name</code>: The name of the trace workflow.</li> <li><code>group_id</code>: The <code>group_id</code> of the trace, which lets you link multiple traces.</li> <li><code>trace_metadata</code>: Additional metadata to include with the trace.</li> </ul>"}]}